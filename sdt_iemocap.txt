--- 1 ---
Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.5, batch_size=64, hidden_dim=1024, n_head=8, epochs=150, temp=2, tensorboard=False, class_weight=True, Dataset='IEMOCAP')
Running on GPU
temp 2
total parameters: 79687704
training parameters: 79687704
loss: 7.877317428588867
loss: 11.776787757873535
loss: 8.071625709533691
epoch: 1, train_loss: 9.626999855041504, train_acc: 15.01, train_fscore: 13.49, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.071599960327148, test_acc: 20.52, test_fscore: 13.68, time: 3.63 sec
loss: 8.291715621948242
loss: 8.588519096374512
loss: 8.726579666137695
epoch: 2, train_loss: 8.435700416564941, train_acc: 27.93, train_fscore: 25.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.72659969329834, test_acc: 35.24, test_fscore: 27.03, time: 2.95 sec
loss: 9.01269817352295
loss: 8.108131408691406
loss: 7.141110897064209
epoch: 3, train_loss: 8.59469985961914, train_acc: 35.27, train_fscore: 30.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.14109992980957, test_acc: 38.76, test_fscore: 29.62, time: 3.77 sec
loss: 7.437477111816406
loss: 7.241649150848389
loss: 7.211901664733887
epoch: 4, train_loss: 7.347499847412109, train_acc: 40.71, train_fscore: 31.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.211900234222412, test_acc: 37.03, test_fscore: 31.55, time: 4.8 sec
loss: 7.3531365394592285
loss: 7.5557427406311035
loss: 7.119721412658691
epoch: 5, train_loss: 7.450099945068359, train_acc: 41.17, train_fscore: 35.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.119699954986572, test_acc: 35.86, test_fscore: 30.22, time: 5.77 sec
loss: 7.28498649597168
loss: 6.818604946136475
loss: 6.58373498916626
epoch: 6, train_loss: 7.059700012207031, train_acc: 43.89, train_fscore: 39.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.583700180053711, test_acc: 45.59, test_fscore: 42.21, time: 6.51 sec
loss: 6.754751682281494
loss: 6.5283002853393555
loss: 6.653420448303223
epoch: 7, train_loss: 6.64870023727417, train_acc: 50.86, train_fscore: 46.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.65339994430542, test_acc: 53.3, test_fscore: 49.91, time: 5.28 sec
loss: 6.702971935272217
loss: 6.661890029907227
loss: 6.600710868835449
epoch: 8, train_loss: 6.6844000816345215, train_acc: 55.63, train_fscore: 51.51, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.6006999015808105, test_acc: 57.92, test_fscore: 54.93, time: 6.18 sec
loss: 6.612339019775391
loss: 6.397043704986572
loss: 6.312973499298096
epoch: 9, train_loss: 6.513199806213379, train_acc: 56.76, train_fscore: 53.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.313000202178955, test_acc: 56.75, test_fscore: 56.56, time: 5.85 sec
loss: 6.2758073806762695
loss: 6.110683441162109
loss: 6.237760066986084
epoch: 10, train_loss: 6.200300216674805, train_acc: 58.95, train_fscore: 58.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.237800121307373, test_acc: 47.63, test_fscore: 48.47, time: 6.07 sec
              precision    recall  f1-score   support

           0     0.2849    0.3403    0.3101     144.0
           1     0.7354    0.7714    0.7530     245.0
           2     0.5730    0.4193    0.4842     384.0
           3     0.4960    0.7235    0.5885     170.0
           4     0.6361    0.6488    0.6424     299.0
           5     0.5694    0.5381    0.5533     381.0

    accuracy                         0.5675    1623.0
   macro avg     0.5491    0.5736    0.5553    1623.0
weighted avg     0.5747    0.5675    0.5656    1623.0

[[ 49.  11.   9.  10.  64.   1.]
 [  8. 189.  17.   3.   7.  21.]
 [ 64.  31. 161.  19.  16.  93.]
 [  0.   1.  11. 123.   9.  26.]
 [ 43.   4.  31.  13. 194.  14.]
 [  8.  21.  52.  80.  15. 205.]]
loss: 6.142899990081787
loss: 6.058940410614014
loss: 6.129246234893799
epoch: 11, train_loss: 6.104800224304199, train_acc: 54.87, train_fscore: 54.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.129199981689453, test_acc: 44.49, test_fscore: 44.46, time: 6.77 sec
loss: 5.975276947021484
loss: 5.854191780090332
loss: 5.839991569519043
epoch: 12, train_loss: 5.917200088500977, train_acc: 55.89, train_fscore: 55.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.840000152587891, test_acc: 54.1, test_fscore: 54.18, time: 5.53 sec
loss: 5.650241851806641
loss: 5.700145721435547
loss: 5.618930339813232
epoch: 13, train_loss: 5.672800064086914, train_acc: 59.95, train_fscore: 58.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.618899822235107, test_acc: 60.01, test_fscore: 58.86, time: 6.46 sec
loss: 5.695508003234863
loss: 5.21562385559082
loss: 5.481503009796143
epoch: 14, train_loss: 5.480000019073486, train_acc: 61.46, train_fscore: 59.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.481500148773193, test_acc: 60.94, test_fscore: 59.27, time: 6.66 sec
loss: 5.410566329956055
loss: 5.2565436363220215
loss: 5.2770094871521
epoch: 15, train_loss: 5.339099884033203, train_acc: 62.15, train_fscore: 59.85, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.2769999504089355, test_acc: 60.07, test_fscore: 58.9, time: 6.29 sec
loss: 5.168479919433594
loss: 5.167607307434082
loss: 5.135614395141602
epoch: 16, train_loss: 5.168099880218506, train_acc: 62.36, train_fscore: 60.5, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.1356000900268555, test_acc: 60.94, test_fscore: 60.54, time: 6.03 sec
loss: 5.217450141906738
loss: 4.9197678565979
loss: 5.046485900878906
epoch: 17, train_loss: 5.072700023651123, train_acc: 63.18, train_fscore: 61.62, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.046500205993652, test_acc: 60.75, test_fscore: 61.27, time: 5.54 sec
loss: 4.757580280303955
loss: 5.189114570617676
loss: 4.970476150512695
epoch: 18, train_loss: 4.956699848175049, train_acc: 64.51, train_fscore: 63.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.9704999923706055, test_acc: 60.94, test_fscore: 61.77, time: 6.9 sec
loss: 5.037483215332031
loss: 4.64000940322876
loss: 4.861861705780029
epoch: 19, train_loss: 4.847400188446045, train_acc: 65.09, train_fscore: 64.37, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.8618998527526855, test_acc: 62.66, test_fscore: 63.51, time: 5.69 sec
loss: 4.867093086242676
loss: 4.625182151794434
loss: 4.737881660461426
epoch: 20, train_loss: 4.752299785614014, train_acc: 64.66, train_fscore: 63.97, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.7378997802734375, test_acc: 64.2, test_fscore: 64.82, time: 6.46 sec
              precision    recall  f1-score   support

           0     0.3455    0.5278    0.4176     144.0
           1     0.7426    0.7184    0.7303     245.0
           2     0.6361    0.5781    0.6057     384.0
           3     0.6238    0.7412    0.6774     170.0
           4     0.8142    0.6890    0.7464     299.0
           5     0.6519    0.6194    0.6353     381.0

    accuracy                         0.6420    1623.0
   macro avg     0.6357    0.6456    0.6354    1623.0
weighted avg     0.6616    0.6420    0.6482    1623.0

[[ 76.  11.  12.   5.  39.   1.]
 [  6. 176.  34.   2.   0.  27.]
 [ 53.  27. 222.  20.   4.  58.]
 [  0.   0.   5. 126.   0.  39.]
 [ 81.   0.  11.   0. 206.   1.]
 [  4.  23.  65.  49.   4. 236.]]
loss: 4.710938930511475
loss: 4.585973262786865
loss: 4.638509273529053
epoch: 21, train_loss: 4.656099796295166, train_acc: 65.49, train_fscore: 64.89, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.638500213623047, test_acc: 64.14, test_fscore: 64.7, time: 6.44 sec
loss: 4.687796115875244
loss: 4.520914554595947
loss: 4.5891432762146
epoch: 22, train_loss: 4.608799934387207, train_acc: 65.06, train_fscore: 64.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.589099884033203, test_acc: 63.46, test_fscore: 63.68, time: 6.51 sec
loss: 4.522841453552246
loss: 4.530283451080322
loss: 4.531832695007324
epoch: 23, train_loss: 4.526299953460693, train_acc: 64.92, train_fscore: 63.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.531799793243408, test_acc: 64.2, test_fscore: 64.2, time: 6.12 sec
loss: 4.4396891593933105
loss: 4.501942157745361
loss: 4.46506929397583
epoch: 24, train_loss: 4.46750020980835, train_acc: 65.78, train_fscore: 64.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.465099811553955, test_acc: 64.08, test_fscore: 64.35, time: 6.47 sec
loss: 4.273232460021973
loss: 4.547588348388672
loss: 4.415950298309326
epoch: 25, train_loss: 4.4008002281188965, train_acc: 66.04, train_fscore: 65.17, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.415999889373779, test_acc: 64.26, test_fscore: 64.71, time: 5.85 sec
loss: 4.551870346069336
loss: 4.124736785888672
loss: 4.394596099853516
epoch: 26, train_loss: 4.349599838256836, train_acc: 66.27, train_fscore: 65.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.394599914550781, test_acc: 64.26, test_fscore: 64.89, time: 6.69 sec
loss: 4.343407154083252
loss: 4.237735271453857
loss: 4.3739213943481445
epoch: 27, train_loss: 4.293600082397461, train_acc: 67.21, train_fscore: 66.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.373899936676025, test_acc: 64.57, test_fscore: 65.18, time: 5.8 sec
loss: 4.345953941345215
loss: 4.123624801635742
loss: 4.342580318450928
epoch: 28, train_loss: 4.242800235748291, train_acc: 68.07, train_fscore: 67.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.342599868774414, test_acc: 65.31, test_fscore: 65.57, time: 5.72 sec
loss: 4.263312816619873
loss: 4.135689735412598
loss: 4.295724391937256
epoch: 29, train_loss: 4.204899787902832, train_acc: 67.09, train_fscore: 66.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.2957000732421875, test_acc: 65.99, test_fscore: 66.08, time: 5.52 sec
loss: 4.355514049530029
loss: 3.93928861618042
loss: 4.2500176429748535
epoch: 30, train_loss: 4.169300079345703, train_acc: 68.02, train_fscore: 67.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.25, test_acc: 65.68, test_fscore: 65.9, time: 6.19 sec
              precision    recall  f1-score   support

           0     0.3931    0.4722    0.4290     144.0
           1     0.7634    0.6980    0.7292     245.0
           2     0.6821    0.5365    0.6006     384.0
           3     0.5792    0.7529    0.6547     170.0
           4     0.8125    0.8261    0.8192     299.0
           5     0.6291    0.6588    0.6436     381.0

    accuracy                         0.6599    1623.0
   macro avg     0.6432    0.6574    0.6461    1623.0
weighted avg     0.6695    0.6599    0.6608    1623.0

[[ 68.   9.  13.   6.  47.   1.]
 [  6. 171.  33.   0.   0.  35.]
 [ 53.  23. 206.  26.   5.  71.]
 [  0.   0.   2. 128.   0.  40.]
 [ 43.   0.   8.   0. 247.   1.]
 [  3.  21.  40.  61.   5. 251.]]
loss: 4.154173851013184
loss: 4.033973217010498
loss: 4.225383758544922
epoch: 31, train_loss: 4.097099781036377, train_acc: 68.52, train_fscore: 67.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.225399971008301, test_acc: 66.11, test_fscore: 66.44, time: 6.32 sec
loss: 3.9370837211608887
loss: 4.280301094055176
loss: 4.194998264312744
epoch: 32, train_loss: 4.093200206756592, train_acc: 69.23, train_fscore: 68.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.195000171661377, test_acc: 66.17, test_fscore: 66.62, time: 6.31 sec
loss: 4.015623569488525
loss: 3.9557995796203613
loss: 4.1689910888671875
epoch: 33, train_loss: 3.9886999130249023, train_acc: 69.38, train_fscore: 68.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.169000148773193, test_acc: 66.97, test_fscore: 67.27, time: 6.13 sec
loss: 3.884673595428467
loss: 4.084600925445557
loss: 4.152693271636963
epoch: 34, train_loss: 3.977400064468384, train_acc: 70.15, train_fscore: 69.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.152699947357178, test_acc: 66.85, test_fscore: 67.21, time: 5.59 sec
loss: 3.8451626300811768
loss: 4.065056324005127
loss: 4.099535942077637
epoch: 35, train_loss: 3.949399948120117, train_acc: 69.72, train_fscore: 69.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.0995001792907715, test_acc: 67.22, test_fscore: 67.45, time: 6.18 sec
loss: 3.92474365234375
loss: 3.870506763458252
loss: 4.062474250793457
epoch: 36, train_loss: 3.8991000652313232, train_acc: 70.71, train_fscore: 70.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.0625, test_acc: 67.53, test_fscore: 67.87, time: 6.52 sec
loss: 3.881511926651001
loss: 3.861427068710327
loss: 4.026349067687988
epoch: 37, train_loss: 3.871799945831299, train_acc: 71.2, train_fscore: 70.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.026299953460693, test_acc: 67.41, test_fscore: 67.83, time: 6.46 sec
loss: 3.891200065612793
loss: 3.7206122875213623
loss: 3.982797145843506
epoch: 38, train_loss: 3.81030011177063, train_acc: 70.91, train_fscore: 70.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.982800006866455, test_acc: 67.65, test_fscore: 67.91, time: 5.79 sec
loss: 3.787017822265625
loss: 3.7718820571899414
loss: 3.9530389308929443
epoch: 39, train_loss: 3.779599905014038, train_acc: 71.03, train_fscore: 70.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.953000068664551, test_acc: 67.9, test_fscore: 68.03, time: 6.17 sec
loss: 3.665292739868164
loss: 3.8665144443511963
loss: 3.9268856048583984
epoch: 40, train_loss: 3.75600004196167, train_acc: 71.27, train_fscore: 70.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9268999099731445, test_acc: 67.9, test_fscore: 68.15, time: 6.08 sec
              precision    recall  f1-score   support

           0     0.4530    0.5694    0.5046     144.0
           1     0.7709    0.7143    0.7415     245.0
           2     0.6789    0.6276    0.6522     384.0
           3     0.6077    0.7471    0.6702     170.0
           4     0.8322    0.7960    0.8137     299.0
           5     0.6548    0.6273    0.6408     381.0

    accuracy                         0.6790    1623.0
   macro avg     0.6662    0.6803    0.6705    1623.0
weighted avg     0.6879    0.6790    0.6815    1623.0

[[ 82.   8.  15.   4.  34.   1.]
 [  5. 175.  30.   0.   0.  35.]
 [ 41.  22. 241.  23.   9.  48.]
 [  0.   0.   2. 127.   0.  41.]
 [ 52.   0.   8.   0. 238.   1.]
 [  1.  22.  59.  55.   5. 239.]]
loss: 3.842654228210449
loss: 3.555007219314575
loss: 3.9047951698303223
epoch: 41, train_loss: 3.7123000621795654, train_acc: 72.41, train_fscore: 72.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9047999382019043, test_acc: 67.78, test_fscore: 68.11, time: 5.71 sec
loss: 3.6347131729125977
loss: 3.7359776496887207
loss: 3.8717591762542725
epoch: 42, train_loss: 3.6807000637054443, train_acc: 71.96, train_fscore: 71.63, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.871799945831299, test_acc: 67.34, test_fscore: 67.93, time: 7.25 sec
loss: 3.553391456604004
loss: 3.778903007507324
loss: 3.8361399173736572
epoch: 43, train_loss: 3.654599905014038, train_acc: 72.53, train_fscore: 72.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8361001014709473, test_acc: 67.78, test_fscore: 68.28, time: 6.44 sec
loss: 3.6385507583618164
loss: 3.5421457290649414
loss: 3.8116707801818848
epoch: 44, train_loss: 3.5966999530792236, train_acc: 72.98, train_fscore: 72.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8117001056671143, test_acc: 68.45, test_fscore: 68.71, time: 6.7 sec
loss: 3.7485392093658447
loss: 3.3466949462890625
loss: 3.806082248687744
epoch: 45, train_loss: 3.5631000995635986, train_acc: 72.79, train_fscore: 72.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8060998916625977, test_acc: 68.64, test_fscore: 68.84, time: 6.56 sec
loss: 3.579728126525879
loss: 3.5603904724121094
loss: 3.7871012687683105
epoch: 46, train_loss: 3.57069993019104, train_acc: 72.77, train_fscore: 72.43, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.787100076675415, test_acc: 68.15, test_fscore: 68.54, time: 5.76 sec
loss: 3.546535015106201
loss: 3.511943817138672
loss: 3.770827531814575
epoch: 47, train_loss: 3.5304999351501465, train_acc: 73.72, train_fscore: 73.43, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7708001136779785, test_acc: 68.39, test_fscore: 68.71, time: 6.11 sec
loss: 3.493701934814453
loss: 3.534482479095459
loss: 3.7498199939727783
epoch: 48, train_loss: 3.512700080871582, train_acc: 74.11, train_fscore: 73.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.749799966812134, test_acc: 68.39, test_fscore: 68.85, time: 7.01 sec
loss: 3.4584789276123047
loss: 3.477588415145874
loss: 3.7291512489318848
epoch: 49, train_loss: 3.467600107192993, train_acc: 73.7, train_fscore: 73.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7291998863220215, test_acc: 68.39, test_fscore: 68.84, time: 5.37 sec
loss: 3.4733572006225586
loss: 3.428877830505371
loss: 3.730254888534546
epoch: 50, train_loss: 3.4523000717163086, train_acc: 73.43, train_fscore: 73.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.730299949645996, test_acc: 68.7, test_fscore: 68.99, time: 6.82 sec
              precision    recall  f1-score   support

           0     0.4737    0.6250    0.5389     144.0
           1     0.7826    0.7347    0.7579     245.0
           2     0.7112    0.6094    0.6564     384.0
           3     0.6172    0.7588    0.6807     170.0
           4     0.8417    0.7826    0.8111     299.0
           5     0.6408    0.6509    0.6458     381.0

    accuracy                         0.6870    1623.0
   macro avg     0.6779    0.6936    0.6818    1623.0
weighted avg     0.6986    0.6870    0.6899    1623.0

[[ 90.   8.  13.   2.  29.   2.]
 [  5. 180.  22.   0.   0.  38.]
 [ 40.  21. 234.  22.   9.  58.]
 [  0.   0.   1. 129.   0.  40.]
 [ 55.   1.   8.   0. 234.   1.]
 [  0.  20.  51.  56.   6. 248.]]
loss: 3.380887031555176
loss: 3.4660027027130127
loss: 3.7076587677001953
epoch: 51, train_loss: 3.420799970626831, train_acc: 74.2, train_fscore: 73.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.70770001411438, test_acc: 68.58, test_fscore: 68.94, time: 6.32 sec
loss: 3.3562839031219482
loss: 3.4432806968688965
loss: 3.670119524002075
epoch: 52, train_loss: 3.3970000743865967, train_acc: 74.15, train_fscore: 73.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.670099973678589, test_acc: 68.82, test_fscore: 69.24, time: 6.08 sec
loss: 3.302633047103882
loss: 3.451366424560547
loss: 3.6663424968719482
epoch: 53, train_loss: 3.369499921798706, train_acc: 74.8, train_fscore: 74.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.666300058364868, test_acc: 69.25, test_fscore: 69.67, time: 6.2 sec
loss: 3.2613542079925537
loss: 3.490746259689331
loss: 3.6351261138916016
epoch: 54, train_loss: 3.37719988822937, train_acc: 74.23, train_fscore: 73.97, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6350998878479004, test_acc: 69.19, test_fscore: 69.57, time: 5.83 sec
loss: 3.2721798419952393
loss: 3.423337936401367
loss: 3.6246066093444824
epoch: 55, train_loss: 3.341099977493286, train_acc: 74.89, train_fscore: 74.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6245999336242676, test_acc: 68.95, test_fscore: 69.39, time: 6.65 sec
loss: 3.396049737930298
loss: 3.25142240524292
loss: 3.6149709224700928
epoch: 56, train_loss: 3.329400062561035, train_acc: 75.2, train_fscore: 75.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.615000009536743, test_acc: 68.88, test_fscore: 69.27, time: 5.6 sec
loss: 3.298454761505127
loss: 3.2696077823638916
loss: 3.572369337081909
epoch: 57, train_loss: 3.2841999530792236, train_acc: 75.37, train_fscore: 75.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5724000930786133, test_acc: 68.95, test_fscore: 69.31, time: 6.52 sec
loss: 3.324826240539551
loss: 3.222381353378296
loss: 3.5503506660461426
epoch: 58, train_loss: 3.2772998809814453, train_acc: 75.09, train_fscore: 74.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5504000186920166, test_acc: 69.62, test_fscore: 70.04, time: 6.19 sec
loss: 3.1749210357666016
loss: 3.3500218391418457
loss: 3.5469472408294678
epoch: 59, train_loss: 3.2560999393463135, train_acc: 75.61, train_fscore: 75.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5469000339508057, test_acc: 69.44, test_fscore: 69.8, time: 6.65 sec
loss: 3.316847085952759
loss: 3.1359968185424805
loss: 3.540475845336914
epoch: 60, train_loss: 3.2327001094818115, train_acc: 75.52, train_fscore: 75.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5404999256134033, test_acc: 70.18, test_fscore: 70.53, time: 5.82 sec
              precision    recall  f1-score   support

           0     0.4794    0.6458    0.5503     144.0
           1     0.8136    0.7306    0.7699     245.0
           2     0.7188    0.6458    0.6804     384.0
           3     0.6341    0.7647    0.6933     170.0
           4     0.8484    0.7860    0.8160     299.0
           5     0.6649    0.6667    0.6658     381.0

    accuracy                         0.7018    1623.0
   macro avg     0.6932    0.7066    0.6959    1623.0
weighted avg     0.7142    0.7018    0.7053    1623.0

[[ 93.   8.  15.   0.  28.   0.]
 [  5. 179.  22.   0.   0.  39.]
 [ 41.  17. 248.  21.   8.  49.]
 [  0.   0.   1. 130.   0.  39.]
 [ 54.   1.   8.   0. 235.   1.]
 [  1.  15.  51.  54.   6. 254.]]
loss: 3.191650152206421
loss: 3.262157917022705
loss: 3.5411059856414795
epoch: 61, train_loss: 3.224100112915039, train_acc: 76.09, train_fscore: 75.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.541100025177002, test_acc: 70.06, test_fscore: 70.56, time: 6.84 sec
loss: 3.07527232170105
loss: 3.3574705123901367
loss: 3.5430502891540527
epoch: 62, train_loss: 3.2083001136779785, train_acc: 75.78, train_fscore: 75.63, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.543100118637085, test_acc: 69.62, test_fscore: 70.1, time: 7.03 sec
loss: 3.3604836463928223
loss: 3.0219175815582275
loss: 3.5272269248962402
epoch: 63, train_loss: 3.2065999507904053, train_acc: 76.71, train_fscore: 76.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5271999835968018, test_acc: 69.69, test_fscore: 70.17, time: 5.73 sec
loss: 3.220848560333252
loss: 3.0998566150665283
loss: 3.5139389038085938
epoch: 64, train_loss: 3.1637001037597656, train_acc: 76.49, train_fscore: 76.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5139000415802, test_acc: 69.13, test_fscore: 69.55, time: 6.17 sec
loss: 3.1659116744995117
loss: 3.087362766265869
loss: 3.4806156158447266
epoch: 65, train_loss: 3.1322999000549316, train_acc: 76.71, train_fscore: 76.44, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.480600118637085, test_acc: 70.06, test_fscore: 70.45, time: 6.45 sec
loss: 3.066340446472168
loss: 3.188887357711792
loss: 3.4997997283935547
epoch: 66, train_loss: 3.1266000270843506, train_acc: 76.8, train_fscore: 76.62, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.499799966812134, test_acc: 69.93, test_fscore: 70.4, time: 5.99 sec
loss: 3.0869648456573486
loss: 3.1809191703796387
loss: 3.4708213806152344
epoch: 67, train_loss: 3.130500078201294, train_acc: 77.14, train_fscore: 77.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4707999229431152, test_acc: 69.93, test_fscore: 70.41, time: 6.21 sec
loss: 3.1625030040740967
loss: 3.036296844482422
loss: 3.4363462924957275
epoch: 68, train_loss: 3.1054999828338623, train_acc: 77.07, train_fscore: 76.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.436300039291382, test_acc: 70.18, test_fscore: 70.59, time: 5.71 sec
loss: 3.022412061691284
loss: 3.192974090576172
loss: 3.429537773132324
epoch: 69, train_loss: 3.1054000854492188, train_acc: 76.92, train_fscore: 76.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.429500102996826, test_acc: 70.73, test_fscore: 71.11, time: 6.71 sec
loss: 3.1113245487213135
loss: 3.027231216430664
loss: 3.4303674697875977
epoch: 70, train_loss: 3.0706000328063965, train_acc: 77.28, train_fscore: 77.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4303998947143555, test_acc: 70.3, test_fscore: 70.73, time: 5.88 sec
              precision    recall  f1-score   support

           0     0.4952    0.7222    0.5876     144.0
           1     0.8044    0.7388    0.7702     245.0
           2     0.7217    0.6484    0.6831     384.0
           3     0.6418    0.7588    0.6954     170.0
           4     0.8759    0.7793    0.8248     299.0
           5     0.6702    0.6614    0.6658     381.0

    accuracy                         0.7073    1623.0
   macro avg     0.7016    0.7182    0.7045    1623.0
weighted avg     0.7221    0.7073    0.7111    1623.0

[[104.   7.  12.   0.  21.   0.]
 [  5. 181.  24.   0.   0.  35.]
 [ 45.  18. 249.  19.   5.  48.]
 [  0.   0.   1. 129.   0.  40.]
 [ 55.   2.   8.   0. 233.   1.]
 [  1.  17.  51.  53.   7. 252.]]
loss: 3.0701301097869873
loss: 3.048478603363037
loss: 3.4303231239318848
epoch: 71, train_loss: 3.06030011177063, train_acc: 77.44, train_fscore: 77.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.430299997329712, test_acc: 70.43, test_fscore: 70.84, time: 6.45 sec
loss: 3.1196000576019287
loss: 2.9327845573425293
loss: 3.4212117195129395
epoch: 72, train_loss: 3.0353000164031982, train_acc: 77.54, train_fscore: 77.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4212000370025635, test_acc: 70.43, test_fscore: 70.82, time: 6.94 sec
loss: 3.0028958320617676
loss: 3.051178216934204
loss: 3.4027769565582275
epoch: 73, train_loss: 3.024899959564209, train_acc: 77.95, train_fscore: 77.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4028000831604004, test_acc: 70.55, test_fscore: 70.96, time: 6.04 sec
loss: 2.770946979522705
loss: 3.2765581607818604
loss: 3.4096057415008545
epoch: 74, train_loss: 3.0095999240875244, train_acc: 78.36, train_fscore: 78.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.409600019454956, test_acc: 70.55, test_fscore: 70.98, time: 6.4 sec
loss: 2.94191312789917
loss: 3.0463814735412598
loss: 3.4117367267608643
epoch: 75, train_loss: 2.990299940109253, train_acc: 78.06, train_fscore: 77.94, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4117000102996826, test_acc: 70.79, test_fscore: 71.19, time: 6.05 sec
loss: 3.0146732330322266
loss: 2.9256398677825928
loss: 3.3975133895874023
epoch: 76, train_loss: 2.976599931716919, train_acc: 77.92, train_fscore: 77.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3975000381469727, test_acc: 70.24, test_fscore: 70.68, time: 6.41 sec
loss: 3.050652503967285
loss: 2.8550848960876465
loss: 3.359794855117798
epoch: 77, train_loss: 2.9663000106811523, train_acc: 78.23, train_fscore: 78.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.359800100326538, test_acc: 70.86, test_fscore: 71.21, time: 6.06 sec
loss: 3.003980875015259
loss: 2.891460418701172
loss: 3.3674445152282715
epoch: 78, train_loss: 2.954699993133545, train_acc: 78.16, train_fscore: 78.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3673999309539795, test_acc: 70.98, test_fscore: 71.34, time: 7.52 sec
loss: 3.0677638053894043
loss: 2.7887685298919678
loss: 3.381293296813965
epoch: 79, train_loss: 2.93530011177063, train_acc: 78.61, train_fscore: 78.44, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3812999725341797, test_acc: 70.3, test_fscore: 70.77, time: 4.81 sec
loss: 2.8270254135131836
loss: 3.04490327835083
loss: 3.3684160709381104
epoch: 80, train_loss: 2.9312000274658203, train_acc: 78.69, train_fscore: 78.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3684000968933105, test_acc: 70.67, test_fscore: 71.08, time: 6.84 sec
              precision    recall  f1-score   support

           0     0.5095    0.7431    0.6045     144.0
           1     0.8178    0.7510    0.7830     245.0
           2     0.7175    0.6615    0.6883     384.0
           3     0.6396    0.7412    0.6866     170.0
           4     0.8759    0.7793    0.8248     299.0
           5     0.6685    0.6509    0.6596     381.0

    accuracy                         0.7098    1623.0
   macro avg     0.7048    0.7211    0.7078    1623.0
weighted avg     0.7237    0.7098    0.7134    1623.0

[[107.   5.  12.   0.  20.   0.]
 [  3. 184.  23.   0.   1.  34.]
 [ 45.  18. 254.  18.   4.  45.]
 [  0.   0.   1. 126.   0.  43.]
 [ 54.   2.   9.   0. 233.   1.]
 [  1.  16.  55.  53.   8. 248.]]
loss: 2.9119162559509277
loss: 2.893555164337158
loss: 3.3386199474334717
epoch: 81, train_loss: 2.903700113296509, train_acc: 78.92, train_fscore: 78.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.338599920272827, test_acc: 70.67, test_fscore: 71.01, time: 6.22 sec
loss: 2.957777976989746
loss: 2.8397669792175293
loss: 3.3418102264404297
epoch: 82, train_loss: 2.908099889755249, train_acc: 78.47, train_fscore: 78.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3417999744415283, test_acc: 70.79, test_fscore: 71.21, time: 6.62 sec
loss: 2.8924996852874756
loss: 2.9014461040496826
loss: 3.3433732986450195
epoch: 83, train_loss: 2.8968000411987305, train_acc: 79.0, train_fscore: 78.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.343400001525879, test_acc: 71.04, test_fscore: 71.39, time: 5.91 sec
loss: 2.8742947578430176
loss: 2.868802785873413
loss: 3.3401389122009277
epoch: 84, train_loss: 2.8719000816345215, train_acc: 79.23, train_fscore: 79.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.340100049972534, test_acc: 70.55, test_fscore: 70.97, time: 6.04 sec
loss: 2.875307321548462
loss: 2.8489840030670166
loss: 3.3298709392547607
epoch: 85, train_loss: 2.862799882888794, train_acc: 78.93, train_fscore: 78.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.329900026321411, test_acc: 70.79, test_fscore: 71.23, time: 6.41 sec
loss: 2.8772783279418945
loss: 2.844733715057373
loss: 3.337277889251709
epoch: 86, train_loss: 2.861599922180176, train_acc: 79.47, train_fscore: 79.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3373000621795654, test_acc: 71.16, test_fscore: 71.52, time: 5.59 sec
loss: 2.7730507850646973
loss: 2.901348352432251
loss: 3.335175037384033
epoch: 87, train_loss: 2.832900047302246, train_acc: 79.43, train_fscore: 79.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.335200071334839, test_acc: 70.73, test_fscore: 71.17, time: 6.43 sec
loss: 2.8278450965881348
loss: 2.8195302486419678
loss: 3.325653314590454
epoch: 88, train_loss: 2.823899984359741, train_acc: 79.85, train_fscore: 79.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.325700044631958, test_acc: 70.79, test_fscore: 71.17, time: 6.4 sec
loss: 2.7767958641052246
loss: 2.8626604080200195
loss: 3.313136577606201
epoch: 89, train_loss: 2.8173000812530518, train_acc: 79.31, train_fscore: 79.17, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3131000995635986, test_acc: 71.04, test_fscore: 71.43, time: 6.15 sec
loss: 2.7785582542419434
loss: 2.8429129123687744
loss: 3.312037944793701
epoch: 90, train_loss: 2.806999921798706, train_acc: 80.1, train_fscore: 79.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.312000036239624, test_acc: 70.86, test_fscore: 71.28, time: 6.07 sec
              precision    recall  f1-score   support

           0     0.5166    0.7569    0.6141     144.0
           1     0.8243    0.7469    0.7837     245.0
           2     0.7286    0.6641    0.6948     384.0
           3     0.6268    0.7706    0.6913     170.0
           4     0.8712    0.7692    0.8171     299.0
           5     0.6730    0.6483    0.6604     381.0

    accuracy                         0.7116    1623.0
   macro avg     0.7068    0.7260    0.7102    1623.0
weighted avg     0.7268    0.7116    0.7152    1623.0

[[109.   4.   8.   0.  23.   0.]
 [  3. 183.  21.   2.   1.  35.]
 [ 44.  17. 255.  18.   4.  46.]
 [  0.   0.   1. 131.   0.  38.]
 [ 54.   2.  12.   0. 230.   1.]
 [  1.  16.  53.  58.   6. 247.]]
loss: 2.859632968902588
loss: 2.697120189666748
loss: 3.3425164222717285
epoch: 91, train_loss: 2.783400058746338, train_acc: 80.15, train_fscore: 80.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3424999713897705, test_acc: 70.55, test_fscore: 70.95, time: 6.57 sec
loss: 2.816376209259033
loss: 2.762035608291626
loss: 3.3422470092773438
epoch: 92, train_loss: 2.7904000282287598, train_acc: 80.62, train_fscore: 80.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3422000408172607, test_acc: 70.61, test_fscore: 71.02, time: 6.75 sec
loss: 2.65058970451355
loss: 2.9009320735931396
loss: 3.306643486022949
epoch: 93, train_loss: 2.773400068283081, train_acc: 79.83, train_fscore: 79.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3066000938415527, test_acc: 70.24, test_fscore: 70.67, time: 5.66 sec
loss: 2.801866054534912
loss: 2.7347235679626465
loss: 3.2910304069519043
epoch: 94, train_loss: 2.7699999809265137, train_acc: 80.91, train_fscore: 80.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2909998893737793, test_acc: 70.61, test_fscore: 71.02, time: 6.32 sec
loss: 2.7670185565948486
loss: 2.693570375442505
loss: 3.3079776763916016
epoch: 95, train_loss: 2.734100103378296, train_acc: 80.57, train_fscore: 80.43, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.308000087738037, test_acc: 70.36, test_fscore: 70.79, time: 6.08 sec
loss: 2.754862070083618
loss: 2.7106552124023438
loss: 3.319371461868286
epoch: 96, train_loss: 2.734800100326538, train_acc: 80.5, train_fscore: 80.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3194000720977783, test_acc: 70.43, test_fscore: 70.84, time: 6.44 sec
loss: 2.747141122817993
loss: 2.745222330093384
loss: 3.318993091583252
epoch: 97, train_loss: 2.746299982070923, train_acc: 80.55, train_fscore: 80.44, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.319000005722046, test_acc: 70.43, test_fscore: 70.83, time: 6.9 sec
loss: 2.6750311851501465
loss: 2.7982029914855957
loss: 3.308854818344116
epoch: 98, train_loss: 2.731800079345703, train_acc: 80.41, train_fscore: 80.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3089001178741455, test_acc: 70.73, test_fscore: 71.13, time: 6.12 sec
loss: 2.6128101348876953
loss: 2.8203420639038086
loss: 3.3106913566589355
epoch: 99, train_loss: 2.7114999294281006, train_acc: 81.02, train_fscore: 80.9, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.310699939727783, test_acc: 70.92, test_fscore: 71.29, time: 6.4 sec
loss: 2.615309715270996
loss: 2.801861047744751
loss: 3.29024076461792
epoch: 100, train_loss: 2.703000068664551, train_acc: 81.05, train_fscore: 80.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2901999950408936, test_acc: 70.79, test_fscore: 71.2, time: 6.49 sec
              precision    recall  f1-score   support

           0     0.5166    0.7569    0.6141     144.0
           1     0.8243    0.7469    0.7837     245.0
           2     0.7286    0.6641    0.6948     384.0
           3     0.6268    0.7706    0.6913     170.0
           4     0.8712    0.7692    0.8171     299.0
           5     0.6730    0.6483    0.6604     381.0

    accuracy                         0.7116    1623.0
   macro avg     0.7068    0.7260    0.7102    1623.0
weighted avg     0.7268    0.7116    0.7152    1623.0

[[109.   4.   8.   0.  23.   0.]
 [  3. 183.  21.   2.   1.  35.]
 [ 44.  17. 255.  18.   4.  46.]
 [  0.   0.   1. 131.   0.  38.]
 [ 54.   2.  12.   0. 230.   1.]
 [  1.  16.  53.  58.   6. 247.]]
loss: 2.755689859390259
loss: 2.5886332988739014
loss: 3.2817370891571045
epoch: 101, train_loss: 2.6772000789642334, train_acc: 81.15, train_fscore: 81.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2816998958587646, test_acc: 70.79, test_fscore: 71.21, time: 6.82 sec
loss: 2.661348581314087
loss: 2.7326414585113525
loss: 3.293031930923462
epoch: 102, train_loss: 2.6933999061584473, train_acc: 81.43, train_fscore: 81.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2929999828338623, test_acc: 70.67, test_fscore: 71.13, time: 6.13 sec
loss: 2.6584630012512207
loss: 2.6704797744750977
loss: 3.3068909645080566
epoch: 103, train_loss: 2.664400100708008, train_acc: 81.38, train_fscore: 81.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3069000244140625, test_acc: 70.61, test_fscore: 71.0, time: 6.67 sec
loss: 2.624769449234009
loss: 2.696408987045288
loss: 3.288257360458374
epoch: 104, train_loss: 2.6593000888824463, train_acc: 81.43, train_fscore: 81.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.288300037384033, test_acc: 70.61, test_fscore: 71.01, time: 5.9 sec
loss: 2.5593056678771973
loss: 2.7648470401763916
loss: 3.3257391452789307
epoch: 105, train_loss: 2.6594998836517334, train_acc: 81.15, train_fscore: 81.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.325700044631958, test_acc: 70.86, test_fscore: 71.25, time: 6.61 sec
loss: 2.5578503608703613
loss: 2.7825589179992676
loss: 3.2926599979400635
epoch: 106, train_loss: 2.6628000736236572, train_acc: 81.38, train_fscore: 81.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2927000522613525, test_acc: 70.79, test_fscore: 71.23, time: 5.54 sec
loss: 2.6215548515319824
loss: 2.6501219272613525
loss: 3.28003191947937
epoch: 107, train_loss: 2.635200023651123, train_acc: 82.0, train_fscore: 81.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2799999713897705, test_acc: 71.23, test_fscore: 71.64, time: 6.01 sec
loss: 2.711469888687134
loss: 2.487070083618164
loss: 3.31634259223938
epoch: 108, train_loss: 2.6071999073028564, train_acc: 81.94, train_fscore: 81.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3162999153137207, test_acc: 71.35, test_fscore: 71.74, time: 6.18 sec
loss: 2.4883005619049072
loss: 2.739656925201416
loss: 3.260686159133911
epoch: 109, train_loss: 2.611299991607666, train_acc: 82.07, train_fscore: 81.97, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.260699987411499, test_acc: 71.29, test_fscore: 71.72, time: 6.42 sec
loss: 2.547163963317871
loss: 2.6560497283935547
loss: 3.259474515914917
epoch: 110, train_loss: 2.6001999378204346, train_acc: 81.31, train_fscore: 81.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.259500026702881, test_acc: 71.41, test_fscore: 71.81, time: 6.46 sec
              precision    recall  f1-score   support

           0     0.5045    0.7708    0.6099     144.0
           1     0.8333    0.7755    0.8034     245.0
           2     0.7166    0.6849    0.7004     384.0
           3     0.6614    0.7353    0.6964     170.0
           4     0.8725    0.7324    0.7964     299.0
           5     0.6821    0.6588    0.6702     381.0

    accuracy                         0.7141    1623.0
   macro avg     0.7117    0.7263    0.7128    1623.0
weighted avg     0.7302    0.7141    0.7181    1623.0

[[111.   5.   7.   0.  21.   0.]
 [  2. 190.  21.   1.   1.  30.]
 [ 43.  18. 263.  15.   3.  42.]
 [  0.   0.   1. 125.   0.  44.]
 [ 63.   1.  15.   0. 219.   1.]
 [  1.  14.  60.  48.   7. 251.]]
loss: 2.5856804847717285
loss: 2.6465578079223633
loss: 3.2976326942443848
epoch: 111, train_loss: 2.6143999099731445, train_acc: 81.86, train_fscore: 81.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.297600030899048, test_acc: 70.43, test_fscore: 70.9, time: 7.02 sec
loss: 2.3929431438446045
loss: 2.779583215713501
loss: 3.2990806102752686
epoch: 112, train_loss: 2.5754001140594482, train_acc: 82.15, train_fscore: 82.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.299099922180176, test_acc: 70.36, test_fscore: 70.76, time: 5.72 sec
loss: 2.625896453857422
loss: 2.5651967525482178
loss: 3.328479290008545
epoch: 113, train_loss: 2.5964999198913574, train_acc: 82.29, train_fscore: 82.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3285000324249268, test_acc: 71.29, test_fscore: 71.72, time: 6.66 sec
loss: 2.523378849029541
loss: 2.614166021347046
loss: 3.2812511920928955
epoch: 114, train_loss: 2.563499927520752, train_acc: 82.08, train_fscore: 82.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2813000679016113, test_acc: 70.67, test_fscore: 71.16, time: 5.61 sec
loss: 2.555861234664917
loss: 2.5686182975769043
loss: 3.2822940349578857
epoch: 115, train_loss: 2.5617001056671143, train_acc: 82.12, train_fscore: 81.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2822999954223633, test_acc: 71.35, test_fscore: 71.75, time: 6.62 sec
loss: 2.4970006942749023
loss: 2.6082425117492676
loss: 3.2854621410369873
epoch: 116, train_loss: 2.5501999855041504, train_acc: 82.43, train_fscore: 82.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2855000495910645, test_acc: 70.73, test_fscore: 71.14, time: 6.4 sec
loss: 2.6255123615264893
loss: 2.453558921813965
loss: 3.2740933895111084
epoch: 117, train_loss: 2.5441999435424805, train_acc: 82.74, train_fscore: 82.64, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2741000652313232, test_acc: 70.98, test_fscore: 71.43, time: 5.93 sec
loss: 2.4787721633911133
loss: 2.613792657852173
loss: 3.30521821975708
epoch: 118, train_loss: 2.539599895477295, train_acc: 82.93, train_fscore: 82.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3052000999450684, test_acc: 71.1, test_fscore: 71.5, time: 6.38 sec
loss: 2.5467686653137207
loss: 2.4497451782226562
loss: 3.3115735054016113
epoch: 119, train_loss: 2.503700017929077, train_acc: 82.65, train_fscore: 82.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3115999698638916, test_acc: 70.67, test_fscore: 71.16, time: 5.71 sec
loss: 2.5617315769195557
loss: 2.4614086151123047
loss: 3.261734962463379
epoch: 120, train_loss: 2.5167999267578125, train_acc: 82.67, train_fscore: 82.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.261699914932251, test_acc: 71.1, test_fscore: 71.48, time: 4.1 sec
              precision    recall  f1-score   support

           0     0.5045    0.7708    0.6099     144.0
           1     0.8333    0.7755    0.8034     245.0
           2     0.7166    0.6849    0.7004     384.0
           3     0.6614    0.7353    0.6964     170.0
           4     0.8725    0.7324    0.7964     299.0
           5     0.6821    0.6588    0.6702     381.0

    accuracy                         0.7141    1623.0
   macro avg     0.7117    0.7263    0.7128    1623.0
weighted avg     0.7302    0.7141    0.7181    1623.0

[[111.   5.   7.   0.  21.   0.]
 [  2. 190.  21.   1.   1.  30.]
 [ 43.  18. 263.  15.   3.  42.]
 [  0.   0.   1. 125.   0.  44.]
 [ 63.   1.  15.   0. 219.   1.]
 [  1.  14.  60.  48.   7. 251.]]
loss: 2.357451915740967
loss: 2.655656576156616
loss: 3.267474889755249
epoch: 121, train_loss: 2.506200075149536, train_acc: 83.15, train_fscore: 83.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2674999237060547, test_acc: 71.47, test_fscore: 71.87, time: 6.33 sec
loss: 2.6691386699676514
loss: 2.299142837524414
loss: 3.2903707027435303
epoch: 122, train_loss: 2.5078001022338867, train_acc: 82.82, train_fscore: 82.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2904000282287598, test_acc: 70.92, test_fscore: 71.4, time: 6.09 sec
loss: 2.5323398113250732
loss: 2.444228172302246
loss: 3.328859806060791
epoch: 123, train_loss: 2.4916999340057373, train_acc: 83.2, train_fscore: 83.12, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.328900098800659, test_acc: 71.23, test_fscore: 71.65, time: 6.18 sec
loss: 2.5880982875823975
loss: 2.396263360977173
loss: 3.2949888706207275
epoch: 124, train_loss: 2.5004000663757324, train_acc: 83.92, train_fscore: 83.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2950000762939453, test_acc: 70.86, test_fscore: 71.27, time: 6.71 sec
loss: 2.52083420753479
loss: 2.4363150596618652
loss: 3.2753190994262695
epoch: 125, train_loss: 2.4804999828338623, train_acc: 83.56, train_fscore: 83.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2753000259399414, test_acc: 71.1, test_fscore: 71.5, time: 5.85 sec
loss: 2.509321689605713
loss: 2.44319486618042
loss: 3.258871555328369
epoch: 126, train_loss: 2.4804000854492188, train_acc: 83.39, train_fscore: 83.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2588999271392822, test_acc: 71.35, test_fscore: 71.71, time: 6.64 sec
loss: 2.476325273513794
loss: 2.4044296741485596
loss: 3.2781119346618652
epoch: 127, train_loss: 2.4433999061584473, train_acc: 83.53, train_fscore: 83.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.27810001373291, test_acc: 71.04, test_fscore: 71.47, time: 5.92 sec
loss: 2.4054951667785645
loss: 2.5585391521453857
loss: 3.3181097507476807
epoch: 128, train_loss: 2.4755001068115234, train_acc: 83.27, train_fscore: 83.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3180999755859375, test_acc: 70.79, test_fscore: 71.22, time: 6.24 sec
loss: 2.3863978385925293
loss: 2.5187010765075684
loss: 3.2949471473693848
epoch: 129, train_loss: 2.4477999210357666, train_acc: 83.79, train_fscore: 83.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2948999404907227, test_acc: 71.35, test_fscore: 71.72, time: 6.56 sec
loss: 2.4619221687316895
loss: 2.379220485687256
loss: 3.287365198135376
epoch: 130, train_loss: 2.423099994659424, train_acc: 83.87, train_fscore: 83.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.287400007247925, test_acc: 70.98, test_fscore: 71.42, time: 6.4 sec
              precision    recall  f1-score   support

           0     0.5093    0.7639    0.6111     144.0
           1     0.8407    0.7755    0.8068     245.0
           2     0.7178    0.6823    0.6996     384.0
           3     0.6546    0.7471    0.6978     170.0
           4     0.8720    0.7291    0.7942     299.0
           5     0.6801    0.6640    0.6720     381.0

    accuracy                         0.7147    1623.0
   macro avg     0.7124    0.7270    0.7136    1623.0
weighted avg     0.7308    0.7147    0.7187    1623.0

[[110.   4.   6.   0.  24.   0.]
 [  2. 190.  20.   1.   0.  32.]
 [ 42.  17. 262.  16.   3.  44.]
 [  0.   0.   1. 127.   0.  42.]
 [ 61.   1.  18.   0. 218.   1.]
 [  1.  14.  58.  50.   5. 253.]]
loss: 2.363015651702881
loss: 2.4732248783111572
loss: 3.293339729309082
epoch: 131, train_loss: 2.416300058364868, train_acc: 83.84, train_fscore: 83.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.293299913406372, test_acc: 71.53, test_fscore: 71.92, time: 6.3 sec
loss: 2.4752888679504395
loss: 2.369138479232788
loss: 3.3067023754119873
epoch: 132, train_loss: 2.427999973297119, train_acc: 84.2, train_fscore: 84.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3066999912261963, test_acc: 71.1, test_fscore: 71.5, time: 6.67 sec
loss: 2.3112077713012695
loss: 2.5283970832824707
loss: 3.3530521392822266
epoch: 133, train_loss: 2.413100004196167, train_acc: 84.53, train_fscore: 84.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.353100061416626, test_acc: 70.92, test_fscore: 71.38, time: 5.74 sec
loss: 2.4737565517425537
loss: 2.394500732421875
loss: 3.31523060798645
epoch: 134, train_loss: 2.4351999759674072, train_acc: 83.79, train_fscore: 83.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.315200090408325, test_acc: 71.35, test_fscore: 71.77, time: 6.75 sec
loss: 2.497797966003418
loss: 2.297049045562744
loss: 3.2708520889282227
epoch: 135, train_loss: 2.4022998809814453, train_acc: 84.39, train_fscore: 84.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.270900011062622, test_acc: 70.98, test_fscore: 71.39, time: 5.22 sec
loss: 2.3680508136749268
loss: 2.4327900409698486
loss: 3.306915760040283
epoch: 136, train_loss: 2.3975000381469727, train_acc: 84.11, train_fscore: 83.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3069000244140625, test_acc: 70.73, test_fscore: 71.16, time: 6.62 sec
loss: 2.355839729309082
loss: 2.436805248260498
loss: 3.2978787422180176
epoch: 137, train_loss: 2.3924999237060547, train_acc: 84.39, train_fscore: 84.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2978999614715576, test_acc: 71.53, test_fscore: 71.99, time: 6.1 sec
loss: 2.33768367767334
loss: 2.3621392250061035
loss: 3.2901241779327393
epoch: 138, train_loss: 2.349299907684326, train_acc: 84.46, train_fscore: 84.37, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.29010009765625, test_acc: 71.29, test_fscore: 71.7, time: 6.36 sec
loss: 2.2064802646636963
loss: 2.6044347286224365
loss: 3.3371798992156982
epoch: 139, train_loss: 2.3959999084472656, train_acc: 84.65, train_fscore: 84.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3371999263763428, test_acc: 71.9, test_fscore: 72.3, time: 4.28 sec
loss: 2.4107506275177
loss: 2.318274736404419
loss: 3.2966465950012207
epoch: 140, train_loss: 2.36929988861084, train_acc: 84.34, train_fscore: 84.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.296600103378296, test_acc: 71.35, test_fscore: 71.75, time: 5.78 sec
              precision    recall  f1-score   support

           0     0.5090    0.7847    0.6175     144.0
           1     0.8421    0.7837    0.8118     245.0
           2     0.7184    0.7109    0.7147     384.0
           3     0.6447    0.7471    0.6921     170.0
           4     0.8802    0.7124    0.7874     299.0
           5     0.7034    0.6535    0.6776     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7163    0.7321    0.7168    1623.0
weighted avg     0.7371    0.7190    0.7230    1623.0

[[113.   4.   6.   0.  21.   0.]
 [  2. 192.  20.   3.   0.  28.]
 [ 41.  17. 273.  16.   3.  34.]
 [  0.   0.   1. 127.   0.  42.]
 [ 65.   1.  19.   0. 213.   1.]
 [  1.  14.  61.  51.   5. 249.]]
loss: 2.2936758995056152
loss: 2.4345085620880127
loss: 3.3249619007110596
epoch: 141, train_loss: 2.356600046157837, train_acc: 84.41, train_fscore: 84.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.325000047683716, test_acc: 71.1, test_fscore: 71.52, time: 6.54 sec
loss: 2.346318006515503
loss: 2.3412885665893555
loss: 3.3606860637664795
epoch: 142, train_loss: 2.3440001010894775, train_acc: 84.75, train_fscore: 84.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3606998920440674, test_acc: 71.47, test_fscore: 71.92, time: 6.3 sec
loss: 2.2720589637756348
loss: 2.4000284671783447
loss: 3.3182718753814697
epoch: 143, train_loss: 2.33240008354187, train_acc: 84.73, train_fscore: 84.64, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3183000087738037, test_acc: 71.66, test_fscore: 72.14, time: 6.05 sec
loss: 2.4048004150390625
loss: 2.285611867904663
loss: 3.312347888946533
epoch: 144, train_loss: 2.3499999046325684, train_acc: 84.89, train_fscore: 84.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.312299966812134, test_acc: 71.23, test_fscore: 71.6, time: 6.13 sec
loss: 2.3714733123779297
loss: 2.321897506713867
loss: 3.3064169883728027
epoch: 145, train_loss: 2.3482000827789307, train_acc: 84.85, train_fscore: 84.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3064000606536865, test_acc: 71.72, test_fscore: 72.12, time: 6.91 sec
loss: 2.288902759552002
loss: 2.3592894077301025
loss: 3.2998173236846924
epoch: 146, train_loss: 2.3236000537872314, train_acc: 85.22, train_fscore: 85.12, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.299799919128418, test_acc: 71.23, test_fscore: 71.66, time: 6.09 sec
loss: 2.3583571910858154
loss: 2.2667691707611084
loss: 3.3299717903137207
epoch: 147, train_loss: 2.315700054168701, train_acc: 85.16, train_fscore: 85.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3299999237060547, test_acc: 71.6, test_fscore: 72.02, time: 5.93 sec
loss: 2.2113935947418213
loss: 2.418581485748291
loss: 3.355238914489746
epoch: 148, train_loss: 2.3085999488830566, train_acc: 85.18, train_fscore: 85.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3552000522613525, test_acc: 70.67, test_fscore: 71.1, time: 5.36 sec
loss: 2.234778881072998
loss: 2.3844594955444336
loss: 3.357431650161743
epoch: 149, train_loss: 2.3076999187469482, train_acc: 85.03, train_fscore: 84.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3573999404907227, test_acc: 70.92, test_fscore: 71.37, time: 5.58 sec
loss: 2.290996789932251
loss: 2.310175657272339
loss: 3.332782030105591
epoch: 150, train_loss: 2.299799919128418, train_acc: 85.54, train_fscore: 85.44, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3327999114990234, test_acc: 70.98, test_fscore: 71.35, time: 6.57 sec
              precision    recall  f1-score   support

           0     0.5090    0.7847    0.6175     144.0
           1     0.8421    0.7837    0.8118     245.0
           2     0.7184    0.7109    0.7147     384.0
           3     0.6447    0.7471    0.6921     170.0
           4     0.8802    0.7124    0.7874     299.0
           5     0.7034    0.6535    0.6776     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7163    0.7321    0.7168    1623.0
weighted avg     0.7371    0.7190    0.7230    1623.0

[[113.   4.   6.   0.  21.   0.]
 [  2. 192.  20.   3.   0.  28.]
 [ 41.  17. 273.  16.   3.  34.]
 [  0.   0.   1. 127.   0.  42.]
 [ 65.   1.  19.   0. 213.   1.]
 [  1.  14.  61.  51.   5. 249.]]
Test performance..
F-Score: 72.3
F-Score-index: 139
              precision    recall  f1-score   support

           0     0.5090    0.7847    0.6175     144.0
           1     0.8421    0.7837    0.8118     245.0
           2     0.7184    0.7109    0.7147     384.0
           3     0.6447    0.7471    0.6921     170.0
           4     0.8802    0.7124    0.7874     299.0
           5     0.7034    0.6535    0.6776     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7163    0.7321    0.7168    1623.0
weighted avg     0.7371    0.7190    0.7230    1623.0

[[113.   4.   6.   0.  21.   0.]
 [  2. 192.  20.   3.   0.  28.]
 [ 41.  17. 273.  16.   3.  34.]
 [  0.   0.   1. 127.   0.  42.]
 [ 65.   1.  19.   0. 213.   1.]
 [  1.  14.  61.  51.   5. 249.]]
--- 2 ---
Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.5, batch_size=64, hidden_dim=1024, n_head=8, epochs=150, temp=2, tensorboard=False, class_weight=True, Dataset='IEMOCAP')
Running on GPU
temp 2
total parameters: 79687704
training parameters: 79687704
loss: 7.979472637176514
loss: 12.04023265838623
loss: 7.773510932922363
epoch: 1, train_loss: 9.89799976348877, train_acc: 17.5, train_fscore: 17.66, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.773499965667725, test_acc: 25.02, test_fscore: 17.46, time: 7.73 sec
loss: 7.959342956542969
loss: 8.363065719604492
loss: 8.543088912963867
epoch: 2, train_loss: 8.149200439453125, train_acc: 26.9, train_fscore: 22.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.543100357055664, test_acc: 15.65, test_fscore: 10.58, time: 6.77 sec
loss: 8.85054874420166
loss: 7.701310634613037
loss: 6.917954444885254
epoch: 3, train_loss: 8.315099716186523, train_acc: 30.77, train_fscore: 25.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.918000221252441, test_acc: 33.21, test_fscore: 30.24, time: 5.75 sec
loss: 7.119807243347168
loss: 7.173225402832031
loss: 7.278882026672363
epoch: 4, train_loss: 7.144000053405762, train_acc: 37.81, train_fscore: 34.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.278900146484375, test_acc: 46.95, test_fscore: 36.6, time: 6.35 sec
loss: 7.451732635498047
loss: 7.231541633605957
loss: 6.948053359985352
epoch: 5, train_loss: 7.352200031280518, train_acc: 43.68, train_fscore: 36.64, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.9481000900268555, test_acc: 48.31, test_fscore: 46.91, time: 5.85 sec
loss: 7.062306880950928
loss: 6.6568145751953125
loss: 6.584079742431641
epoch: 6, train_loss: 6.877399921417236, train_acc: 47.92, train_fscore: 47.51, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.584099769592285, test_acc: 38.39, test_fscore: 36.6, time: 6.91 sec
loss: 6.579785346984863
loss: 6.650852203369141
loss: 6.65728235244751
epoch: 7, train_loss: 6.613399982452393, train_acc: 45.96, train_fscore: 43.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.657299995422363, test_acc: 37.77, test_fscore: 32.7, time: 5.24 sec
loss: 6.542057991027832
loss: 6.7110371589660645
loss: 6.531510353088379
epoch: 8, train_loss: 6.6234002113342285, train_acc: 49.1, train_fscore: 44.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.531499862670898, test_acc: 44.61, test_fscore: 41.18, time: 6.16 sec
loss: 6.429163932800293
loss: 6.322563648223877
loss: 6.30453634262085
epoch: 9, train_loss: 6.377699851989746, train_acc: 56.13, train_fscore: 54.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.304500102996826, test_acc: 55.88, test_fscore: 56.03, time: 6.17 sec
loss: 6.240166664123535
loss: 6.044337749481201
loss: 6.23342752456665
epoch: 10, train_loss: 6.155799865722656, train_acc: 59.54, train_fscore: 58.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.233399868011475, test_acc: 54.59, test_fscore: 52.29, time: 6.18 sec
              precision    recall  f1-score   support

           0     0.2640    0.4583    0.3350     144.0
           1     0.6677    0.8449    0.7459     245.0
           2     0.5609    0.3958    0.4641     384.0
           3     0.5470    0.5824    0.5641     170.0
           4     0.7034    0.5552    0.6206     299.0
           5     0.5787    0.5696    0.5741     381.0

    accuracy                         0.5588    1623.0
   macro avg     0.5536    0.5677    0.5506    1623.0
weighted avg     0.5796    0.5588    0.5603    1623.0

[[ 66.  15.  12.   4.  46.   1.]
 [ 10. 207.   9.   3.   1.  15.]
 [ 83.  45. 152.  10.   8.  86.]
 [  3.   6.  13.  99.   8.  41.]
 [ 74.   8.  31.   5. 166.  15.]
 [ 14.  29.  54.  60.   7. 217.]]
loss: 6.084288120269775
loss: 6.022773742675781
loss: 6.074474811553955
epoch: 11, train_loss: 6.05620002746582, train_acc: 59.14, train_fscore: 57.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.07450008392334, test_acc: 53.85, test_fscore: 51.75, time: 7.4 sec
loss: 5.982131004333496
loss: 5.648553848266602
loss: 5.7938055992126465
epoch: 12, train_loss: 5.837100028991699, train_acc: 59.28, train_fscore: 57.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.793799877166748, test_acc: 57.36, test_fscore: 56.48, time: 6.41 sec
loss: 5.494540691375732
loss: 5.6872477531433105
loss: 5.599881649017334
epoch: 13, train_loss: 5.582600116729736, train_acc: 61.76, train_fscore: 60.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.599899768829346, test_acc: 59.46, test_fscore: 59.2, time: 6.11 sec
loss: 5.433876037597656
loss: 5.4069108963012695
loss: 5.417142391204834
epoch: 14, train_loss: 5.421500205993652, train_acc: 62.65, train_fscore: 61.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.417099952697754, test_acc: 60.07, test_fscore: 59.78, time: 6.93 sec
loss: 5.240157604217529
loss: 5.2971930503845215
loss: 5.22630500793457
epoch: 15, train_loss: 5.267499923706055, train_acc: 62.67, train_fscore: 61.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.22629976272583, test_acc: 62.42, test_fscore: 61.69, time: 5.48 sec
loss: 5.226757526397705
loss: 4.958449840545654
loss: 5.094388484954834
epoch: 16, train_loss: 5.103099822998047, train_acc: 63.29, train_fscore: 61.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.094399929046631, test_acc: 63.4, test_fscore: 63.25, time: 6.53 sec
loss: 5.104872703552246
loss: 4.854630470275879
loss: 4.970138072967529
epoch: 17, train_loss: 4.989200115203857, train_acc: 64.1, train_fscore: 62.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.970099925994873, test_acc: 62.72, test_fscore: 63.05, time: 6.04 sec
loss: 5.063460350036621
loss: 4.645917892456055
loss: 4.8215155601501465
epoch: 18, train_loss: 4.873600006103516, train_acc: 65.08, train_fscore: 64.24, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.821499824523926, test_acc: 63.09, test_fscore: 63.55, time: 6.95 sec
loss: 4.710134506225586
loss: 4.81190824508667
loss: 4.710542678833008
epoch: 19, train_loss: 4.756999969482422, train_acc: 65.16, train_fscore: 64.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.7104997634887695, test_acc: 63.09, test_fscore: 63.69, time: 6.6 sec
loss: 4.6635212898254395
loss: 4.717224597930908
loss: 4.6463303565979
epoch: 20, train_loss: 4.687600135803223, train_acc: 65.49, train_fscore: 64.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.646299839019775, test_acc: 63.4, test_fscore: 64.12, time: 5.99 sec
              precision    recall  f1-score   support

           0     0.3485    0.5833    0.4364     144.0
           1     0.7233    0.7469    0.7349     245.0
           2     0.6667    0.5417    0.5977     384.0
           3     0.5688    0.7294    0.6392     170.0
           4     0.8622    0.6488    0.7405     299.0
           5     0.6310    0.6194    0.6252     381.0

    accuracy                         0.6340    1623.0
   macro avg     0.6334    0.6449    0.6290    1623.0
weighted avg     0.6644    0.6340    0.6412    1623.0

[[ 84.  14.  12.   5.  28.   1.]
 [  2. 183.  31.   1.   0.  28.]
 [ 52.  32. 208.  24.   2.  66.]
 [  0.   0.   4. 124.   0.  42.]
 [ 96.   0.   8.   0. 194.   1.]
 [  7.  24.  49.  64.   1. 236.]]
loss: 4.852475166320801
loss: 4.269776821136475
loss: 4.55842399597168
epoch: 21, train_loss: 4.5879998207092285, train_acc: 65.9, train_fscore: 65.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.5584001541137695, test_acc: 64.45, test_fscore: 65.01, time: 6.74 sec
loss: 4.515694618225098
loss: 4.532317161560059
loss: 4.483743667602539
epoch: 22, train_loss: 4.523399829864502, train_acc: 65.59, train_fscore: 64.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.483699798583984, test_acc: 65.8, test_fscore: 65.88, time: 5.88 sec
loss: 4.413319110870361
loss: 4.498751163482666
loss: 4.407330513000488
epoch: 23, train_loss: 4.455699920654297, train_acc: 66.02, train_fscore: 65.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.407299995422363, test_acc: 65.56, test_fscore: 65.72, time: 6.14 sec
loss: 4.547799587249756
loss: 4.278243064880371
loss: 4.383707046508789
epoch: 24, train_loss: 4.4197001457214355, train_acc: 65.58, train_fscore: 64.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.383699893951416, test_acc: 64.88, test_fscore: 65.22, time: 6.04 sec
loss: 4.235272407531738
loss: 4.479297637939453
loss: 4.393605709075928
epoch: 25, train_loss: 4.350399971008301, train_acc: 66.51, train_fscore: 65.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.393599987030029, test_acc: 64.76, test_fscore: 65.11, time: 6.57 sec
loss: 4.295623302459717
loss: 4.272891521453857
loss: 4.373039245605469
epoch: 26, train_loss: 4.284900188446045, train_acc: 67.92, train_fscore: 67.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.373000144958496, test_acc: 64.2, test_fscore: 64.74, time: 6.88 sec
loss: 4.292745113372803
loss: 4.1828932762146
loss: 4.323878288269043
epoch: 27, train_loss: 4.241799831390381, train_acc: 67.28, train_fscore: 66.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.32390022277832, test_acc: 64.82, test_fscore: 65.29, time: 4.36 sec
loss: 4.104118347167969
loss: 4.277570724487305
loss: 4.2708539962768555
epoch: 28, train_loss: 4.188000202178955, train_acc: 68.5, train_fscore: 67.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.270899772644043, test_acc: 65.62, test_fscore: 65.86, time: 3.73 sec
loss: 4.327098369598389
loss: 3.9312424659729004
loss: 4.247697353363037
epoch: 29, train_loss: 4.1407999992370605, train_acc: 68.16, train_fscore: 67.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.247700214385986, test_acc: 66.36, test_fscore: 66.55, time: 5.28 sec
loss: 4.152032375335693
loss: 4.049457550048828
loss: 4.210002422332764
epoch: 30, train_loss: 4.104300022125244, train_acc: 68.55, train_fscore: 67.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.210000038146973, test_acc: 65.62, test_fscore: 66.13, time: 4.8 sec
              precision    recall  f1-score   support

           0     0.4277    0.5139    0.4669     144.0
           1     0.7317    0.7347    0.7332     245.0
           2     0.7006    0.5729    0.6304     384.0
           3     0.5511    0.7294    0.6278     170.0
           4     0.8288    0.8094    0.8190     299.0
           5     0.6354    0.6220    0.6286     381.0

    accuracy                         0.6636    1623.0
   macro avg     0.6459    0.6637    0.6510    1623.0
weighted avg     0.6737    0.6636    0.6655    1623.0

[[ 74.  11.  11.   7.  40.   1.]
 [  4. 180.  29.   0.   0.  32.]
 [ 46.  27. 220.  28.   5.  58.]
 [  0.   0.   2. 124.   0.  44.]
 [ 48.   0.   8.   0. 242.   1.]
 [  1.  28.  44.  66.   5. 237.]]
loss: 3.914620876312256
loss: 4.245046615600586
loss: 4.167397975921631
epoch: 31, train_loss: 4.065000057220459, train_acc: 68.86, train_fscore: 68.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.167399883270264, test_acc: 65.93, test_fscore: 66.24, time: 6.92 sec
loss: 4.056978702545166
loss: 3.950083017349243
loss: 4.161105632781982
epoch: 32, train_loss: 4.005899906158447, train_acc: 69.29, train_fscore: 68.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.161099910736084, test_acc: 65.74, test_fscore: 66.3, time: 5.36 sec
loss: 3.91902232170105
loss: 3.9938206672668457
loss: 4.113088607788086
epoch: 33, train_loss: 3.9539999961853027, train_acc: 69.76, train_fscore: 69.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.113100051879883, test_acc: 66.54, test_fscore: 67.04, time: 6.68 sec
loss: 3.9451026916503906
loss: 3.904463529586792
loss: 4.0717949867248535
epoch: 34, train_loss: 3.9268999099731445, train_acc: 70.22, train_fscore: 69.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.071800231933594, test_acc: 67.16, test_fscore: 67.48, time: 5.71 sec
loss: 3.793056011199951
loss: 3.9878644943237305
loss: 4.048252105712891
epoch: 35, train_loss: 3.8801000118255615, train_acc: 71.0, train_fscore: 70.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.048299789428711, test_acc: 66.79, test_fscore: 67.25, time: 6.63 sec
loss: 3.7521772384643555
loss: 3.930248737335205
loss: 3.970698118209839
epoch: 36, train_loss: 3.841099977493286, train_acc: 71.29, train_fscore: 70.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9707000255584717, test_acc: 68.21, test_fscore: 68.47, time: 5.89 sec
loss: 3.685061454772949
loss: 3.9371538162231445
loss: 3.9366965293884277
epoch: 37, train_loss: 3.8010001182556152, train_acc: 71.24, train_fscore: 70.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9367001056671143, test_acc: 68.33, test_fscore: 68.72, time: 6.16 sec
loss: 3.7761261463165283
loss: 3.7788665294647217
loss: 3.9302682876586914
epoch: 38, train_loss: 3.7774999141693115, train_acc: 71.57, train_fscore: 71.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.930299997329712, test_acc: 67.84, test_fscore: 68.41, time: 5.47 sec
loss: 3.7378482818603516
loss: 3.706479072570801
loss: 3.8892178535461426
epoch: 39, train_loss: 3.7235000133514404, train_acc: 71.17, train_fscore: 70.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.88919997215271, test_acc: 68.21, test_fscore: 68.62, time: 6.89 sec
loss: 3.7391395568847656
loss: 3.608772039413452
loss: 3.8756496906280518
epoch: 40, train_loss: 3.678800106048584, train_acc: 71.98, train_fscore: 71.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8756000995635986, test_acc: 67.9, test_fscore: 68.4, time: 6.71 sec
              precision    recall  f1-score   support

           0     0.4359    0.5903    0.5015     144.0
           1     0.7754    0.7469    0.7609     245.0
           2     0.7072    0.6354    0.6694     384.0
           3     0.6050    0.7118    0.6541     170.0
           4     0.8448    0.7826    0.8125     299.0
           5     0.6541    0.6352    0.6445     381.0

    accuracy                         0.6833    1623.0
   macro avg     0.6704    0.6837    0.6738    1623.0
weighted avg     0.6956    0.6833    0.6872    1623.0

[[ 85.   7.  12.   5.  35.   0.]
 [  5. 183.  22.   0.   0.  35.]
 [ 47.  22. 244.  22.   4.  45.]
 [  0.   0.   2. 121.   0.  47.]
 [ 56.   0.   8.   0. 234.   1.]
 [  2.  24.  57.  52.   4. 242.]]
loss: 3.4747185707092285
loss: 3.9007134437561035
loss: 3.844259738922119
epoch: 41, train_loss: 3.67330002784729, train_acc: 72.72, train_fscore: 72.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8443000316619873, test_acc: 68.58, test_fscore: 69.1, time: 5.62 sec
loss: 3.5929033756256104
loss: 3.6746201515197754
loss: 3.817657709121704
epoch: 42, train_loss: 3.6326000690460205, train_acc: 72.65, train_fscore: 72.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.817699909210205, test_acc: 68.39, test_fscore: 68.71, time: 5.82 sec
loss: 3.6305384635925293
loss: 3.5525779724121094
loss: 3.7919199466705322
epoch: 43, train_loss: 3.5941998958587646, train_acc: 72.5, train_fscore: 72.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7918999195098877, test_acc: 68.08, test_fscore: 68.55, time: 6.65 sec
loss: 3.450875759124756
loss: 3.707170248031616
loss: 3.773101568222046
epoch: 44, train_loss: 3.5743000507354736, train_acc: 73.17, train_fscore: 72.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.773099899291992, test_acc: 68.64, test_fscore: 69.09, time: 5.64 sec
loss: 3.661409378051758
loss: 3.421189308166504
loss: 3.764941453933716
epoch: 45, train_loss: 3.5443999767303467, train_acc: 73.41, train_fscore: 73.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.764899969100952, test_acc: 68.52, test_fscore: 68.91, time: 6.67 sec
loss: 3.43444561958313
loss: 3.620974540710449
loss: 3.745962381362915
epoch: 46, train_loss: 3.5239999294281006, train_acc: 73.51, train_fscore: 73.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.746000051498413, test_acc: 68.39, test_fscore: 68.99, time: 6.68 sec
loss: 3.3867456912994385
loss: 3.6085379123687744
loss: 3.714526891708374
epoch: 47, train_loss: 3.489000082015991, train_acc: 73.2, train_fscore: 72.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7144999504089355, test_acc: 69.01, test_fscore: 69.44, time: 6.03 sec
loss: 3.2899022102355957
loss: 3.6136136054992676
loss: 3.6921722888946533
epoch: 48, train_loss: 3.452500104904175, train_acc: 73.58, train_fscore: 73.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.692199945449829, test_acc: 69.01, test_fscore: 69.42, time: 6.55 sec
loss: 3.4297735691070557
loss: 3.463606595993042
loss: 3.673626661300659
epoch: 49, train_loss: 3.445499897003174, train_acc: 73.67, train_fscore: 73.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6735999584198, test_acc: 68.39, test_fscore: 68.91, time: 6.48 sec
loss: 3.3857812881469727
loss: 3.4611382484436035
loss: 3.649937629699707
epoch: 50, train_loss: 3.4230000972747803, train_acc: 74.23, train_fscore: 74.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.649899959564209, test_acc: 68.7, test_fscore: 69.16, time: 6.16 sec
              precision    recall  f1-score   support

           0     0.4505    0.6319    0.5260     144.0
           1     0.7797    0.7510    0.7651     245.0
           2     0.7205    0.6510    0.6840     384.0
           3     0.5972    0.7588    0.6684     170.0
           4     0.8561    0.7559    0.8028     299.0
           5     0.6704    0.6299    0.6495     381.0

    accuracy                         0.6901    1623.0
   macro avg     0.6790    0.6964    0.6826    1623.0
weighted avg     0.7058    0.6901    0.6944    1623.0

[[ 91.   9.  12.   4.  28.   0.]
 [  4. 184.  20.   0.   0.  37.]
 [ 44.  23. 250.  22.   4.  41.]
 [  0.   0.   2. 129.   0.  39.]
 [ 63.   1.   8.   0. 226.   1.]
 [  0.  19.  55.  61.   6. 240.]]
loss: 3.374946117401123
loss: 3.4201622009277344
loss: 3.6502060890197754
epoch: 51, train_loss: 3.396399974822998, train_acc: 73.96, train_fscore: 73.7, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6501998901367188, test_acc: 68.7, test_fscore: 69.18, time: 6.5 sec
loss: 3.36629056930542
loss: 3.3551504611968994
loss: 3.640275001525879
epoch: 52, train_loss: 3.361299991607666, train_acc: 75.08, train_fscore: 74.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6403000354766846, test_acc: 68.7, test_fscore: 69.21, time: 5.67 sec
loss: 3.2954065799713135
loss: 3.4217631816864014
loss: 3.6141228675842285
epoch: 53, train_loss: 3.353300094604492, train_acc: 74.77, train_fscore: 74.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6140999794006348, test_acc: 69.07, test_fscore: 69.51, time: 6.84 sec
loss: 3.3977696895599365
loss: 3.1977319717407227
loss: 3.624817132949829
epoch: 54, train_loss: 3.3013999462127686, train_acc: 74.54, train_fscore: 74.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.624799966812134, test_acc: 68.64, test_fscore: 69.18, time: 6.08 sec
loss: 3.250091791152954
loss: 3.400230884552002
loss: 3.59238338470459
epoch: 55, train_loss: 3.3213999271392822, train_acc: 75.28, train_fscore: 75.09, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.592400074005127, test_acc: 68.76, test_fscore: 69.26, time: 6.2 sec
loss: 3.1439032554626465
loss: 3.4805538654327393
loss: 3.5794057846069336
epoch: 56, train_loss: 3.294800043106079, train_acc: 75.61, train_fscore: 75.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.579400062561035, test_acc: 68.58, test_fscore: 68.99, time: 5.98 sec
loss: 3.2786340713500977
loss: 3.240556001663208
loss: 3.60587739944458
epoch: 57, train_loss: 3.260699987411499, train_acc: 75.15, train_fscore: 74.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6059000492095947, test_acc: 68.76, test_fscore: 69.27, time: 4.76 sec
loss: 3.2035017013549805
loss: 3.3297066688537598
loss: 3.5542638301849365
epoch: 58, train_loss: 3.2627999782562256, train_acc: 75.46, train_fscore: 75.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.55430006980896, test_acc: 69.07, test_fscore: 69.62, time: 6.21 sec
loss: 3.1258137226104736
loss: 3.356363296508789
loss: 3.535534143447876
epoch: 59, train_loss: 3.237299919128418, train_acc: 75.59, train_fscore: 75.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5355000495910645, test_acc: 69.81, test_fscore: 70.31, time: 6.04 sec
loss: 3.150524616241455
loss: 3.245474100112915
loss: 3.5384573936462402
epoch: 60, train_loss: 3.1977999210357666, train_acc: 76.44, train_fscore: 76.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5385000705718994, test_acc: 68.76, test_fscore: 69.36, time: 6.79 sec
              precision    recall  f1-score   support

           0     0.4575    0.6736    0.5449     144.0
           1     0.8017    0.7592    0.7799     245.0
           2     0.7199    0.6693    0.6937     384.0
           3     0.6139    0.7294    0.6667     170.0
           4     0.8716    0.7492    0.8058     299.0
           5     0.6749    0.6430    0.6586     381.0

    accuracy                         0.6981    1623.0
   macro avg     0.6899    0.7039    0.6916    1623.0
weighted avg     0.7153    0.6981    0.7031    1623.0

[[ 97.   9.  14.   0.  24.   0.]
 [  3. 186.  21.   0.   0.  35.]
 [ 46.  19. 257.  21.   4.  37.]
 [  0.   0.   1. 124.   0.  45.]
 [ 65.   1.   8.   0. 224.   1.]
 [  1.  17.  56.  57.   5. 245.]]
loss: 3.2062301635742188
loss: 3.1800527572631836
loss: 3.5344362258911133
epoch: 61, train_loss: 3.194000005722046, train_acc: 76.2, train_fscore: 75.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.53439998626709, test_acc: 69.44, test_fscore: 69.84, time: 6.68 sec
loss: 3.2204370498657227
loss: 3.115389347076416
loss: 3.5096592903137207
epoch: 62, train_loss: 3.1740000247955322, train_acc: 76.32, train_fscore: 76.06, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.509700059890747, test_acc: 69.01, test_fscore: 69.54, time: 6.58 sec
loss: 3.04866361618042
loss: 3.3008084297180176
loss: 3.495443105697632
epoch: 63, train_loss: 3.1666998863220215, train_acc: 76.61, train_fscore: 76.43, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4953999519348145, test_acc: 69.07, test_fscore: 69.66, time: 6.66 sec
loss: 3.2181663513183594
loss: 3.053514003753662
loss: 3.490334987640381
epoch: 64, train_loss: 3.141200065612793, train_acc: 76.95, train_fscore: 76.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.490299940109253, test_acc: 69.99, test_fscore: 70.46, time: 6.71 sec
loss: 3.1983680725097656
loss: 3.0307440757751465
loss: 3.477142810821533
epoch: 65, train_loss: 3.11680006980896, train_acc: 76.92, train_fscore: 76.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.477099895477295, test_acc: 70.24, test_fscore: 70.71, time: 5.87 sec
loss: 3.0796730518341064
loss: 3.1595749855041504
loss: 3.448587656021118
epoch: 66, train_loss: 3.1147000789642334, train_acc: 76.73, train_fscore: 76.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4486000537872314, test_acc: 70.18, test_fscore: 70.62, time: 7.17 sec
loss: 3.1153714656829834
loss: 3.0735583305358887
loss: 3.4330482482910156
epoch: 67, train_loss: 3.0952000617980957, train_acc: 77.09, train_fscore: 76.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.433000087738037, test_acc: 70.43, test_fscore: 70.83, time: 6.14 sec
loss: 3.11206316947937
loss: 3.0202839374542236
loss: 3.4684717655181885
epoch: 68, train_loss: 3.069999933242798, train_acc: 77.3, train_fscore: 77.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4684998989105225, test_acc: 70.12, test_fscore: 70.62, time: 6.11 sec
loss: 3.106379747390747
loss: 3.0142910480499268
loss: 3.446197748184204
epoch: 69, train_loss: 3.066699981689453, train_acc: 77.31, train_fscore: 77.17, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.446199893951416, test_acc: 70.67, test_fscore: 71.12, time: 6.46 sec
loss: 3.0771870613098145
loss: 3.0016963481903076
loss: 3.438145637512207
epoch: 70, train_loss: 3.0446999073028564, train_acc: 77.49, train_fscore: 77.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4381000995635986, test_acc: 71.16, test_fscore: 71.55, time: 6.09 sec
              precision    recall  f1-score   support

           0     0.4927    0.7014    0.5788     144.0
           1     0.8174    0.7673    0.7916     245.0
           2     0.7385    0.6693    0.7022     384.0
           3     0.6256    0.7471    0.6810     170.0
           4     0.8619    0.7726    0.8148     299.0
           5     0.6802    0.6588    0.6693     381.0

    accuracy                         0.7116    1623.0
   macro avg     0.7027    0.7194    0.7063    1623.0
weighted avg     0.7258    0.7116    0.7155    1623.0

[[101.   7.  11.   0.  25.   0.]
 [  2. 188.  19.   1.   0.  35.]
 [ 44.  18. 257.  19.   6.  40.]
 [  0.   0.   1. 127.   0.  42.]
 [ 57.   2.   8.   0. 231.   1.]
 [  1.  15.  52.  56.   6. 251.]]
loss: 3.089787006378174
loss: 2.9907941818237305
loss: 3.4483420848846436
epoch: 71, train_loss: 3.0439000129699707, train_acc: 77.78, train_fscore: 77.62, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4482998847961426, test_acc: 70.12, test_fscore: 70.64, time: 6.67 sec
loss: 2.9389960765838623
loss: 3.1031908988952637
loss: 3.4230809211730957
epoch: 72, train_loss: 3.014899969100952, train_acc: 77.8, train_fscore: 77.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.423099994659424, test_acc: 70.36, test_fscore: 70.73, time: 5.42 sec
loss: 3.0127105712890625
loss: 2.9765877723693848
loss: 3.4173667430877686
epoch: 73, train_loss: 2.9964001178741455, train_acc: 77.99, train_fscore: 77.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4173998832702637, test_acc: 70.61, test_fscore: 70.97, time: 5.45 sec
loss: 2.9421544075012207
loss: 3.031325578689575
loss: 3.4037163257598877
epoch: 74, train_loss: 2.9821999073028564, train_acc: 77.73, train_fscore: 77.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.403700113296509, test_acc: 70.12, test_fscore: 70.6, time: 6.4 sec
loss: 2.9268698692321777
loss: 3.0575082302093506
loss: 3.3924460411071777
epoch: 75, train_loss: 2.9888999462127686, train_acc: 78.04, train_fscore: 77.9, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.392400026321411, test_acc: 71.04, test_fscore: 71.4, time: 6.07 sec
loss: 3.152332305908203
loss: 2.77518367767334
loss: 3.384647846221924
epoch: 76, train_loss: 2.985100030899048, train_acc: 78.16, train_fscore: 77.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3845999240875244, test_acc: 71.16, test_fscore: 71.54, time: 6.35 sec
loss: 2.946971893310547
loss: 2.9333126544952393
loss: 3.4044227600097656
epoch: 77, train_loss: 2.9409000873565674, train_acc: 78.9, train_fscore: 78.75, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.404400110244751, test_acc: 70.86, test_fscore: 71.34, time: 5.83 sec
loss: 2.9240505695343018
loss: 2.9614310264587402
loss: 3.4044933319091797
epoch: 78, train_loss: 2.940999984741211, train_acc: 78.67, train_fscore: 78.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4045000076293945, test_acc: 71.16, test_fscore: 71.49, time: 6.14 sec
loss: 2.890995502471924
loss: 2.9844648838043213
loss: 3.3630337715148926
epoch: 79, train_loss: 2.9321999549865723, train_acc: 78.43, train_fscore: 78.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.36299991607666, test_acc: 71.41, test_fscore: 71.8, time: 6.32 sec
loss: 2.8961172103881836
loss: 2.93774676322937
loss: 3.378587007522583
epoch: 80, train_loss: 2.916800022125244, train_acc: 78.33, train_fscore: 78.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3785998821258545, test_acc: 70.55, test_fscore: 71.03, time: 5.94 sec
              precision    recall  f1-score   support

           0     0.5000    0.7431    0.5978     144.0
           1     0.8202    0.7633    0.7907     245.0
           2     0.7301    0.6693    0.6984     384.0
           3     0.6373    0.7647    0.6952     170.0
           4     0.8740    0.7659    0.8164     299.0
           5     0.6860    0.6535    0.6694     381.0

    accuracy                         0.7141    1623.0
   macro avg     0.7079    0.7266    0.7113    1623.0
weighted avg     0.7297    0.7141    0.7180    1623.0

[[107.   7.   9.   0.  21.   0.]
 [  2. 187.  22.   1.   0.  33.]
 [ 45.  17. 257.  19.   5.  41.]
 [  0.   0.   1. 130.   0.  39.]
 [ 59.   2.   8.   0. 229.   1.]
 [  1.  15.  55.  54.   7. 249.]]
loss: 2.882828950881958
loss: 2.9004697799682617
loss: 3.3881888389587402
epoch: 81, train_loss: 2.8907999992370605, train_acc: 78.9, train_fscore: 78.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.388200044631958, test_acc: 71.53, test_fscore: 71.89, time: 6.22 sec
loss: 2.790367364883423
loss: 3.0100860595703125
loss: 3.3687756061553955
epoch: 82, train_loss: 2.897700071334839, train_acc: 79.23, train_fscore: 79.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.368799924850464, test_acc: 71.53, test_fscore: 71.89, time: 6.1 sec
loss: 2.8064992427825928
loss: 2.948864221572876
loss: 3.359616279602051
epoch: 83, train_loss: 2.8738999366760254, train_acc: 79.5, train_fscore: 79.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.359600067138672, test_acc: 71.66, test_fscore: 72.06, time: 5.41 sec
loss: 2.8403334617614746
loss: 2.9017958641052246
loss: 3.3448612689971924
epoch: 84, train_loss: 2.868000030517578, train_acc: 79.41, train_fscore: 79.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.344899892807007, test_acc: 71.9, test_fscore: 72.26, time: 5.84 sec
loss: 2.829925537109375
loss: 2.867234468460083
loss: 3.3435208797454834
epoch: 85, train_loss: 2.847399950027466, train_acc: 79.72, train_fscore: 79.57, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3434998989105225, test_acc: 71.78, test_fscore: 72.15, time: 6.56 sec
loss: 2.9075546264648438
loss: 2.7887470722198486
loss: 3.336477279663086
epoch: 86, train_loss: 2.851099967956543, train_acc: 79.33, train_fscore: 79.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3364999294281006, test_acc: 71.66, test_fscore: 72.05, time: 6.26 sec
loss: 2.8605122566223145
loss: 2.796954393386841
loss: 3.3345375061035156
epoch: 87, train_loss: 2.8315999507904053, train_acc: 79.69, train_fscore: 79.59, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3345000743865967, test_acc: 71.84, test_fscore: 72.17, time: 5.92 sec
loss: 2.7961368560791016
loss: 2.846592426300049
loss: 3.346156120300293
epoch: 88, train_loss: 2.8192999362945557, train_acc: 79.85, train_fscore: 79.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3461999893188477, test_acc: 71.35, test_fscore: 71.73, time: 6.84 sec
loss: 2.8305797576904297
loss: 2.768714189529419
loss: 3.3384628295898438
epoch: 89, train_loss: 2.7994000911712646, train_acc: 80.03, train_fscore: 79.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3385000228881836, test_acc: 71.23, test_fscore: 71.63, time: 5.99 sec
loss: 2.7462637424468994
loss: 2.8847665786743164
loss: 3.3301026821136475
epoch: 90, train_loss: 2.810800075531006, train_acc: 80.1, train_fscore: 79.97, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3301000595092773, test_acc: 71.29, test_fscore: 71.57, time: 4.32 sec
              precision    recall  f1-score   support

           0     0.5093    0.7569    0.6089     144.0
           1     0.8170    0.7837    0.8000     245.0
           2     0.7414    0.6719    0.7049     384.0
           3     0.6404    0.7647    0.6971     170.0
           4     0.8736    0.7625    0.8143     299.0
           5     0.6906    0.6562    0.6729     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7121    0.7327    0.7164    1623.0
weighted avg     0.7341    0.7190    0.7226    1623.0

[[109.   6.   8.   0.  21.   0.]
 [  2. 192.  19.   1.   0.  31.]
 [ 43.  19. 258.  17.   6.  41.]
 [  0.   0.   1. 130.   0.  39.]
 [ 59.   2.   9.   0. 228.   1.]
 [  1.  16.  53.  55.   6. 250.]]
loss: 2.8921828269958496
loss: 2.6640613079071045
loss: 3.3400816917419434
epoch: 91, train_loss: 2.785399913787842, train_acc: 80.03, train_fscore: 79.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.340100049972534, test_acc: 71.29, test_fscore: 71.73, time: 6.73 sec
loss: 2.8914427757263184
loss: 2.7066702842712402
loss: 3.340522527694702
epoch: 92, train_loss: 2.8046998977661133, train_acc: 80.14, train_fscore: 80.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3405001163482666, test_acc: 71.6, test_fscore: 71.99, time: 6.19 sec
loss: 2.747330665588379
loss: 2.7858657836914062
loss: 3.308396816253662
epoch: 93, train_loss: 2.765399932861328, train_acc: 80.34, train_fscore: 80.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3083999156951904, test_acc: 71.53, test_fscore: 71.84, time: 6.54 sec
loss: 2.7534055709838867
loss: 2.7718100547790527
loss: 3.3076186180114746
epoch: 94, train_loss: 2.76200008392334, train_acc: 80.86, train_fscore: 80.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3076000213623047, test_acc: 71.29, test_fscore: 71.64, time: 6.9 sec
loss: 2.8348002433776855
loss: 2.6339364051818848
loss: 3.3069846630096436
epoch: 95, train_loss: 2.740299940109253, train_acc: 80.45, train_fscore: 80.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.306999921798706, test_acc: 71.47, test_fscore: 71.76, time: 5.81 sec
loss: 2.7844042778015137
loss: 2.659956932067871
loss: 3.2964305877685547
epoch: 96, train_loss: 2.726599931716919, train_acc: 81.02, train_fscore: 80.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2964000701904297, test_acc: 71.53, test_fscore: 71.85, time: 7.04 sec
loss: 2.7691640853881836
loss: 2.6724836826324463
loss: 3.315197467803955
epoch: 97, train_loss: 2.7228000164031982, train_acc: 80.96, train_fscore: 80.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.315200090408325, test_acc: 71.16, test_fscore: 71.56, time: 5.95 sec
loss: 2.839137077331543
loss: 2.5593364238739014
loss: 3.30511474609375
epoch: 98, train_loss: 2.7119998931884766, train_acc: 80.55, train_fscore: 80.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3050999641418457, test_acc: 71.47, test_fscore: 71.8, time: 6.43 sec
loss: 2.698288917541504
loss: 2.7139735221862793
loss: 3.297884941101074
epoch: 99, train_loss: 2.7056000232696533, train_acc: 81.38, train_fscore: 81.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2978999614715576, test_acc: 71.53, test_fscore: 71.9, time: 6.22 sec
loss: 2.808567523956299
loss: 2.57651424407959
loss: 3.2804574966430664
epoch: 100, train_loss: 2.7016000747680664, train_acc: 81.05, train_fscore: 80.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2804999351501465, test_acc: 71.41, test_fscore: 71.75, time: 6.06 sec
              precision    recall  f1-score   support

           0     0.5093    0.7569    0.6089     144.0
           1     0.8170    0.7837    0.8000     245.0
           2     0.7414    0.6719    0.7049     384.0
           3     0.6404    0.7647    0.6971     170.0
           4     0.8736    0.7625    0.8143     299.0
           5     0.6906    0.6562    0.6729     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7121    0.7327    0.7164    1623.0
weighted avg     0.7341    0.7190    0.7226    1623.0

[[109.   6.   8.   0.  21.   0.]
 [  2. 192.  19.   1.   0.  31.]
 [ 43.  19. 258.  17.   6.  41.]
 [  0.   0.   1. 130.   0.  39.]
 [ 59.   2.   9.   0. 228.   1.]
 [  1.  16.  53.  55.   6. 250.]]
loss: 2.587731122970581
loss: 2.781381845474243
loss: 3.276996612548828
epoch: 101, train_loss: 2.6774001121520996, train_acc: 81.36, train_fscore: 81.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2769999504089355, test_acc: 71.47, test_fscore: 71.85, time: 6.32 sec
loss: 2.6175215244293213
loss: 2.738654851913452
loss: 3.3080337047576904
epoch: 102, train_loss: 2.674099922180176, train_acc: 81.15, train_fscore: 81.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.308000087738037, test_acc: 71.47, test_fscore: 71.81, time: 6.36 sec
loss: 2.7315235137939453
loss: 2.5735437870025635
loss: 3.320195198059082
epoch: 103, train_loss: 2.660399913787842, train_acc: 80.77, train_fscore: 80.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.320199966430664, test_acc: 71.29, test_fscore: 71.67, time: 6.43 sec
loss: 2.529134511947632
loss: 2.7555887699127197
loss: 3.2799413204193115
epoch: 104, train_loss: 2.636699914932251, train_acc: 81.74, train_fscore: 81.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.279900074005127, test_acc: 71.23, test_fscore: 71.59, time: 6.74 sec
loss: 2.739142417907715
loss: 2.519892454147339
loss: 3.270615577697754
epoch: 105, train_loss: 2.6349000930786133, train_acc: 81.24, train_fscore: 81.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2706000804901123, test_acc: 71.66, test_fscore: 71.98, time: 6.54 sec
loss: 2.6970579624176025
loss: 2.5394444465637207
loss: 3.288541793823242
epoch: 106, train_loss: 2.626499891281128, train_acc: 81.86, train_fscore: 81.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2885000705718994, test_acc: 71.6, test_fscore: 71.96, time: 5.9 sec
loss: 2.6911826133728027
loss: 2.548480987548828
loss: 3.303267002105713
epoch: 107, train_loss: 2.625699996948242, train_acc: 81.65, train_fscore: 81.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.303299903869629, test_acc: 71.47, test_fscore: 71.82, time: 6.63 sec
loss: 2.478294849395752
loss: 2.8170762062072754
loss: 3.320784330368042
epoch: 108, train_loss: 2.6289000511169434, train_acc: 81.65, train_fscore: 81.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3208000659942627, test_acc: 71.53, test_fscore: 71.97, time: 6.43 sec
loss: 2.540402412414551
loss: 2.6924052238464355
loss: 3.319209098815918
epoch: 109, train_loss: 2.6108999252319336, train_acc: 81.89, train_fscore: 81.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.319200038909912, test_acc: 71.29, test_fscore: 71.67, time: 6.08 sec
loss: 2.630605459213257
loss: 2.539290189743042
loss: 3.318758487701416
epoch: 110, train_loss: 2.588399887084961, train_acc: 82.22, train_fscore: 82.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3187999725341797, test_acc: 71.23, test_fscore: 71.56, time: 5.83 sec
              precision    recall  f1-score   support

           0     0.5093    0.7569    0.6089     144.0
           1     0.8170    0.7837    0.8000     245.0
           2     0.7414    0.6719    0.7049     384.0
           3     0.6404    0.7647    0.6971     170.0
           4     0.8736    0.7625    0.8143     299.0
           5     0.6906    0.6562    0.6729     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7121    0.7327    0.7164    1623.0
weighted avg     0.7341    0.7190    0.7226    1623.0

[[109.   6.   8.   0.  21.   0.]
 [  2. 192.  19.   1.   0.  31.]
 [ 43.  19. 258.  17.   6.  41.]
 [  0.   0.   1. 130.   0.  39.]
 [ 59.   2.   9.   0. 228.   1.]
 [  1.  16.  53.  55.   6. 250.]]
loss: 2.561779737472534
loss: 2.62094783782959
loss: 3.299637794494629
epoch: 111, train_loss: 2.588900089263916, train_acc: 82.43, train_fscore: 82.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2995998859405518, test_acc: 71.1, test_fscore: 71.52, time: 6.22 sec
loss: 2.5701961517333984
loss: 2.592209577560425
loss: 3.290346622467041
epoch: 112, train_loss: 2.580899953842163, train_acc: 82.44, train_fscore: 82.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.290299892425537, test_acc: 71.66, test_fscore: 72.02, time: 5.86 sec
loss: 2.4896202087402344
loss: 2.671293258666992
loss: 3.3233015537261963
epoch: 113, train_loss: 2.576900005340576, train_acc: 81.98, train_fscore: 81.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3232998847961426, test_acc: 71.16, test_fscore: 71.53, time: 6.26 sec
loss: 2.6194543838500977
loss: 2.4822404384613037
loss: 3.3030738830566406
epoch: 114, train_loss: 2.558799982070923, train_acc: 82.53, train_fscore: 82.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.303100109100342, test_acc: 71.53, test_fscore: 71.91, time: 6.41 sec
loss: 2.5035653114318848
loss: 2.6272099018096924
loss: 3.280979633331299
epoch: 115, train_loss: 2.560699939727783, train_acc: 82.5, train_fscore: 82.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2809998989105225, test_acc: 71.41, test_fscore: 71.74, time: 6.35 sec
loss: 2.542712688446045
loss: 2.5664069652557373
loss: 3.2848994731903076
epoch: 116, train_loss: 2.553299903869629, train_acc: 82.38, train_fscore: 82.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.284899950027466, test_acc: 71.47, test_fscore: 71.87, time: 6.33 sec
loss: 2.4640073776245117
loss: 2.687941074371338
loss: 3.2964026927948
epoch: 117, train_loss: 2.5596001148223877, train_acc: 82.96, train_fscore: 82.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2964000701904297, test_acc: 71.35, test_fscore: 71.75, time: 5.69 sec
loss: 2.542104721069336
loss: 2.5517146587371826
loss: 3.3008525371551514
epoch: 118, train_loss: 2.5464999675750732, train_acc: 83.24, train_fscore: 83.12, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3008999824523926, test_acc: 71.84, test_fscore: 72.16, time: 6.73 sec
loss: 2.509185314178467
loss: 2.513882637023926
loss: 3.2770133018493652
epoch: 119, train_loss: 2.511199951171875, train_acc: 83.1, train_fscore: 83.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2769999504089355, test_acc: 71.66, test_fscore: 72.05, time: 5.55 sec
loss: 2.488955020904541
loss: 2.5240836143493652
loss: 3.2813148498535156
epoch: 120, train_loss: 2.5053000450134277, train_acc: 82.93, train_fscore: 82.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2813000679016113, test_acc: 71.47, test_fscore: 71.84, time: 6.69 sec
              precision    recall  f1-score   support

           0     0.5093    0.7569    0.6089     144.0
           1     0.8170    0.7837    0.8000     245.0
           2     0.7414    0.6719    0.7049     384.0
           3     0.6404    0.7647    0.6971     170.0
           4     0.8736    0.7625    0.8143     299.0
           5     0.6906    0.6562    0.6729     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7121    0.7327    0.7164    1623.0
weighted avg     0.7341    0.7190    0.7226    1623.0

[[109.   6.   8.   0.  21.   0.]
 [  2. 192.  19.   1.   0.  31.]
 [ 43.  19. 258.  17.   6.  41.]
 [  0.   0.   1. 130.   0.  39.]
 [ 59.   2.   9.   0. 228.   1.]
 [  1.  16.  53.  55.   6. 250.]]
loss: 2.4763898849487305
loss: 2.5666255950927734
loss: 3.2785589694976807
epoch: 121, train_loss: 2.5195999145507812, train_acc: 82.98, train_fscore: 82.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.278599977493286, test_acc: 71.72, test_fscore: 72.12, time: 6.11 sec
loss: 2.4549500942230225
loss: 2.5553553104400635
loss: 3.3018386363983154
epoch: 122, train_loss: 2.5044000148773193, train_acc: 83.13, train_fscore: 83.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.301800012588501, test_acc: 71.9, test_fscore: 72.29, time: 6.13 sec
loss: 2.4532384872436523
loss: 2.503835916519165
loss: 3.3128299713134766
epoch: 123, train_loss: 2.477299928665161, train_acc: 83.25, train_fscore: 83.15, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3127999305725098, test_acc: 71.53, test_fscore: 71.91, time: 6.24 sec
loss: 2.5328240394592285
loss: 2.3995418548583984
loss: 3.3199703693389893
epoch: 124, train_loss: 2.4723000526428223, train_acc: 83.65, train_fscore: 83.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.319999933242798, test_acc: 71.1, test_fscore: 71.45, time: 5.8 sec
loss: 2.441157341003418
loss: 2.522505283355713
loss: 3.29654598236084
epoch: 125, train_loss: 2.4781999588012695, train_acc: 83.44, train_fscore: 83.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2964999675750732, test_acc: 71.41, test_fscore: 71.78, time: 6.57 sec
loss: 2.3874194622039795
loss: 2.5889761447906494
loss: 3.2887821197509766
epoch: 126, train_loss: 2.474299907684326, train_acc: 83.32, train_fscore: 83.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.288800001144409, test_acc: 71.84, test_fscore: 72.19, time: 4.74 sec
loss: 2.3859786987304688
loss: 2.5656380653381348
loss: 3.2961080074310303
epoch: 127, train_loss: 2.464200019836426, train_acc: 83.82, train_fscore: 83.72, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.296099901199341, test_acc: 71.6, test_fscore: 72.0, time: 6.53 sec
loss: 2.402228832244873
loss: 2.5389442443847656
loss: 3.2977635860443115
epoch: 128, train_loss: 2.468400001525879, train_acc: 83.61, train_fscore: 83.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.297800064086914, test_acc: 71.66, test_fscore: 71.93, time: 5.95 sec
loss: 2.5230979919433594
loss: 2.3956196308135986
loss: 3.303274154663086
epoch: 129, train_loss: 2.4616000652313232, train_acc: 83.82, train_fscore: 83.7, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.303299903869629, test_acc: 71.41, test_fscore: 71.75, time: 6.19 sec
loss: 2.475766181945801
loss: 2.4386801719665527
loss: 3.311436176300049
epoch: 130, train_loss: 2.458400011062622, train_acc: 83.39, train_fscore: 83.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3113999366760254, test_acc: 71.84, test_fscore: 72.15, time: 5.99 sec
              precision    recall  f1-score   support

           0     0.5180    0.7986    0.6284     144.0
           1     0.8400    0.7714    0.8043     245.0
           2     0.7310    0.7005    0.7154     384.0
           3     0.6318    0.7471    0.6846     170.0
           4     0.8795    0.7324    0.7993     299.0
           5     0.6927    0.6509    0.6712     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7155    0.7335    0.7172    1623.0
weighted avg     0.7365    0.7190    0.7229    1623.0

[[115.   3.   7.   0.  19.   0.]
 [  2. 189.  18.   3.   0.  33.]
 [ 42.  17. 269.  16.   6.  34.]
 [  0.   0.   1. 127.   0.  42.]
 [ 62.   2.  15.   0. 219.   1.]
 [  1.  14.  58.  55.   5. 248.]]
loss: 2.353423595428467
loss: 2.5150413513183594
loss: 3.306425094604492
epoch: 131, train_loss: 2.4279000759124756, train_acc: 84.23, train_fscore: 84.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3064000606536865, test_acc: 71.6, test_fscore: 71.97, time: 6.73 sec
loss: 2.3635897636413574
loss: 2.471243381500244
loss: 3.3150157928466797
epoch: 132, train_loss: 2.414099931716919, train_acc: 84.03, train_fscore: 83.94, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.315000057220459, test_acc: 71.41, test_fscore: 71.76, time: 5.36 sec
loss: 2.4580743312835693
loss: 2.35575532913208
loss: 3.301243305206299
epoch: 133, train_loss: 2.4131999015808105, train_acc: 84.27, train_fscore: 84.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3011999130249023, test_acc: 71.47, test_fscore: 71.83, time: 6.45 sec
loss: 2.240828275680542
loss: 2.5564322471618652
loss: 3.297490358352661
epoch: 134, train_loss: 2.382999897003174, train_acc: 83.91, train_fscore: 83.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.297499895095825, test_acc: 71.47, test_fscore: 71.82, time: 6.34 sec
loss: 2.2794861793518066
loss: 2.537067174911499
loss: 3.331033945083618
epoch: 135, train_loss: 2.3961000442504883, train_acc: 84.3, train_fscore: 84.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3310000896453857, test_acc: 71.53, test_fscore: 71.89, time: 5.58 sec
loss: 2.4777238368988037
loss: 2.3204636573791504
loss: 3.3157010078430176
epoch: 136, train_loss: 2.4005000591278076, train_acc: 84.35, train_fscore: 84.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.315700054168701, test_acc: 71.72, test_fscore: 72.09, time: 7.16 sec
loss: 2.4904494285583496
loss: 2.2710304260253906
loss: 3.327667474746704
epoch: 137, train_loss: 2.3824000358581543, train_acc: 84.75, train_fscore: 84.66, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.327699899673462, test_acc: 72.58, test_fscore: 72.84, time: 5.68 sec
loss: 2.337953567504883
loss: 2.4453163146972656
loss: 3.2769217491149902
epoch: 138, train_loss: 2.387200117111206, train_acc: 84.54, train_fscore: 84.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.276900053024292, test_acc: 72.15, test_fscore: 72.47, time: 6.22 sec
loss: 2.4168496131896973
loss: 2.3451807498931885
loss: 3.278209686279297
epoch: 139, train_loss: 2.3831000328063965, train_acc: 83.99, train_fscore: 83.89, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2781999111175537, test_acc: 71.9, test_fscore: 72.29, time: 7.04 sec
loss: 2.375131130218506
loss: 2.3495588302612305
loss: 3.3058252334594727
epoch: 140, train_loss: 2.363100051879883, train_acc: 84.72, train_fscore: 84.59, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.305799961090088, test_acc: 72.03, test_fscore: 72.36, time: 5.97 sec
              precision    recall  f1-score   support

           0     0.5625    0.7500    0.6429     144.0
           1     0.8326    0.7714    0.8008     245.0
           2     0.7292    0.7083    0.7186     384.0
           3     0.6337    0.7529    0.6882     170.0
           4     0.8534    0.7592    0.8035     299.0
           5     0.6997    0.6667    0.6828     381.0

    accuracy                         0.7258    1623.0
   macro avg     0.7185    0.7348    0.7228    1623.0
weighted avg     0.7360    0.7258    0.7284    1623.0

[[108.   4.   6.   0.  26.   0.]
 [  2. 189.  21.   3.   0.  30.]
 [ 32.  20. 272.  15.   8.  37.]
 [  0.   0.   1. 128.   0.  41.]
 [ 50.   2.  19.   0. 227.   1.]
 [  0.  12.  54.  56.   5. 254.]]
loss: 2.296184778213501
loss: 2.4312665462493896
loss: 3.352567195892334
epoch: 141, train_loss: 2.3585000038146973, train_acc: 85.08, train_fscore: 84.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.35260009765625, test_acc: 71.29, test_fscore: 71.66, time: 6.38 sec
loss: 2.290668487548828
loss: 2.4481496810913086
loss: 3.3169164657592773
epoch: 142, train_loss: 2.363600015640259, train_acc: 84.92, train_fscore: 84.85, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3169000148773193, test_acc: 71.29, test_fscore: 71.69, time: 6.12 sec
loss: 2.332606077194214
loss: 2.3858234882354736
loss: 3.350243091583252
epoch: 143, train_loss: 2.357800006866455, train_acc: 84.72, train_fscore: 84.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3501999378204346, test_acc: 71.6, test_fscore: 71.96, time: 6.46 sec
loss: 2.3645808696746826
loss: 2.3355212211608887
loss: 3.3176016807556152
epoch: 144, train_loss: 2.3513998985290527, train_acc: 84.72, train_fscore: 84.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3176000118255615, test_acc: 71.47, test_fscore: 71.83, time: 6.38 sec
loss: 2.2720627784729004
loss: 2.346782684326172
loss: 3.3087148666381836
epoch: 145, train_loss: 2.308199882507324, train_acc: 84.96, train_fscore: 84.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3087000846862793, test_acc: 71.6, test_fscore: 71.91, time: 5.98 sec
loss: 2.279529333114624
loss: 2.363546848297119
loss: 3.3241937160491943
epoch: 146, train_loss: 2.3183999061584473, train_acc: 85.08, train_fscore: 85.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.324199914932251, test_acc: 72.09, test_fscore: 72.46, time: 6.58 sec
loss: 2.247529983520508
loss: 2.384490489959717
loss: 3.321124315261841
epoch: 147, train_loss: 2.312700033187866, train_acc: 85.11, train_fscore: 85.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3210999965667725, test_acc: 72.09, test_fscore: 72.47, time: 6.05 sec
loss: 2.2613842487335205
loss: 2.3955862522125244
loss: 3.3213000297546387
epoch: 148, train_loss: 2.3208999633789062, train_acc: 84.87, train_fscore: 84.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3213000297546387, test_acc: 72.4, test_fscore: 72.74, time: 6.48 sec
loss: 2.2924368381500244
loss: 2.2995078563690186
loss: 3.3372650146484375
epoch: 149, train_loss: 2.2957000732421875, train_acc: 85.18, train_fscore: 85.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3373000621795654, test_acc: 71.84, test_fscore: 72.23, time: 6.43 sec
loss: 2.2600529193878174
loss: 2.3237690925598145
loss: 3.3246872425079346
epoch: 150, train_loss: 2.2892000675201416, train_acc: 85.32, train_fscore: 85.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.324700117111206, test_acc: 71.6, test_fscore: 71.99, time: 6.11 sec
              precision    recall  f1-score   support

           0     0.5625    0.7500    0.6429     144.0
           1     0.8326    0.7714    0.8008     245.0
           2     0.7292    0.7083    0.7186     384.0
           3     0.6337    0.7529    0.6882     170.0
           4     0.8534    0.7592    0.8035     299.0
           5     0.6997    0.6667    0.6828     381.0

    accuracy                         0.7258    1623.0
   macro avg     0.7185    0.7348    0.7228    1623.0
weighted avg     0.7360    0.7258    0.7284    1623.0

[[108.   4.   6.   0.  26.   0.]
 [  2. 189.  21.   3.   0.  30.]
 [ 32.  20. 272.  15.   8.  37.]
 [  0.   0.   1. 128.   0.  41.]
 [ 50.   2.  19.   0. 227.   1.]
 [  0.  12.  54.  56.   5. 254.]]
Test performance..
F-Score: 72.84
F-Score-index: 137
              precision    recall  f1-score   support

           0     0.5625    0.7500    0.6429     144.0
           1     0.8326    0.7714    0.8008     245.0
           2     0.7292    0.7083    0.7186     384.0
           3     0.6337    0.7529    0.6882     170.0
           4     0.8534    0.7592    0.8035     299.0
           5     0.6997    0.6667    0.6828     381.0

    accuracy                         0.7258    1623.0
   macro avg     0.7185    0.7348    0.7228    1623.0
weighted avg     0.7360    0.7258    0.7284    1623.0

[[108.   4.   6.   0.  26.   0.]
 [  2. 189.  21.   3.   0.  30.]
 [ 32.  20. 272.  15.   8.  37.]
 [  0.   0.   1. 128.   0.  41.]
 [ 50.   2.  19.   0. 227.   1.]
 [  0.  12.  54.  56.   5. 254.]]
--- 3 ---
Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.5, batch_size=64, hidden_dim=1024, n_head=8, epochs=150, temp=2, tensorboard=False, class_weight=True, Dataset='IEMOCAP')
Running on GPU
temp 2
total parameters: 79687704
training parameters: 79687704
loss: 7.8907084465026855
loss: 11.964431762695312
loss: 8.384871482849121
epoch: 1, train_loss: 9.700400352478027, train_acc: 17.88, train_fscore: 17.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.384900093078613, test_acc: 26.19, test_fscore: 13.8, time: 8.33 sec
loss: 8.496806144714355
loss: 8.229446411132812
loss: 8.392510414123535
epoch: 2, train_loss: 8.37030029296875, train_acc: 29.35, train_fscore: 24.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.392499923706055, test_acc: 24.83, test_fscore: 15.27, time: 7.13 sec
loss: 8.718520164489746
loss: 8.040262222290039
loss: 7.227564811706543
epoch: 3, train_loss: 8.411800384521484, train_acc: 30.41, train_fscore: 27.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.22760009765625, test_acc: 28.28, test_fscore: 24.05, time: 7.37 sec
loss: 7.4119343757629395
loss: 7.374168872833252
loss: 7.387016296386719
epoch: 4, train_loss: 7.394199848175049, train_acc: 35.56, train_fscore: 30.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.38700008392334, test_acc: 44.79, test_fscore: 36.0, time: 6.45 sec
loss: 7.455524921417236
loss: 7.30806303024292
loss: 7.13595724105835
epoch: 5, train_loss: 7.385900020599365, train_acc: 44.03, train_fscore: 36.9, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.136000156402588, test_acc: 47.07, test_fscore: 40.57, time: 6.75 sec
loss: 7.092371463775635
loss: 6.904192924499512
loss: 6.711827278137207
epoch: 6, train_loss: 7.008399963378906, train_acc: 47.28, train_fscore: 41.66, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.7118000984191895, test_acc: 40.85, test_fscore: 38.43, time: 6.29 sec
loss: 6.796020984649658
loss: 6.681879997253418
loss: 6.690105438232422
epoch: 7, train_loss: 6.7418999671936035, train_acc: 47.81, train_fscore: 45.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.690100193023682, test_acc: 35.86, test_fscore: 34.13, time: 6.05 sec
loss: 6.709893226623535
loss: 6.585171699523926
loss: 6.529341697692871
epoch: 8, train_loss: 6.650100231170654, train_acc: 46.57, train_fscore: 44.63, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.529300212860107, test_acc: 42.7, test_fscore: 43.16, time: 6.44 sec
loss: 6.493616580963135
loss: 6.38520622253418
loss: 6.394165992736816
epoch: 9, train_loss: 6.443900108337402, train_acc: 52.43, train_fscore: 52.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.394199848175049, test_acc: 53.79, test_fscore: 53.54, time: 5.63 sec
loss: 6.288665771484375
loss: 6.22820520401001
loss: 6.262289524078369
epoch: 10, train_loss: 6.261000156402588, train_acc: 57.21, train_fscore: 55.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.26230001449585, test_acc: 56.19, test_fscore: 54.79, time: 6.6 sec
              precision    recall  f1-score   support

           0     0.2782    0.2569    0.2671     144.0
           1     0.6769    0.8122    0.7384     245.0
           2     0.6749    0.3568    0.4668     384.0
           3     0.4201    0.8353    0.5591     170.0
           4     0.6324    0.7826    0.6996     299.0
           5     0.5719    0.4278    0.4895     381.0

    accuracy                         0.5619    1623.0
   macro avg     0.5424    0.5786    0.5367    1623.0
weighted avg     0.5813    0.5619    0.5479    1623.0

[[ 37.  18.   4.  11.  74.   0.]
 [  5. 199.   8.   5.   6.  22.]
 [ 53.  46. 137.  35.  29.  84.]
 [  0.   3.   4. 142.   7.  14.]
 [ 31.   4.  15.  13. 234.   2.]
 [  7.  24.  35. 132.  20. 163.]]
loss: 6.063684463500977
loss: 6.115910530090332
loss: 6.082425594329834
epoch: 11, train_loss: 6.087200164794922, train_acc: 57.92, train_fscore: 55.7, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.082399845123291, test_acc: 55.58, test_fscore: 54.13, time: 5.5 sec
loss: 5.992411136627197
loss: 5.749222755432129
loss: 5.840435981750488
epoch: 12, train_loss: 5.877699851989746, train_acc: 59.83, train_fscore: 57.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.840400218963623, test_acc: 56.01, test_fscore: 55.83, time: 6.75 sec
loss: 5.836184978485107
loss: 5.470758438110352
loss: 5.618750095367432
epoch: 13, train_loss: 5.659200191497803, train_acc: 61.2, train_fscore: 60.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.618800163269043, test_acc: 57.55, test_fscore: 57.55, time: 6.29 sec
loss: 5.421278476715088
loss: 5.579623222351074
loss: 5.430243968963623
epoch: 14, train_loss: 5.496799945831299, train_acc: 62.19, train_fscore: 61.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.430200099945068, test_acc: 59.21, test_fscore: 58.24, time: 6.46 sec
loss: 5.468567848205566
loss: 5.121866226196289
loss: 5.237715721130371
epoch: 15, train_loss: 5.308000087738037, train_acc: 61.36, train_fscore: 59.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.23769998550415, test_acc: 61.98, test_fscore: 60.57, time: 7.05 sec
loss: 5.200712203979492
loss: 5.125711441040039
loss: 5.067101001739502
epoch: 16, train_loss: 5.166800022125244, train_acc: 61.65, train_fscore: 59.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.0671000480651855, test_acc: 64.33, test_fscore: 63.52, time: 5.92 sec
loss: 5.176860809326172
loss: 4.944939136505127
loss: 4.950050354003906
epoch: 17, train_loss: 5.066999912261963, train_acc: 63.22, train_fscore: 61.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.950099945068359, test_acc: 64.26, test_fscore: 64.33, time: 6.6 sec
loss: 4.902498722076416
loss: 5.020404815673828
loss: 4.868899345397949
epoch: 18, train_loss: 4.958700180053711, train_acc: 64.37, train_fscore: 63.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.868899822235107, test_acc: 64.26, test_fscore: 64.56, time: 5.85 sec
loss: 4.86412239074707
loss: 4.822603702545166
loss: 4.747309684753418
epoch: 19, train_loss: 4.84499979019165, train_acc: 65.4, train_fscore: 64.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.747300148010254, test_acc: 63.65, test_fscore: 64.36, time: 6.42 sec
loss: 4.853268623352051
loss: 4.647653579711914
loss: 4.654022216796875
epoch: 20, train_loss: 4.7621002197265625, train_acc: 64.51, train_fscore: 63.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.6539998054504395, test_acc: 63.71, test_fscore: 64.64, time: 6.42 sec
              precision    recall  f1-score   support

           0     0.3294    0.5833    0.4211     144.0
           1     0.7598    0.7102    0.7342     245.0
           2     0.6566    0.5677    0.6089     384.0
           3     0.5971    0.7235    0.6543     170.0
           4     0.8496    0.6421    0.7314     299.0
           5     0.6480    0.6378    0.6429     381.0

    accuracy                         0.6371    1623.0
   macro avg     0.6401    0.6441    0.6321    1623.0
weighted avg     0.6705    0.6371    0.6464    1623.0

[[ 84.  11.  16.   2.  30.   1.]
 [  7. 174.  32.   3.   0.  29.]
 [ 56.  26. 218.  22.   3.  59.]
 [  0.   0.   5. 123.   0.  42.]
 [ 99.   0.   7.   0. 192.   1.]
 [  9.  18.  54.  56.   1. 243.]]
loss: 4.783020496368408
loss: 4.5342254638671875
loss: 4.610635757446289
epoch: 21, train_loss: 4.662799835205078, train_acc: 64.6, train_fscore: 63.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.610599994659424, test_acc: 64.82, test_fscore: 65.3, time: 6.47 sec
loss: 4.8856306076049805
loss: 4.26898193359375
loss: 4.520696640014648
epoch: 22, train_loss: 4.594299793243408, train_acc: 65.75, train_fscore: 65.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.520699977874756, test_acc: 65.5, test_fscore: 65.69, time: 6.78 sec
loss: 4.684902667999268
loss: 4.352173805236816
loss: 4.459773540496826
epoch: 23, train_loss: 4.538099765777588, train_acc: 66.21, train_fscore: 65.43, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.459799766540527, test_acc: 64.26, test_fscore: 64.37, time: 6.14 sec
loss: 4.511857032775879
loss: 4.416823863983154
loss: 4.427850723266602
epoch: 24, train_loss: 4.467599868774414, train_acc: 65.47, train_fscore: 64.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.4278998374938965, test_acc: 65.06, test_fscore: 65.08, time: 6.59 sec
loss: 4.485448837280273
loss: 4.355868816375732
loss: 4.374175548553467
epoch: 25, train_loss: 4.424900054931641, train_acc: 65.63, train_fscore: 64.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.374199867248535, test_acc: 65.31, test_fscore: 65.57, time: 6.91 sec
loss: 4.328193664550781
loss: 4.386567115783691
loss: 4.363295078277588
epoch: 26, train_loss: 4.356800079345703, train_acc: 66.39, train_fscore: 65.62, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.36329984664917, test_acc: 64.39, test_fscore: 64.98, time: 6.56 sec
loss: 4.262286186218262
loss: 4.38802433013916
loss: 4.358615875244141
epoch: 27, train_loss: 4.3171000480651855, train_acc: 67.25, train_fscore: 66.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.35860013961792, test_acc: 64.94, test_fscore: 65.4, time: 6.73 sec
loss: 4.248419761657715
loss: 4.297787189483643
loss: 4.305077075958252
epoch: 28, train_loss: 4.271200180053711, train_acc: 67.01, train_fscore: 66.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.305099964141846, test_acc: 65.62, test_fscore: 65.94, time: 7.38 sec
loss: 4.239584922790527
loss: 4.195146560668945
loss: 4.277836322784424
epoch: 29, train_loss: 4.219600200653076, train_acc: 67.35, train_fscore: 66.63, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.2778000831604, test_acc: 65.43, test_fscore: 65.82, time: 7.17 sec
loss: 4.24923038482666
loss: 4.092570781707764
loss: 4.244775295257568
epoch: 30, train_loss: 4.17579984664917, train_acc: 67.59, train_fscore: 66.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.244800090789795, test_acc: 66.05, test_fscore: 66.23, time: 5.86 sec
              precision    recall  f1-score   support

           0     0.4171    0.5069    0.4577     144.0
           1     0.7511    0.7143    0.7322     245.0
           2     0.6730    0.5521    0.6066     384.0
           3     0.5728    0.7176    0.6371     170.0
           4     0.8328    0.8161    0.8243     299.0
           5     0.6244    0.6457    0.6348     381.0

    accuracy                         0.6605    1623.0
   macro avg     0.6452    0.6588    0.6488    1623.0
weighted avg     0.6696    0.6605    0.6623    1623.0

[[ 73.  11.  14.   6.  39.   1.]
 [  3. 175.  33.   0.   0.  34.]
 [ 50.  25. 212.  26.   5.  66.]
 [  0.   0.   2. 122.   0.  46.]
 [ 46.   0.   8.   0. 244.   1.]
 [  3.  22.  46.  59.   5. 246.]]
loss: 4.080242156982422
loss: 4.2219157218933105
loss: 4.193072319030762
epoch: 31, train_loss: 4.144499778747559, train_acc: 68.31, train_fscore: 67.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.1930999755859375, test_acc: 65.99, test_fscore: 66.37, time: 6.31 sec
loss: 3.91522216796875
loss: 4.326892375946045
loss: 4.174638271331787
epoch: 32, train_loss: 4.105199813842773, train_acc: 68.45, train_fscore: 67.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.174600124359131, test_acc: 65.87, test_fscore: 66.33, time: 5.84 sec
loss: 3.9489059448242188
loss: 4.199497699737549
loss: 4.167264461517334
epoch: 33, train_loss: 4.066500186920166, train_acc: 69.28, train_fscore: 68.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.167300224304199, test_acc: 66.73, test_fscore: 67.04, time: 6.46 sec
loss: 3.976008892059326
loss: 4.061291217803955
loss: 4.142873764038086
epoch: 34, train_loss: 4.016600131988525, train_acc: 69.04, train_fscore: 68.44, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.142899990081787, test_acc: 66.79, test_fscore: 67.28, time: 6.92 sec
loss: 4.071562767028809
loss: 3.864530086517334
loss: 4.091580867767334
epoch: 35, train_loss: 3.9765000343322754, train_acc: 69.76, train_fscore: 69.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.091599941253662, test_acc: 67.04, test_fscore: 67.41, time: 6.06 sec
loss: 3.817934989929199
loss: 4.025569438934326
loss: 4.04885196685791
epoch: 36, train_loss: 3.920300006866455, train_acc: 69.81, train_fscore: 69.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.048900127410889, test_acc: 67.71, test_fscore: 68.02, time: 6.32 sec
loss: 3.929086208343506
loss: 3.8703207969665527
loss: 3.997821569442749
epoch: 37, train_loss: 3.900899887084961, train_acc: 70.36, train_fscore: 69.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.99780011177063, test_acc: 66.73, test_fscore: 67.33, time: 5.41 sec
loss: 3.785036087036133
loss: 3.9600718021392822
loss: 3.9721839427948
epoch: 38, train_loss: 3.8673999309539795, train_acc: 70.59, train_fscore: 70.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9721999168395996, test_acc: 67.53, test_fscore: 67.85, time: 6.94 sec
loss: 3.712033748626709
loss: 3.950483798980713
loss: 3.9627492427825928
epoch: 39, train_loss: 3.8201000690460205, train_acc: 71.24, train_fscore: 70.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9626998901367188, test_acc: 67.9, test_fscore: 68.23, time: 6.26 sec
loss: 3.7597761154174805
loss: 3.828291893005371
loss: 3.9064414501190186
epoch: 40, train_loss: 3.7923998832702637, train_acc: 71.38, train_fscore: 70.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.906399965286255, test_acc: 68.52, test_fscore: 68.84, time: 6.12 sec
              precision    recall  f1-score   support

           0     0.4526    0.5972    0.5150     144.0
           1     0.7802    0.7388    0.7589     245.0
           2     0.7029    0.6224    0.6602     384.0
           3     0.5981    0.7529    0.6667     170.0
           4     0.8459    0.7893    0.8166     299.0
           5     0.6576    0.6352    0.6462     381.0

    accuracy                         0.6852    1623.0
   macro avg     0.6729    0.6893    0.6773    1623.0
weighted avg     0.6971    0.6852    0.6884    1623.0

[[ 86.   9.  13.   5.  30.   1.]
 [  4. 181.  24.   0.   0.  36.]
 [ 45.  22. 239.  22.   8.  48.]
 [  0.   0.   2. 128.   0.  40.]
 [ 53.   1.   8.   0. 236.   1.]
 [  2.  19.  54.  59.   5. 242.]]
loss: 3.6626088619232178
loss: 3.8746120929718018
loss: 3.877284049987793
epoch: 41, train_loss: 3.7637999057769775, train_acc: 71.02, train_fscore: 70.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8773000240325928, test_acc: 67.71, test_fscore: 68.3, time: 5.95 sec
loss: 3.6487185955047607
loss: 3.7848520278930664
loss: 3.8642303943634033
epoch: 42, train_loss: 3.7137999534606934, train_acc: 71.98, train_fscore: 71.66, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8642001152038574, test_acc: 68.21, test_fscore: 68.64, time: 6.67 sec
loss: 3.751901626586914
loss: 3.583787679672241
loss: 3.8307738304138184
epoch: 43, train_loss: 3.674499988555908, train_acc: 72.19, train_fscore: 71.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8308000564575195, test_acc: 68.58, test_fscore: 68.99, time: 7.05 sec
loss: 3.712477445602417
loss: 3.5800228118896484
loss: 3.7932629585266113
epoch: 44, train_loss: 3.65120005607605, train_acc: 71.89, train_fscore: 71.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.793299913406372, test_acc: 68.58, test_fscore: 69.01, time: 5.54 sec
loss: 3.5032401084899902
loss: 3.7638697624206543
loss: 3.7569174766540527
epoch: 45, train_loss: 3.6289000511169434, train_acc: 72.1, train_fscore: 71.75, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7569000720977783, test_acc: 68.58, test_fscore: 68.87, time: 6.47 sec
loss: 3.633056640625
loss: 3.545553684234619
loss: 3.736464738845825
epoch: 46, train_loss: 3.591099977493286, train_acc: 72.91, train_fscore: 72.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7365000247955322, test_acc: 68.7, test_fscore: 69.15, time: 6.55 sec
loss: 3.4605236053466797
loss: 3.672328472137451
loss: 3.717043399810791
epoch: 47, train_loss: 3.5585999488830566, train_acc: 73.08, train_fscore: 72.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7170000076293945, test_acc: 68.76, test_fscore: 69.19, time: 5.83 sec
loss: 3.5757133960723877
loss: 3.4611332416534424
loss: 3.7108099460601807
epoch: 48, train_loss: 3.524399995803833, train_acc: 73.44, train_fscore: 73.17, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7107999324798584, test_acc: 68.88, test_fscore: 69.27, time: 6.59 sec
loss: 3.524057388305664
loss: 3.4857730865478516
loss: 3.687990665435791
epoch: 49, train_loss: 3.506999969482422, train_acc: 73.44, train_fscore: 73.16, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.687999963760376, test_acc: 68.76, test_fscore: 69.16, time: 6.65 sec
loss: 3.4205377101898193
loss: 3.5366532802581787
loss: 3.6552491188049316
epoch: 50, train_loss: 3.476799964904785, train_acc: 73.82, train_fscore: 73.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6552000045776367, test_acc: 68.82, test_fscore: 69.2, time: 6.02 sec
              precision    recall  f1-score   support

           0     0.4541    0.6181    0.5235     144.0
           1     0.7895    0.7347    0.7611     245.0
           2     0.7109    0.6276    0.6667     384.0
           3     0.6150    0.7706    0.6841     170.0
           4     0.8519    0.7692    0.8084     299.0
           5     0.6552    0.6483    0.6517     381.0

    accuracy                         0.6888    1623.0
   macro avg     0.6794    0.6947    0.6826    1623.0
weighted avg     0.7028    0.6888    0.6927    1623.0

[[ 89.   9.  14.   3.  28.   1.]
 [  4. 180.  23.   0.   0.  38.]
 [ 43.  21. 241.  21.   6.  52.]
 [  0.   0.   1. 131.   0.  38.]
 [ 59.   1.   8.   0. 230.   1.]
 [  1.  17.  52.  58.   6. 247.]]
loss: 3.531114101409912
loss: 3.366621971130371
loss: 3.649685859680176
epoch: 51, train_loss: 3.457900047302246, train_acc: 74.1, train_fscore: 73.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6496999263763428, test_acc: 68.88, test_fscore: 69.3, time: 6.28 sec
loss: 3.5309841632843018
loss: 3.3226194381713867
loss: 3.6181397438049316
epoch: 52, train_loss: 3.437000036239624, train_acc: 74.04, train_fscore: 73.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6180999279022217, test_acc: 69.44, test_fscore: 69.81, time: 3.54 sec
loss: 3.369720458984375
loss: 3.460954427719116
loss: 3.62522554397583
epoch: 53, train_loss: 3.4130001068115234, train_acc: 74.01, train_fscore: 73.75, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.625200033187866, test_acc: 69.19, test_fscore: 69.6, time: 3.29 sec
loss: 3.471930980682373
loss: 3.3163156509399414
loss: 3.62025785446167
epoch: 54, train_loss: 3.3968000411987305, train_acc: 74.22, train_fscore: 73.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.620300054550171, test_acc: 69.01, test_fscore: 69.42, time: 4.99 sec
loss: 3.4060006141662598
loss: 3.3207967281341553
loss: 3.57957124710083
epoch: 55, train_loss: 3.3677000999450684, train_acc: 74.92, train_fscore: 74.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5796000957489014, test_acc: 69.32, test_fscore: 69.75, time: 4.67 sec
loss: 3.3422436714172363
loss: 3.3392982482910156
loss: 3.5639028549194336
epoch: 56, train_loss: 3.34089994430542, train_acc: 74.68, train_fscore: 74.44, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5638999938964844, test_acc: 69.13, test_fscore: 69.62, time: 5.89 sec
loss: 3.3535072803497314
loss: 3.3304262161254883
loss: 3.5707616806030273
epoch: 57, train_loss: 3.3429999351501465, train_acc: 74.46, train_fscore: 74.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5708000659942627, test_acc: 68.88, test_fscore: 69.25, time: 5.82 sec
loss: 3.3159103393554688
loss: 3.2760167121887207
loss: 3.5770583152770996
epoch: 58, train_loss: 3.2980000972747803, train_acc: 75.51, train_fscore: 75.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5771000385284424, test_acc: 68.88, test_fscore: 69.31, time: 5.59 sec
loss: 3.3464198112487793
loss: 3.240901231765747
loss: 3.5388495922088623
epoch: 59, train_loss: 3.3001999855041504, train_acc: 75.08, train_fscore: 74.85, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.538800001144409, test_acc: 69.19, test_fscore: 69.62, time: 6.83 sec
loss: 3.2173218727111816
loss: 3.363121271133423
loss: 3.502361297607422
epoch: 60, train_loss: 3.286900043487549, train_acc: 75.32, train_fscore: 75.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5023999214172363, test_acc: 69.44, test_fscore: 69.78, time: 6.36 sec
              precision    recall  f1-score   support

           0     0.4611    0.6181    0.5282     144.0
           1     0.7863    0.7510    0.7683     245.0
           2     0.7155    0.6484    0.6803     384.0
           3     0.6294    0.7294    0.6757     170.0
           4     0.8493    0.7726    0.8091     299.0
           5     0.6596    0.6562    0.6579     381.0

    accuracy                         0.6944    1623.0
   macro avg     0.6836    0.6959    0.6866    1623.0
weighted avg     0.7061    0.6944    0.6981    1623.0

[[ 89.   9.  16.   1.  28.   1.]
 [  4. 184.  20.   0.   0.  37.]
 [ 43.  20. 249.  21.   6.  45.]
 [  0.   0.   1. 124.   0.  45.]
 [ 57.   2.   8.   0. 231.   1.]
 [  0.  19.  54.  51.   7. 250.]]
loss: 3.185521364212036
loss: 3.333441972732544
loss: 3.514303684234619
epoch: 61, train_loss: 3.253999948501587, train_acc: 76.21, train_fscore: 75.97, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5143001079559326, test_acc: 68.88, test_fscore: 69.32, time: 6.28 sec
loss: 3.17434024810791
loss: 3.3072710037231445
loss: 3.5080456733703613
epoch: 62, train_loss: 3.233599901199341, train_acc: 76.47, train_fscore: 76.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.507999897003174, test_acc: 69.38, test_fscore: 69.82, time: 6.6 sec
loss: 3.0412216186523438
loss: 3.417696952819824
loss: 3.488527774810791
epoch: 63, train_loss: 3.222100019454956, train_acc: 76.51, train_fscore: 76.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4885001182556152, test_acc: 69.5, test_fscore: 69.9, time: 5.23 sec
loss: 3.2427380084991455
loss: 3.10060977935791
loss: 3.4683849811553955
epoch: 64, train_loss: 3.176800012588501, train_acc: 77.06, train_fscore: 76.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.468400001525879, test_acc: 69.56, test_fscore: 69.98, time: 6.16 sec
loss: 3.223250389099121
loss: 3.143695592880249
loss: 3.4766385555267334
epoch: 65, train_loss: 3.1884000301361084, train_acc: 76.09, train_fscore: 75.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.476599931716919, test_acc: 69.75, test_fscore: 70.16, time: 6.29 sec
loss: 3.094625234603882
loss: 3.252800464630127
loss: 3.470259428024292
epoch: 66, train_loss: 3.168800115585327, train_acc: 76.28, train_fscore: 76.12, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4702999591827393, test_acc: 69.69, test_fscore: 70.11, time: 6.8 sec
loss: 3.185183048248291
loss: 3.111544609069824
loss: 3.4434614181518555
epoch: 67, train_loss: 3.151900053024292, train_acc: 76.11, train_fscore: 75.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.44350004196167, test_acc: 69.69, test_fscore: 70.07, time: 6.07 sec
loss: 2.9688358306884766
loss: 3.297475576400757
loss: 3.4498226642608643
epoch: 68, train_loss: 3.121299982070923, train_acc: 77.14, train_fscore: 76.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4498000144958496, test_acc: 70.12, test_fscore: 70.55, time: 6.91 sec
loss: 3.1417713165283203
loss: 3.120405673980713
loss: 3.4265637397766113
epoch: 69, train_loss: 3.1317999362945557, train_acc: 76.78, train_fscore: 76.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4265999794006348, test_acc: 69.62, test_fscore: 70.05, time: 5.99 sec
loss: 3.1131505966186523
loss: 3.0779592990875244
loss: 3.3925468921661377
epoch: 70, train_loss: 3.0968000888824463, train_acc: 77.09, train_fscore: 76.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3924999237060547, test_acc: 70.12, test_fscore: 70.54, time: 6.15 sec
              precision    recall  f1-score   support

           0     0.4834    0.7083    0.5746     144.0
           1     0.8026    0.7469    0.7738     245.0
           2     0.7241    0.6562    0.6885     384.0
           3     0.6250    0.7353    0.6757     170.0
           4     0.8736    0.7625    0.8143     299.0
           5     0.6613    0.6509    0.6561     381.0

    accuracy                         0.7012    1623.0
   macro avg     0.6950    0.7100    0.6972    1623.0
weighted avg     0.7170    0.7012    0.7055    1623.0

[[102.   9.  12.   0.  21.   0.]
 [  3. 183.  21.   0.   0.  38.]
 [ 45.  18. 252.  20.   5.  44.]
 [  0.   0.   1. 125.   0.  44.]
 [ 60.   2.   8.   0. 228.   1.]
 [  1.  16.  54.  55.   7. 248.]]
loss: 3.1728854179382324
loss: 2.9963698387145996
loss: 3.399244546890259
epoch: 71, train_loss: 3.087899923324585, train_acc: 77.11, train_fscore: 76.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.399199962615967, test_acc: 70.49, test_fscore: 70.83, time: 6.51 sec
loss: 3.191009521484375
loss: 2.922037124633789
loss: 3.4162817001342773
epoch: 72, train_loss: 3.064300060272217, train_acc: 77.97, train_fscore: 77.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.416300058364868, test_acc: 70.36, test_fscore: 70.82, time: 5.94 sec
loss: 3.067878246307373
loss: 3.0199553966522217
loss: 3.3839879035949707
epoch: 73, train_loss: 3.0462000370025635, train_acc: 77.59, train_fscore: 77.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.384000062942505, test_acc: 70.43, test_fscore: 70.79, time: 6.42 sec
loss: 3.176731586456299
loss: 2.894591808319092
loss: 3.3676421642303467
epoch: 74, train_loss: 3.0495998859405518, train_acc: 77.26, train_fscore: 77.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3675999641418457, test_acc: 70.43, test_fscore: 70.91, time: 5.58 sec
loss: 3.1819162368774414
loss: 2.869746685028076
loss: 3.356462240219116
epoch: 75, train_loss: 3.0378000736236572, train_acc: 77.76, train_fscore: 77.63, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3564999103546143, test_acc: 70.43, test_fscore: 70.79, time: 6.1 sec
loss: 3.2083044052124023
loss: 2.75456166267395
loss: 3.3836662769317627
epoch: 76, train_loss: 2.9990999698638916, train_acc: 77.61, train_fscore: 77.44, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.383699893951416, test_acc: 70.73, test_fscore: 71.08, time: 6.22 sec
loss: 2.9529166221618652
loss: 3.0638113021850586
loss: 3.3901619911193848
epoch: 77, train_loss: 3.000999927520752, train_acc: 78.33, train_fscore: 78.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.390199899673462, test_acc: 70.49, test_fscore: 70.92, time: 6.28 sec
loss: 2.966738700866699
loss: 3.0062408447265625
loss: 3.358497142791748
epoch: 78, train_loss: 2.98580002784729, train_acc: 78.28, train_fscore: 78.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3585000038146973, test_acc: 70.49, test_fscore: 70.83, time: 6.51 sec
loss: 3.0594089031219482
loss: 2.8628013134002686
loss: 3.337141752243042
epoch: 79, train_loss: 2.9697000980377197, train_acc: 78.23, train_fscore: 78.09, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.337100028991699, test_acc: 70.73, test_fscore: 71.13, time: 5.93 sec
loss: 3.1037678718566895
loss: 2.7992095947265625
loss: 3.3317651748657227
epoch: 80, train_loss: 2.960700035095215, train_acc: 78.28, train_fscore: 78.16, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3317999839782715, test_acc: 70.61, test_fscore: 71.0, time: 6.61 sec
              precision    recall  f1-score   support

           0     0.5023    0.7431    0.5994     144.0
           1     0.8079    0.7551    0.7806     245.0
           2     0.7335    0.6667    0.6985     384.0
           3     0.6327    0.7294    0.6776     170.0
           4     0.8783    0.7726    0.8221     299.0
           5     0.6568    0.6430    0.6499     381.0

    accuracy                         0.7073    1623.0
   macro avg     0.7019    0.7183    0.7047    1623.0
weighted avg     0.7223    0.7073    0.7113    1623.0

[[107.   9.   9.   0.  19.   0.]
 [  2. 185.  19.   0.   0.  39.]
 [ 45.  18. 256.  17.   5.  43.]
 [  0.   0.   1. 124.   0.  45.]
 [ 58.   2.   7.   0. 231.   1.]
 [  1.  15.  57.  55.   8. 245.]]
loss: 3.112313985824585
loss: 2.8281073570251465
loss: 3.314178228378296
epoch: 81, train_loss: 2.9772000312805176, train_acc: 78.42, train_fscore: 78.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.314199924468994, test_acc: 70.61, test_fscore: 70.94, time: 5.8 sec
loss: 2.8852386474609375
loss: 2.9804983139038086
loss: 3.355477809906006
epoch: 82, train_loss: 2.9314000606536865, train_acc: 79.17, train_fscore: 79.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3554999828338623, test_acc: 70.67, test_fscore: 71.09, time: 7.12 sec
loss: 3.077857494354248
loss: 2.7437551021575928
loss: 3.3646347522735596
epoch: 83, train_loss: 2.925100088119507, train_acc: 79.02, train_fscore: 78.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3645999431610107, test_acc: 70.43, test_fscore: 70.86, time: 5.97 sec
loss: 3.0295379161834717
loss: 2.776609420776367
loss: 3.3501152992248535
epoch: 84, train_loss: 2.910599946975708, train_acc: 78.92, train_fscore: 78.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.350100040435791, test_acc: 70.79, test_fscore: 71.16, time: 5.24 sec
loss: 2.777970790863037
loss: 3.0504117012023926
loss: 3.325841188430786
epoch: 85, train_loss: 2.905100107192993, train_acc: 79.16, train_fscore: 79.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3257999420166016, test_acc: 70.67, test_fscore: 71.08, time: 6.97 sec
loss: 2.9312331676483154
loss: 2.8395891189575195
loss: 3.3257813453674316
epoch: 86, train_loss: 2.8868000507354736, train_acc: 79.38, train_fscore: 79.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3257999420166016, test_acc: 71.23, test_fscore: 71.58, time: 5.62 sec
loss: 2.9841620922088623
loss: 2.7645368576049805
loss: 3.328188180923462
epoch: 87, train_loss: 2.8808999061584473, train_acc: 79.17, train_fscore: 79.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.328200101852417, test_acc: 71.16, test_fscore: 71.49, time: 6.13 sec
loss: 2.9375123977661133
loss: 2.7887446880340576
loss: 3.319610118865967
epoch: 88, train_loss: 2.867799997329712, train_acc: 79.66, train_fscore: 79.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3196001052856445, test_acc: 70.98, test_fscore: 71.37, time: 6.67 sec
loss: 2.6197147369384766
loss: 3.1064200401306152
loss: 3.3180346488952637
epoch: 89, train_loss: 2.8427999019622803, train_acc: 79.86, train_fscore: 79.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.318000078201294, test_acc: 71.1, test_fscore: 71.49, time: 6.14 sec
loss: 2.89896297454834
loss: 2.806313991546631
loss: 3.2898950576782227
epoch: 90, train_loss: 2.8564999103546143, train_acc: 79.79, train_fscore: 79.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.289900064468384, test_acc: 71.04, test_fscore: 71.42, time: 5.68 sec
              precision    recall  f1-score   support

           0     0.5000    0.7292    0.5932     144.0
           1     0.8085    0.7755    0.7917     245.0
           2     0.7500    0.6641    0.7044     384.0
           3     0.6341    0.7647    0.6933     170.0
           4     0.8684    0.7726    0.8177     299.0
           5     0.6676    0.6430    0.6551     381.0

    accuracy                         0.7123    1623.0
   macro avg     0.7048    0.7248    0.7092    1623.0
weighted avg     0.7270    0.7123    0.7158    1623.0

[[105.   9.   8.   0.  22.   0.]
 [  2. 190.  16.   0.   0.  37.]
 [ 44.  18. 255.  17.   5.  45.]
 [  0.   0.   1. 130.   0.  39.]
 [ 58.   2.   7.   0. 231.   1.]
 [  1.  16.  53.  58.   8. 245.]]
loss: 2.847471237182617
loss: 2.8473410606384277
loss: 3.300488233566284
epoch: 91, train_loss: 2.847399950027466, train_acc: 79.76, train_fscore: 79.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.30049991607666, test_acc: 71.47, test_fscore: 71.85, time: 6.39 sec
loss: 2.8108561038970947
loss: 2.827579975128174
loss: 3.3170619010925293
epoch: 92, train_loss: 2.8183000087738037, train_acc: 80.12, train_fscore: 80.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3171000480651855, test_acc: 71.29, test_fscore: 71.7, time: 5.26 sec
loss: 2.7398264408111572
loss: 2.8621037006378174
loss: 3.327022075653076
epoch: 93, train_loss: 2.7960000038146973, train_acc: 80.31, train_fscore: 80.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3269999027252197, test_acc: 70.86, test_fscore: 71.24, time: 6.26 sec
loss: 2.860377550125122
loss: 2.7162065505981445
loss: 3.3119723796844482
epoch: 94, train_loss: 2.7960000038146973, train_acc: 79.6, train_fscore: 79.44, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.312000036239624, test_acc: 70.92, test_fscore: 71.28, time: 6.22 sec
loss: 2.8229970932006836
loss: 2.714883327484131
loss: 3.300201177597046
epoch: 95, train_loss: 2.7720000743865967, train_acc: 80.33, train_fscore: 80.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3001999855041504, test_acc: 71.6, test_fscore: 71.93, time: 6.57 sec
loss: 2.6956186294555664
loss: 2.8850669860839844
loss: 3.3006556034088135
epoch: 96, train_loss: 2.789299964904785, train_acc: 80.28, train_fscore: 80.15, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3006999492645264, test_acc: 71.35, test_fscore: 71.71, time: 5.86 sec
loss: 2.8034496307373047
loss: 2.724578619003296
loss: 3.269882917404175
epoch: 97, train_loss: 2.767400026321411, train_acc: 80.64, train_fscore: 80.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.26990008354187, test_acc: 71.16, test_fscore: 71.52, time: 6.3 sec
loss: 2.7335572242736816
loss: 2.7785534858703613
loss: 3.2796270847320557
epoch: 98, train_loss: 2.7544000148773193, train_acc: 80.77, train_fscore: 80.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.279599905014038, test_acc: 71.23, test_fscore: 71.55, time: 5.57 sec
loss: 2.7193706035614014
loss: 2.7753329277038574
loss: 3.3124682903289795
epoch: 99, train_loss: 2.7453999519348145, train_acc: 80.65, train_fscore: 80.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3125, test_acc: 71.04, test_fscore: 71.41, time: 6.08 sec
loss: 2.7592928409576416
loss: 2.686038017272949
loss: 3.28926157951355
epoch: 100, train_loss: 2.724600076675415, train_acc: 80.64, train_fscore: 80.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.289299964904785, test_acc: 71.04, test_fscore: 71.39, time: 6.03 sec
              precision    recall  f1-score   support

           0     0.5171    0.7361    0.6074     144.0
           1     0.8274    0.7633    0.7941     245.0
           2     0.7393    0.6719    0.7040     384.0
           3     0.6528    0.7412    0.6942     170.0
           4     0.8603    0.7826    0.8196     299.0
           5     0.6640    0.6588    0.6614     381.0

    accuracy                         0.7160    1623.0
   macro avg     0.7102    0.7256    0.7134    1623.0
weighted avg     0.7284    0.7160    0.7193    1623.0

[[106.   6.   8.   0.  24.   0.]
 [  2. 187.  18.   1.   0.  37.]
 [ 41.  17. 258.  16.   6.  46.]
 [  0.   0.   1. 126.   0.  43.]
 [ 55.   1.   8.   0. 234.   1.]
 [  1.  15.  56.  50.   8. 251.]]
loss: 2.7589430809020996
loss: 2.691192626953125
loss: 3.2594895362854004
epoch: 101, train_loss: 2.7274999618530273, train_acc: 80.74, train_fscore: 80.63, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.259500026702881, test_acc: 71.41, test_fscore: 71.78, time: 6.11 sec
loss: 2.6265921592712402
loss: 2.8205883502960205
loss: 3.2695984840393066
epoch: 102, train_loss: 2.716099977493286, train_acc: 81.0, train_fscore: 80.89, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2695999145507812, test_acc: 71.84, test_fscore: 72.23, time: 5.64 sec
loss: 2.6960713863372803
loss: 2.7478346824645996
loss: 3.2637970447540283
epoch: 103, train_loss: 2.719099998474121, train_acc: 80.72, train_fscore: 80.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2637999057769775, test_acc: 71.16, test_fscore: 71.5, time: 6.33 sec
loss: 2.649430274963379
loss: 2.7372899055480957
loss: 3.2827351093292236
epoch: 104, train_loss: 2.6882998943328857, train_acc: 81.08, train_fscore: 80.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2827000617980957, test_acc: 71.53, test_fscore: 71.93, time: 6.19 sec
loss: 2.793905735015869
loss: 2.5600876808166504
loss: 3.2906360626220703
epoch: 105, train_loss: 2.6946001052856445, train_acc: 81.22, train_fscore: 81.12, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.290600061416626, test_acc: 70.36, test_fscore: 70.68, time: 6.41 sec
loss: 2.657074213027954
loss: 2.6977787017822266
loss: 3.317267417907715
epoch: 106, train_loss: 2.6756999492645264, train_acc: 81.79, train_fscore: 81.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3173000812530518, test_acc: 71.1, test_fscore: 71.43, time: 6.6 sec
loss: 2.641927480697632
loss: 2.709153652191162
loss: 3.294307231903076
epoch: 107, train_loss: 2.6740000247955322, train_acc: 81.51, train_fscore: 81.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.294300079345703, test_acc: 70.67, test_fscore: 71.08, time: 6.54 sec
loss: 2.776602268218994
loss: 2.5105855464935303
loss: 3.2338247299194336
epoch: 108, train_loss: 2.649399995803833, train_acc: 81.15, train_fscore: 81.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.233799934387207, test_acc: 71.72, test_fscore: 72.06, time: 5.92 sec
loss: 2.7692606449127197
loss: 2.5307679176330566
loss: 3.2431578636169434
epoch: 109, train_loss: 2.6531999111175537, train_acc: 82.17, train_fscore: 82.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2432000637054443, test_acc: 71.41, test_fscore: 71.86, time: 6.67 sec
loss: 2.6215007305145264
loss: 2.631983757019043
loss: 3.25950288772583
epoch: 110, train_loss: 2.626699924468994, train_acc: 81.96, train_fscore: 81.89, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.259500026702881, test_acc: 71.9, test_fscore: 72.3, time: 6.36 sec
              precision    recall  f1-score   support

           0     0.5200    0.8125    0.6341     144.0
           1     0.8333    0.7755    0.8034     245.0
           2     0.7407    0.6771    0.7075     384.0
           3     0.6495    0.7412    0.6923     170.0
           4     0.8911    0.7391    0.8080     299.0
           5     0.6711    0.6640    0.6675     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7176    0.7349    0.7188    1623.0
weighted avg     0.7369    0.7190    0.7230    1623.0

[[117.   4.   7.   0.  16.   0.]
 [  2. 190.  16.   1.   0.  36.]
 [ 43.  18. 260.  15.   4.  44.]
 [  0.   0.   1. 126.   0.  43.]
 [ 62.   1.  14.   0. 221.   1.]
 [  1.  15.  53.  52.   7. 253.]]
loss: 2.6877312660217285
loss: 2.5654845237731934
loss: 3.278043270111084
epoch: 111, train_loss: 2.6308000087738037, train_acc: 81.67, train_fscore: 81.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2780001163482666, test_acc: 71.41, test_fscore: 71.73, time: 6.28 sec
loss: 2.647458076477051
loss: 2.5671591758728027
loss: 3.259286880493164
epoch: 112, train_loss: 2.610300064086914, train_acc: 81.88, train_fscore: 81.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2592999935150146, test_acc: 71.41, test_fscore: 71.76, time: 5.81 sec
loss: 2.6407361030578613
loss: 2.588433265686035
loss: 3.2489922046661377
epoch: 113, train_loss: 2.6161000728607178, train_acc: 81.91, train_fscore: 81.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.249000072479248, test_acc: 71.97, test_fscore: 72.25, time: 6.86 sec
loss: 2.604444980621338
loss: 2.562321901321411
loss: 3.260000228881836
epoch: 114, train_loss: 2.5845000743865967, train_acc: 82.55, train_fscore: 82.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.259999990463257, test_acc: 71.84, test_fscore: 72.19, time: 7.41 sec
loss: 2.550100803375244
loss: 2.627652168273926
loss: 3.257936477661133
epoch: 115, train_loss: 2.5885000228881836, train_acc: 82.38, train_fscore: 82.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2578999996185303, test_acc: 71.97, test_fscore: 72.36, time: 5.85 sec
loss: 2.528933525085449
loss: 2.619323968887329
loss: 3.2544772624969482
epoch: 116, train_loss: 2.5708000659942627, train_acc: 82.15, train_fscore: 82.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.254499912261963, test_acc: 71.35, test_fscore: 71.63, time: 6.55 sec
loss: 2.558173656463623
loss: 2.5813777446746826
loss: 3.240743398666382
epoch: 117, train_loss: 2.569499969482422, train_acc: 82.46, train_fscore: 82.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2407000064849854, test_acc: 71.23, test_fscore: 71.57, time: 5.7 sec
loss: 2.525329113006592
loss: 2.612450122833252
loss: 3.2426154613494873
epoch: 118, train_loss: 2.5666000843048096, train_acc: 82.13, train_fscore: 82.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2425999641418457, test_acc: 71.84, test_fscore: 72.2, time: 6.69 sec
loss: 2.5704731941223145
loss: 2.5599231719970703
loss: 3.251171588897705
epoch: 119, train_loss: 2.565500020980835, train_acc: 82.77, train_fscore: 82.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.251199960708618, test_acc: 71.72, test_fscore: 72.1, time: 5.71 sec
loss: 2.55859112739563
loss: 2.505919933319092
loss: 3.2662322521209717
epoch: 120, train_loss: 2.5332999229431152, train_acc: 82.67, train_fscore: 82.57, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.266200065612793, test_acc: 71.47, test_fscore: 71.87, time: 6.21 sec
              precision    recall  f1-score   support

           0     0.5180    0.7986    0.6284     144.0
           1     0.8348    0.7837    0.8084     245.0
           2     0.7333    0.6875    0.7097     384.0
           3     0.6480    0.7471    0.6940     170.0
           4     0.8902    0.7324    0.8037     299.0
           5     0.6802    0.6588    0.6693     381.0

    accuracy                         0.7197    1623.0
   macro avg     0.7174    0.7347    0.7189    1623.0
weighted avg     0.7370    0.7197    0.7236    1623.0

[[115.   5.   7.   0.  17.   0.]
 [  2. 192.  17.   1.   0.  33.]
 [ 40.  18. 264.  16.   4.  42.]
 [  0.   0.   1. 127.   0.  42.]
 [ 65.   0.  14.   0. 219.   1.]
 [  0.  15.  57.  52.   6. 251.]]
loss: 2.56017804145813
loss: 2.5109024047851562
loss: 3.2582650184631348
epoch: 121, train_loss: 2.5380001068115234, train_acc: 82.53, train_fscore: 82.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2583000659942627, test_acc: 71.66, test_fscore: 72.01, time: 6.19 sec
loss: 2.4341824054718018
loss: 2.6376705169677734
loss: 3.24458384513855
epoch: 122, train_loss: 2.5281999111175537, train_acc: 83.22, train_fscore: 83.12, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2446000576019287, test_acc: 71.78, test_fscore: 72.21, time: 6.4 sec
loss: 2.352182626724243
loss: 2.7198262214660645
loss: 3.250319004058838
epoch: 123, train_loss: 2.5297000408172607, train_acc: 82.79, train_fscore: 82.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2502999305725098, test_acc: 71.9, test_fscore: 72.31, time: 6.6 sec
loss: 2.578801155090332
loss: 2.438693046569824
loss: 3.277884006500244
epoch: 124, train_loss: 2.515500068664551, train_acc: 83.27, train_fscore: 83.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.277899980545044, test_acc: 71.78, test_fscore: 72.19, time: 5.2 sec
loss: 2.4826958179473877
loss: 2.5200557708740234
loss: 3.267731189727783
epoch: 125, train_loss: 2.500499963760376, train_acc: 83.12, train_fscore: 83.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.267699956893921, test_acc: 70.92, test_fscore: 71.25, time: 6.38 sec
loss: 2.3725686073303223
loss: 2.6728920936584473
loss: 3.2532405853271484
epoch: 126, train_loss: 2.514699935913086, train_acc: 83.2, train_fscore: 83.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.253200054168701, test_acc: 71.84, test_fscore: 72.2, time: 5.96 sec
loss: 2.470912218093872
loss: 2.517204761505127
loss: 3.2502498626708984
epoch: 127, train_loss: 2.4932000637054443, train_acc: 83.67, train_fscore: 83.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2502999305725098, test_acc: 71.35, test_fscore: 71.74, time: 6.41 sec
loss: 2.51041841506958
loss: 2.4563915729522705
loss: 3.2684502601623535
epoch: 128, train_loss: 2.484100103378296, train_acc: 83.48, train_fscore: 83.37, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2685000896453857, test_acc: 71.66, test_fscore: 72.05, time: 6.5 sec
loss: 2.391343593597412
loss: 2.59846568107605
loss: 3.2679502964019775
epoch: 129, train_loss: 2.4846999645233154, train_acc: 83.41, train_fscore: 83.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2679998874664307, test_acc: 71.04, test_fscore: 71.48, time: 5.97 sec
loss: 2.5048747062683105
loss: 2.3996238708496094
loss: 3.2791647911071777
epoch: 130, train_loss: 2.4586000442504883, train_acc: 83.41, train_fscore: 83.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2792000770568848, test_acc: 71.23, test_fscore: 71.65, time: 6.43 sec
              precision    recall  f1-score   support

           0     0.5180    0.7986    0.6284     144.0
           1     0.8348    0.7837    0.8084     245.0
           2     0.7333    0.6875    0.7097     384.0
           3     0.6480    0.7471    0.6940     170.0
           4     0.8902    0.7324    0.8037     299.0
           5     0.6802    0.6588    0.6693     381.0

    accuracy                         0.7197    1623.0
   macro avg     0.7174    0.7347    0.7189    1623.0
weighted avg     0.7370    0.7197    0.7236    1623.0

[[115.   5.   7.   0.  17.   0.]
 [  2. 192.  17.   1.   0.  33.]
 [ 40.  18. 264.  16.   4.  42.]
 [  0.   0.   1. 127.   0.  42.]
 [ 65.   0.  14.   0. 219.   1.]
 [  0.  15.  57.  52.   6. 251.]]
loss: 2.4192848205566406
loss: 2.4535200595855713
loss: 3.273038625717163
epoch: 131, train_loss: 2.4349000453948975, train_acc: 83.72, train_fscore: 83.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2730000019073486, test_acc: 71.41, test_fscore: 71.77, time: 5.56 sec
loss: 2.452345848083496
loss: 2.452634334564209
loss: 3.2708218097686768
epoch: 132, train_loss: 2.452500104904175, train_acc: 83.77, train_fscore: 83.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2708001136779785, test_acc: 70.98, test_fscore: 71.36, time: 6.33 sec
loss: 2.452481985092163
loss: 2.468074321746826
loss: 3.281663417816162
epoch: 133, train_loss: 2.460099935531616, train_acc: 84.2, train_fscore: 84.12, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2816998958587646, test_acc: 71.35, test_fscore: 71.75, time: 6.03 sec
loss: 2.4447689056396484
loss: 2.4432079792022705
loss: 3.2921674251556396
epoch: 134, train_loss: 2.444000005722046, train_acc: 84.18, train_fscore: 84.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2922000885009766, test_acc: 71.04, test_fscore: 71.45, time: 6.5 sec
loss: 2.436202049255371
loss: 2.407257556915283
loss: 3.2964229583740234
epoch: 135, train_loss: 2.4226999282836914, train_acc: 83.67, train_fscore: 83.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2964000701904297, test_acc: 71.35, test_fscore: 71.72, time: 5.91 sec
loss: 2.453443765640259
loss: 2.418295383453369
loss: 3.261007308959961
epoch: 136, train_loss: 2.4377999305725098, train_acc: 83.98, train_fscore: 83.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.260999917984009, test_acc: 71.47, test_fscore: 71.84, time: 6.37 sec
loss: 2.5039424896240234
loss: 2.3700177669525146
loss: 3.2774033546447754
epoch: 137, train_loss: 2.44320011138916, train_acc: 84.04, train_fscore: 83.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.277400016784668, test_acc: 71.41, test_fscore: 71.84, time: 6.68 sec
loss: 2.3554458618164062
loss: 2.4802422523498535
loss: 3.2803423404693604
epoch: 138, train_loss: 2.4096999168395996, train_acc: 84.39, train_fscore: 84.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2802999019622803, test_acc: 71.23, test_fscore: 71.61, time: 5.96 sec
loss: 2.370473861694336
loss: 2.425180673599243
loss: 3.2607624530792236
epoch: 139, train_loss: 2.3968000411987305, train_acc: 84.23, train_fscore: 84.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2607998847961426, test_acc: 71.78, test_fscore: 72.19, time: 6.55 sec
loss: 2.472731113433838
loss: 2.3017239570617676
loss: 3.263800621032715
epoch: 140, train_loss: 2.393699884414673, train_acc: 84.13, train_fscore: 84.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2637999057769775, test_acc: 71.6, test_fscore: 71.96, time: 6.39 sec
              precision    recall  f1-score   support

           0     0.5180    0.7986    0.6284     144.0
           1     0.8348    0.7837    0.8084     245.0
           2     0.7333    0.6875    0.7097     384.0
           3     0.6480    0.7471    0.6940     170.0
           4     0.8902    0.7324    0.8037     299.0
           5     0.6802    0.6588    0.6693     381.0

    accuracy                         0.7197    1623.0
   macro avg     0.7174    0.7347    0.7189    1623.0
weighted avg     0.7370    0.7197    0.7236    1623.0

[[115.   5.   7.   0.  17.   0.]
 [  2. 192.  17.   1.   0.  33.]
 [ 40.  18. 264.  16.   4.  42.]
 [  0.   0.   1. 127.   0.  42.]
 [ 65.   0.  14.   0. 219.   1.]
 [  0.  15.  57.  52.   6. 251.]]
loss: 2.4614481925964355
loss: 2.2817749977111816
loss: 3.28859806060791
epoch: 141, train_loss: 2.3742001056671143, train_acc: 84.37, train_fscore: 84.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.288599967956543, test_acc: 71.6, test_fscore: 71.96, time: 6.78 sec
loss: 2.4914064407348633
loss: 2.2649989128112793
loss: 3.2932965755462646
epoch: 142, train_loss: 2.3879001140594482, train_acc: 84.34, train_fscore: 84.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.293299913406372, test_acc: 71.41, test_fscore: 71.78, time: 6.25 sec
loss: 2.431156873703003
loss: 2.289764881134033
loss: 3.3042736053466797
epoch: 143, train_loss: 2.3696000576019287, train_acc: 84.34, train_fscore: 84.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.30430006980896, test_acc: 71.53, test_fscore: 71.93, time: 5.99 sec
loss: 2.441582202911377
loss: 2.298670768737793
loss: 3.2979283332824707
epoch: 144, train_loss: 2.3752999305725098, train_acc: 84.08, train_fscore: 83.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2978999614715576, test_acc: 71.23, test_fscore: 71.63, time: 6.43 sec
loss: 2.4440975189208984
loss: 2.2798056602478027
loss: 3.3097686767578125
epoch: 145, train_loss: 2.3673999309539795, train_acc: 84.68, train_fscore: 84.57, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.309799909591675, test_acc: 71.29, test_fscore: 71.64, time: 6.48 sec
loss: 2.4306087493896484
loss: 2.280414581298828
loss: 3.312523365020752
epoch: 146, train_loss: 2.3592000007629395, train_acc: 84.92, train_fscore: 84.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3125, test_acc: 71.9, test_fscore: 72.29, time: 5.88 sec
loss: 2.2046120166778564
loss: 2.5122268199920654
loss: 3.2969870567321777
epoch: 147, train_loss: 2.3538999557495117, train_acc: 84.8, train_fscore: 84.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.296999931335449, test_acc: 71.78, test_fscore: 72.25, time: 6.49 sec
loss: 2.3851802349090576
loss: 2.259629964828491
loss: 3.3057422637939453
epoch: 148, train_loss: 2.3269999027252197, train_acc: 85.11, train_fscore: 85.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3057000637054443, test_acc: 71.29, test_fscore: 71.7, time: 6.38 sec
loss: 2.2517971992492676
loss: 2.420766830444336
loss: 3.3299856185913086
epoch: 149, train_loss: 2.3341000080108643, train_acc: 85.08, train_fscore: 84.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3299999237060547, test_acc: 71.04, test_fscore: 71.45, time: 6.55 sec
loss: 2.3035836219787598
loss: 2.3417117595672607
loss: 3.309110641479492
epoch: 150, train_loss: 2.3208999633789062, train_acc: 84.54, train_fscore: 84.43, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3090999126434326, test_acc: 71.23, test_fscore: 71.59, time: 6.3 sec
              precision    recall  f1-score   support

           0     0.5180    0.7986    0.6284     144.0
           1     0.8348    0.7837    0.8084     245.0
           2     0.7333    0.6875    0.7097     384.0
           3     0.6480    0.7471    0.6940     170.0
           4     0.8902    0.7324    0.8037     299.0
           5     0.6802    0.6588    0.6693     381.0

    accuracy                         0.7197    1623.0
   macro avg     0.7174    0.7347    0.7189    1623.0
weighted avg     0.7370    0.7197    0.7236    1623.0

[[115.   5.   7.   0.  17.   0.]
 [  2. 192.  17.   1.   0.  33.]
 [ 40.  18. 264.  16.   4.  42.]
 [  0.   0.   1. 127.   0.  42.]
 [ 65.   0.  14.   0. 219.   1.]
 [  0.  15.  57.  52.   6. 251.]]
Test performance..
F-Score: 72.36
F-Score-index: 115
              precision    recall  f1-score   support

           0     0.5180    0.7986    0.6284     144.0
           1     0.8348    0.7837    0.8084     245.0
           2     0.7333    0.6875    0.7097     384.0
           3     0.6480    0.7471    0.6940     170.0
           4     0.8902    0.7324    0.8037     299.0
           5     0.6802    0.6588    0.6693     381.0

    accuracy                         0.7197    1623.0
   macro avg     0.7174    0.7347    0.7189    1623.0
weighted avg     0.7370    0.7197    0.7236    1623.0

[[115.   5.   7.   0.  17.   0.]
 [  2. 192.  17.   1.   0.  33.]
 [ 40.  18. 264.  16.   4.  42.]
 [  0.   0.   1. 127.   0.  42.]
 [ 65.   0.  14.   0. 219.   1.]
 [  0.  15.  57.  52.   6. 251.]]
--- 4 ---
Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.5, batch_size=64, hidden_dim=1024, n_head=8, epochs=150, temp=2, tensorboard=False, class_weight=True, Dataset='IEMOCAP')
Running on GPU
temp 2
total parameters: 79687704
training parameters: 79687704
loss: 7.9306111335754395
loss: 11.193896293640137
loss: 7.7545552253723145
epoch: 1, train_loss: 9.52180004119873, train_acc: 15.77, train_fscore: 15.89, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.7546000480651855, test_acc: 29.02, test_fscore: 23.28, time: 8.79 sec
loss: 7.9356608390808105
loss: 8.611207962036133
loss: 8.256281852722168
epoch: 2, train_loss: 8.24590015411377, train_acc: 27.26, train_fscore: 23.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.25629997253418, test_acc: 36.41, test_fscore: 30.15, time: 5.79 sec
loss: 8.418998718261719
loss: 7.818544387817383
loss: 7.376153469085693
epoch: 3, train_loss: 8.127799987792969, train_acc: 31.17, train_fscore: 31.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.376200199127197, test_acc: 29.88, test_fscore: 22.77, time: 6.01 sec
loss: 7.582647323608398
loss: 7.2812089920043945
loss: 6.996629238128662
epoch: 4, train_loss: 7.445700168609619, train_acc: 32.72, train_fscore: 23.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.996600151062012, test_acc: 44.42, test_fscore: 35.65, time: 7.11 sec
loss: 7.134909152984619
loss: 7.193232536315918
loss: 7.089059352874756
epoch: 5, train_loss: 7.161900043487549, train_acc: 45.56, train_fscore: 41.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.089099884033203, test_acc: 38.2, test_fscore: 37.9, time: 5.87 sec
loss: 7.129055500030518
loss: 6.93792724609375
loss: 6.628138065338135
epoch: 6, train_loss: 7.040299892425537, train_acc: 44.92, train_fscore: 42.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.6280999183654785, test_acc: 47.87, test_fscore: 46.16, time: 6.19 sec
loss: 6.735447883605957
loss: 6.481001377105713
loss: 6.535755157470703
epoch: 7, train_loss: 6.609399795532227, train_acc: 53.89, train_fscore: 51.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.535799980163574, test_acc: 56.01, test_fscore: 55.14, time: 6.92 sec
loss: 6.564664840698242
loss: 6.520171642303467
loss: 6.55138635635376
epoch: 8, train_loss: 6.54449987411499, train_acc: 54.32, train_fscore: 52.17, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.551400184631348, test_acc: 49.72, test_fscore: 48.52, time: 6.39 sec
loss: 6.532473564147949
loss: 6.332571029663086
loss: 6.296836853027344
epoch: 9, train_loss: 6.442599773406982, train_acc: 52.75, train_fscore: 51.5, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.296800136566162, test_acc: 52.99, test_fscore: 53.71, time: 6.08 sec
loss: 6.190993309020996
loss: 6.098269939422607
loss: 6.102428436279297
epoch: 10, train_loss: 6.145899772644043, train_acc: 58.62, train_fscore: 58.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.102399826049805, test_acc: 55.08, test_fscore: 55.19, time: 6.36 sec
              precision    recall  f1-score   support

           0     0.3155    0.3681    0.3397     144.0
           1     0.6620    0.7755    0.7143     245.0
           2     0.4504    0.4844    0.4668     384.0
           3     0.5388    0.6529    0.5904     170.0
           4     0.7118    0.5452    0.6174     299.0
           5     0.5969    0.5013    0.5449     381.0

    accuracy                         0.5508    1623.0
   macro avg     0.5459    0.5546    0.5456    1623.0
weighted avg     0.5622    0.5508    0.5519    1623.0

[[ 53.  16.  27.   4.  42.   2.]
 [  2. 190.  33.   3.   2.  15.]
 [ 59.  46. 186.  16.  11.  66.]
 [  0.   1.  22. 111.   4.  32.]
 [ 48.   7.  62.   5. 163.  14.]
 [  6.  27.  83.  67.   7. 191.]]
loss: 5.920845031738281
loss: 5.987678050994873
loss: 5.953315734863281
epoch: 11, train_loss: 5.951600074768066, train_acc: 61.14, train_fscore: 60.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.9532999992370605, test_acc: 54.59, test_fscore: 54.5, time: 6.84 sec
loss: 5.809685230255127
loss: 5.770619869232178
loss: 5.731508255004883
epoch: 12, train_loss: 5.791800022125244, train_acc: 60.12, train_fscore: 59.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.731500148773193, test_acc: 53.85, test_fscore: 54.01, time: 5.75 sec
loss: 5.512415885925293
loss: 5.635624408721924
loss: 5.455231189727783
epoch: 13, train_loss: 5.569900035858154, train_acc: 59.62, train_fscore: 58.09, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.4552001953125, test_acc: 58.66, test_fscore: 58.19, time: 7.12 sec
loss: 5.239672660827637
loss: 5.463868141174316
loss: 5.292787551879883
epoch: 14, train_loss: 5.345399856567383, train_acc: 61.94, train_fscore: 60.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.292799949645996, test_acc: 61.18, test_fscore: 60.79, time: 6.37 sec
loss: 5.356144428253174
loss: 5.06514310836792
loss: 5.194515228271484
epoch: 15, train_loss: 5.2129998207092285, train_acc: 63.03, train_fscore: 61.49, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.194499969482422, test_acc: 62.11, test_fscore: 61.86, time: 5.87 sec
loss: 5.131953239440918
loss: 5.094787120819092
loss: 5.05366849899292
epoch: 16, train_loss: 5.114799976348877, train_acc: 63.65, train_fscore: 62.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.053699970245361, test_acc: 62.05, test_fscore: 61.86, time: 6.24 sec
loss: 5.108942031860352
loss: 4.8353352546691895
loss: 4.920255661010742
epoch: 17, train_loss: 4.987500190734863, train_acc: 63.75, train_fscore: 62.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.920300006866455, test_acc: 61.86, test_fscore: 62.41, time: 6.15 sec
loss: 5.003596305847168
loss: 4.727716445922852
loss: 4.82807731628418
epoch: 18, train_loss: 4.878900051116943, train_acc: 64.2, train_fscore: 63.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.828100204467773, test_acc: 61.37, test_fscore: 62.13, time: 6.66 sec
loss: 4.913445949554443
loss: 4.6234588623046875
loss: 4.728102207183838
epoch: 19, train_loss: 4.772200107574463, train_acc: 63.91, train_fscore: 63.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.728099822998047, test_acc: 62.91, test_fscore: 63.67, time: 5.34 sec
loss: 4.702660083770752
loss: 4.696001052856445
loss: 4.615182876586914
epoch: 20, train_loss: 4.6996002197265625, train_acc: 64.66, train_fscore: 63.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.615200042724609, test_acc: 64.57, test_fscore: 64.86, time: 6.98 sec
              precision    recall  f1-score   support

           0     0.3616    0.4444    0.3988     144.0
           1     0.7556    0.6939    0.7234     245.0
           2     0.6543    0.5964    0.6240     384.0
           3     0.5500    0.7765    0.6439     170.0
           4     0.7986    0.7692    0.7836     299.0
           5     0.6501    0.5853    0.6160     381.0

    accuracy                         0.6457    1623.0
   macro avg     0.6284    0.6443    0.6316    1623.0
weighted avg     0.6583    0.6457    0.6486    1623.0

[[ 64.  12.  14.   5.  48.   1.]
 [  4. 170.  39.   4.   0.  28.]
 [ 44.  23. 229.  27.   5.  56.]
 [  0.   0.   4. 132.   0.  34.]
 [ 61.   0.   7.   0. 230.   1.]
 [  4.  20.  57.  72.   5. 223.]]
loss: 4.686372756958008
loss: 4.484617710113525
loss: 4.566735744476318
epoch: 21, train_loss: 4.594699859619141, train_acc: 65.42, train_fscore: 64.62, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.566699981689453, test_acc: 64.14, test_fscore: 64.57, time: 6.29 sec
loss: 4.716609001159668
loss: 4.3657145500183105
loss: 4.494844436645508
epoch: 22, train_loss: 4.552199840545654, train_acc: 65.18, train_fscore: 64.37, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.494800090789795, test_acc: 64.08, test_fscore: 64.16, time: 6.45 sec
loss: 4.529910564422607
loss: 4.460580825805664
loss: 4.435393333435059
epoch: 23, train_loss: 4.497499942779541, train_acc: 64.97, train_fscore: 63.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.435400009155273, test_acc: 64.76, test_fscore: 65.01, time: 6.42 sec
loss: 4.493581295013428
loss: 4.310816287994385
loss: 4.449966907501221
epoch: 24, train_loss: 4.405900001525879, train_acc: 65.75, train_fscore: 64.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.449999809265137, test_acc: 63.96, test_fscore: 64.64, time: 6.54 sec
loss: 4.37089729309082
loss: 4.411026954650879
loss: 4.427615642547607
epoch: 25, train_loss: 4.389599800109863, train_acc: 66.3, train_fscore: 65.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.427599906921387, test_acc: 64.57, test_fscore: 64.98, time: 6.12 sec
loss: 4.404048442840576
loss: 4.205856800079346
loss: 4.383691310882568
epoch: 26, train_loss: 4.314599990844727, train_acc: 67.19, train_fscore: 66.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.383699893951416, test_acc: 64.26, test_fscore: 64.82, time: 6.06 sec
loss: 4.133188724517822
loss: 4.4166741371154785
loss: 4.329676628112793
epoch: 27, train_loss: 4.268700122833252, train_acc: 67.5, train_fscore: 66.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.329699993133545, test_acc: 65.13, test_fscore: 65.58, time: 6.45 sec
loss: 4.285521984100342
loss: 4.18853235244751
loss: 4.298145294189453
epoch: 28, train_loss: 4.238999843597412, train_acc: 68.09, train_fscore: 67.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.298099994659424, test_acc: 65.62, test_fscore: 66.0, time: 5.23 sec
loss: 4.255942344665527
loss: 4.1159281730651855
loss: 4.267138957977295
epoch: 29, train_loss: 4.188399791717529, train_acc: 68.18, train_fscore: 67.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.267099857330322, test_acc: 65.5, test_fscore: 65.91, time: 6.31 sec
loss: 4.1156840324401855
loss: 4.170716285705566
loss: 4.24583625793457
epoch: 30, train_loss: 4.141900062561035, train_acc: 68.19, train_fscore: 67.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.245800018310547, test_acc: 66.05, test_fscore: 66.43, time: 6.06 sec
              precision    recall  f1-score   support

           0     0.4082    0.5556    0.4706     144.0
           1     0.7426    0.7184    0.7303     245.0
           2     0.6899    0.5677    0.6229     384.0
           3     0.5708    0.7118    0.6335     170.0
           4     0.8381    0.7793    0.8076     299.0
           5     0.6354    0.6404    0.6379     381.0

    accuracy                         0.6605    1623.0
   macro avg     0.6475    0.6622    0.6505    1623.0
weighted avg     0.6749    0.6605    0.6643    1623.0

[[ 80.  10.  12.   6.  35.   1.]
 [  3. 176.  33.   0.   0.  33.]
 [ 49.  26. 218.  26.   6.  59.]
 [  0.   0.   3. 121.   0.  46.]
 [ 58.   0.   7.   0. 233.   1.]
 [  6.  25.  43.  59.   4. 244.]]
loss: 4.171370983123779
loss: 3.9843363761901855
loss: 4.2163615226745605
epoch: 31, train_loss: 4.0808000564575195, train_acc: 69.24, train_fscore: 68.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.216400146484375, test_acc: 66.3, test_fscore: 66.66, time: 6.43 sec
loss: 4.170776844024658
loss: 3.918842077255249
loss: 4.1617584228515625
epoch: 32, train_loss: 4.054500102996826, train_acc: 68.92, train_fscore: 68.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.161799907684326, test_acc: 65.99, test_fscore: 66.5, time: 6.18 sec
loss: 4.098431587219238
loss: 3.8990204334259033
loss: 4.112520217895508
epoch: 33, train_loss: 4.013299942016602, train_acc: 69.19, train_fscore: 68.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.112500190734863, test_acc: 66.67, test_fscore: 66.99, time: 6.55 sec
loss: 4.065272331237793
loss: 3.8709640502929688
loss: 4.079326629638672
epoch: 34, train_loss: 3.9737000465393066, train_acc: 69.85, train_fscore: 69.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.0792999267578125, test_acc: 66.73, test_fscore: 67.12, time: 6.14 sec
loss: 4.057947158813477
loss: 3.7867462635040283
loss: 4.039248943328857
epoch: 35, train_loss: 3.932499885559082, train_acc: 70.21, train_fscore: 69.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.0391998291015625, test_acc: 66.97, test_fscore: 67.5, time: 6.06 sec
loss: 4.1409406661987305
loss: 3.5748376846313477
loss: 4.008452415466309
epoch: 36, train_loss: 3.8766000270843506, train_acc: 70.64, train_fscore: 70.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.008500099182129, test_acc: 67.22, test_fscore: 67.61, time: 6.29 sec
loss: 3.863237142562866
loss: 3.7816946506500244
loss: 3.9780564308166504
epoch: 37, train_loss: 3.8248000144958496, train_acc: 71.07, train_fscore: 70.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.978100061416626, test_acc: 67.53, test_fscore: 67.86, time: 6.03 sec
loss: 3.8287081718444824
loss: 3.762551784515381
loss: 3.9348957538604736
epoch: 38, train_loss: 3.7981998920440674, train_acc: 71.69, train_fscore: 71.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9349000453948975, test_acc: 67.9, test_fscore: 68.24, time: 6.1 sec
loss: 3.848071813583374
loss: 3.663066864013672
loss: 3.895777463912964
epoch: 39, train_loss: 3.7595999240875244, train_acc: 71.74, train_fscore: 71.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8958001136779785, test_acc: 68.27, test_fscore: 68.61, time: 6.29 sec
loss: 3.6688060760498047
loss: 3.82574200630188
loss: 3.8879923820495605
epoch: 40, train_loss: 3.745800018310547, train_acc: 71.81, train_fscore: 71.47, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.888000011444092, test_acc: 67.84, test_fscore: 68.32, time: 6.75 sec
              precision    recall  f1-score   support

           0     0.4503    0.5972    0.5134     144.0
           1     0.7729    0.7224    0.7468     245.0
           2     0.6806    0.6380    0.6586     384.0
           3     0.6173    0.7118    0.6612     170.0
           4     0.8429    0.7893    0.8152     299.0
           5     0.6621    0.6378    0.6497     381.0

    accuracy                         0.6827    1623.0
   macro avg     0.6710    0.6828    0.6742    1623.0
weighted avg     0.6930    0.6827    0.6861    1623.0

[[ 86.   8.  15.   2.  32.   1.]
 [  5. 177.  31.   1.   0.  31.]
 [ 43.  24. 245.  21.   6.  45.]
 [  0.   0.   3. 121.   0.  46.]
 [ 54.   0.   8.   0. 236.   1.]
 [  3.  20.  58.  51.   6. 243.]]
loss: 3.707254409790039
loss: 3.702275514602661
loss: 3.8869454860687256
epoch: 41, train_loss: 3.7049999237060547, train_acc: 72.39, train_fscore: 72.09, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.886899948120117, test_acc: 67.9, test_fscore: 68.34, time: 5.51 sec
loss: 3.68318510055542
loss: 3.60715913772583
loss: 3.8260326385498047
epoch: 42, train_loss: 3.649600028991699, train_acc: 72.27, train_fscore: 71.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8259999752044678, test_acc: 67.78, test_fscore: 68.19, time: 6.65 sec
loss: 3.645038366317749
loss: 3.6062991619110107
loss: 3.7830135822296143
epoch: 43, train_loss: 3.626699924468994, train_acc: 72.86, train_fscore: 72.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7829999923706055, test_acc: 68.33, test_fscore: 68.79, time: 7.12 sec
loss: 3.6469037532806396
loss: 3.5605084896087646
loss: 3.7787678241729736
epoch: 44, train_loss: 3.605799913406372, train_acc: 72.67, train_fscore: 72.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7788000106811523, test_acc: 68.27, test_fscore: 68.71, time: 5.13 sec
loss: 3.388821601867676
loss: 3.7769834995269775
loss: 3.787777900695801
epoch: 45, train_loss: 3.575000047683716, train_acc: 72.91, train_fscore: 72.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7878000736236572, test_acc: 68.21, test_fscore: 68.68, time: 6.04 sec
loss: 3.596914052963257
loss: 3.5002408027648926
loss: 3.769084930419922
epoch: 46, train_loss: 3.5529000759124756, train_acc: 73.27, train_fscore: 73.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7690999507904053, test_acc: 68.52, test_fscore: 68.94, time: 6.18 sec
loss: 3.4553699493408203
loss: 3.579256296157837
loss: 3.7299652099609375
epoch: 47, train_loss: 3.514899969100952, train_acc: 73.65, train_fscore: 73.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7300000190734863, test_acc: 68.7, test_fscore: 69.1, time: 6.83 sec
loss: 3.554112195968628
loss: 3.4344725608825684
loss: 3.7099549770355225
epoch: 48, train_loss: 3.4981000423431396, train_acc: 73.41, train_fscore: 73.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7100000381469727, test_acc: 69.01, test_fscore: 69.42, time: 6.58 sec
loss: 3.370695114135742
loss: 3.568549871444702
loss: 3.683152198791504
epoch: 49, train_loss: 3.462899923324585, train_acc: 74.04, train_fscore: 73.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.683199882507324, test_acc: 69.01, test_fscore: 69.41, time: 5.73 sec
loss: 3.3419318199157715
loss: 3.526780128479004
loss: 3.6590116024017334
epoch: 50, train_loss: 3.428800106048584, train_acc: 73.99, train_fscore: 73.66, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6589999198913574, test_acc: 68.82, test_fscore: 69.27, time: 6.43 sec
              precision    recall  f1-score   support

           0     0.4495    0.6181    0.5205     144.0
           1     0.7802    0.7388    0.7589     245.0
           2     0.7025    0.6458    0.6730     384.0
           3     0.6188    0.7353    0.6720     170.0
           4     0.8470    0.7592    0.8007     299.0
           5     0.6757    0.6562    0.6658     381.0

    accuracy                         0.6901    1623.0
   macro avg     0.6790    0.6922    0.6818    1623.0
weighted avg     0.7033    0.6901    0.6942    1623.0

[[ 89.   9.  17.   1.  28.   0.]
 [  5. 181.  25.   1.   0.  33.]
 [ 42.  22. 248.  23.   6.  43.]
 [  0.   0.   2. 125.   0.  43.]
 [ 62.   1.   8.   0. 227.   1.]
 [  0.  19.  53.  52.   7. 250.]]
loss: 3.478933334350586
loss: 3.318352699279785
loss: 3.6633195877075195
epoch: 51, train_loss: 3.405400037765503, train_acc: 74.15, train_fscore: 73.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.663300037384033, test_acc: 68.64, test_fscore: 69.06, time: 6.07 sec
loss: 3.3365864753723145
loss: 3.4344727993011475
loss: 3.6398282051086426
epoch: 52, train_loss: 3.3835999965667725, train_acc: 74.32, train_fscore: 74.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6398000717163086, test_acc: 68.76, test_fscore: 69.14, time: 6.05 sec
loss: 3.242943286895752
loss: 3.488560676574707
loss: 3.6282429695129395
epoch: 53, train_loss: 3.359800100326538, train_acc: 74.87, train_fscore: 74.62, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.628200054168701, test_acc: 68.52, test_fscore: 69.07, time: 6.44 sec
loss: 3.191300392150879
loss: 3.5254786014556885
loss: 3.5845022201538086
epoch: 54, train_loss: 3.3401999473571777, train_acc: 74.49, train_fscore: 74.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5845000743865967, test_acc: 69.13, test_fscore: 69.52, time: 5.64 sec
loss: 3.398193359375
loss: 3.227297782897949
loss: 3.585998058319092
epoch: 55, train_loss: 3.3148999214172363, train_acc: 74.58, train_fscore: 74.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5859999656677246, test_acc: 68.7, test_fscore: 69.11, time: 6.6 sec
loss: 3.364680290222168
loss: 3.2479448318481445
loss: 3.5743918418884277
epoch: 56, train_loss: 3.3097000122070312, train_acc: 75.18, train_fscore: 75.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.574399948120117, test_acc: 69.25, test_fscore: 69.67, time: 5.63 sec
loss: 3.3298585414886475
loss: 3.207916259765625
loss: 3.5387802124023438
epoch: 57, train_loss: 3.2690999507904053, train_acc: 75.63, train_fscore: 75.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.538800001144409, test_acc: 69.56, test_fscore: 69.97, time: 6.77 sec
loss: 3.2424123287200928
loss: 3.3088371753692627
loss: 3.548865556716919
epoch: 58, train_loss: 3.273099899291992, train_acc: 75.59, train_fscore: 75.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5488998889923096, test_acc: 68.58, test_fscore: 69.11, time: 6.41 sec
loss: 3.1825754642486572
loss: 3.293628692626953
loss: 3.5294036865234375
epoch: 59, train_loss: 3.233299970626831, train_acc: 75.99, train_fscore: 75.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.529400110244751, test_acc: 69.32, test_fscore: 69.71, time: 6.27 sec
loss: 3.272301197052002
loss: 3.1747641563415527
loss: 3.53703236579895
epoch: 60, train_loss: 3.224400043487549, train_acc: 75.71, train_fscore: 75.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5369999408721924, test_acc: 69.13, test_fscore: 69.54, time: 6.72 sec
              precision    recall  f1-score   support

           0     0.4683    0.6667    0.5501     144.0
           1     0.7948    0.7429    0.7679     245.0
           2     0.7127    0.6589    0.6847     384.0
           3     0.6139    0.7294    0.6667     170.0
           4     0.8513    0.7659    0.8063     299.0
           5     0.6749    0.6430    0.6586     381.0

    accuracy                         0.6956    1623.0
   macro avg     0.6860    0.7011    0.6891    1623.0
weighted avg     0.7097    0.6956    0.6997    1623.0

[[ 96.   7.  15.   0.  26.   0.]
 [  5. 182.  24.   2.   0.  32.]
 [ 42.  20. 253.  22.   6.  41.]
 [  0.   0.   2. 124.   0.  44.]
 [ 60.   2.   7.   0. 229.   1.]
 [  2.  18.  54.  54.   8. 245.]]
loss: 3.0346012115478516
loss: 3.435930013656616
loss: 3.505523204803467
epoch: 61, train_loss: 3.2281999588012695, train_acc: 75.75, train_fscore: 75.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.505500078201294, test_acc: 69.62, test_fscore: 70.13, time: 6.28 sec
loss: 3.222774028778076
loss: 3.1603074073791504
loss: 3.471714735031128
epoch: 62, train_loss: 3.1946001052856445, train_acc: 76.45, train_fscore: 76.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4716999530792236, test_acc: 69.69, test_fscore: 70.13, time: 6.22 sec
loss: 3.2146260738372803
loss: 3.0968799591064453
loss: 3.487225294113159
epoch: 63, train_loss: 3.1630001068115234, train_acc: 76.04, train_fscore: 75.85, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4872000217437744, test_acc: 69.32, test_fscore: 69.85, time: 6.69 sec
loss: 3.153856039047241
loss: 3.1492207050323486
loss: 3.457775115966797
epoch: 64, train_loss: 3.151700019836426, train_acc: 76.66, train_fscore: 76.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4577999114990234, test_acc: 69.5, test_fscore: 69.94, time: 5.78 sec
loss: 3.0084755420684814
loss: 3.2597737312316895
loss: 3.46584153175354
epoch: 65, train_loss: 3.126300096511841, train_acc: 76.68, train_fscore: 76.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4658000469207764, test_acc: 69.5, test_fscore: 69.9, time: 6.26 sec
loss: 3.005523204803467
loss: 3.2243075370788574
loss: 3.4756927490234375
epoch: 66, train_loss: 3.1115000247955322, train_acc: 77.35, train_fscore: 77.17, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4756999015808105, test_acc: 69.56, test_fscore: 70.12, time: 7.32 sec
loss: 3.1164495944976807
loss: 3.064640760421753
loss: 3.4613964557647705
epoch: 67, train_loss: 3.092400074005127, train_acc: 77.23, train_fscore: 77.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.461400032043457, test_acc: 69.44, test_fscore: 70.02, time: 6.27 sec
loss: 3.155256748199463
loss: 3.025686025619507
loss: 3.437577247619629
epoch: 68, train_loss: 3.094599962234497, train_acc: 76.78, train_fscore: 76.64, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4375998973846436, test_acc: 70.36, test_fscore: 70.81, time: 6.48 sec
loss: 2.9768893718719482
loss: 3.225862979888916
loss: 3.430690288543701
epoch: 69, train_loss: 3.093100070953369, train_acc: 77.21, train_fscore: 77.06, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4307000637054443, test_acc: 69.87, test_fscore: 70.33, time: 6.62 sec
loss: 3.2478649616241455
loss: 2.8549087047576904
loss: 3.411550998687744
epoch: 70, train_loss: 3.0643999576568604, train_acc: 77.19, train_fscore: 77.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.411600112915039, test_acc: 70.36, test_fscore: 70.8, time: 6.43 sec
              precision    recall  f1-score   support

           0     0.4837    0.7222    0.5794     144.0
           1     0.8153    0.7388    0.7752     245.0
           2     0.7297    0.6536    0.6896     384.0
           3     0.6302    0.7118    0.6685     170.0
           4     0.8647    0.7692    0.8142     299.0
           5     0.6641    0.6693    0.6667     381.0

    accuracy                         0.7036    1623.0
   macro avg     0.6979    0.7108    0.6989    1623.0
weighted avg     0.7198    0.7036    0.7081    1623.0

[[104.   6.  11.   0.  23.   0.]
 [  6. 181.  24.   1.   0.  33.]
 [ 44.  18. 251.  19.   5.  47.]
 [  0.   0.   1. 121.   0.  48.]
 [ 60.   2.   6.   0. 230.   1.]
 [  1.  15.  51.  51.   8. 255.]]
loss: 3.0082645416259766
loss: 3.04925537109375
loss: 3.393573522567749
epoch: 71, train_loss: 3.0278000831604004, train_acc: 77.35, train_fscore: 77.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3935999870300293, test_acc: 70.36, test_fscore: 70.77, time: 7.33 sec
loss: 2.8662586212158203
loss: 3.2251646518707275
loss: 3.398176431655884
epoch: 72, train_loss: 3.0399999618530273, train_acc: 77.31, train_fscore: 77.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.398200035095215, test_acc: 70.3, test_fscore: 70.76, time: 6.08 sec
loss: 3.045297145843506
loss: 2.980679512023926
loss: 3.3950483798980713
epoch: 73, train_loss: 3.0162999629974365, train_acc: 77.97, train_fscore: 77.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3949999809265137, test_acc: 70.61, test_fscore: 70.91, time: 6.52 sec
loss: 2.900322675704956
loss: 3.1118364334106445
loss: 3.3879621028900146
epoch: 74, train_loss: 3.0037999153137207, train_acc: 77.78, train_fscore: 77.59, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.388000011444092, test_acc: 70.67, test_fscore: 71.07, time: 5.72 sec
loss: 2.9905292987823486
loss: 2.9939212799072266
loss: 3.3855342864990234
epoch: 75, train_loss: 2.9921000003814697, train_acc: 78.28, train_fscore: 78.15, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.385499954223633, test_acc: 70.55, test_fscore: 71.01, time: 6.95 sec
loss: 2.972957134246826
loss: 2.971083641052246
loss: 3.3913168907165527
epoch: 76, train_loss: 2.972100019454956, train_acc: 78.42, train_fscore: 78.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3912999629974365, test_acc: 70.18, test_fscore: 70.64, time: 6.37 sec
loss: 2.832674980163574
loss: 3.132390022277832
loss: 3.3797218799591064
epoch: 77, train_loss: 2.9735000133514404, train_acc: 78.57, train_fscore: 78.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.379699945449829, test_acc: 70.36, test_fscore: 70.69, time: 3.93 sec
loss: 3.0054969787597656
loss: 2.879323720932007
loss: 3.351388931274414
epoch: 78, train_loss: 2.9456000328063965, train_acc: 78.06, train_fscore: 77.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3513998985290527, test_acc: 71.04, test_fscore: 71.41, time: 2.62 sec
loss: 2.973081111907959
loss: 2.9110465049743652
loss: 3.3489837646484375
epoch: 79, train_loss: 2.942500114440918, train_acc: 79.07, train_fscore: 78.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3489999771118164, test_acc: 70.55, test_fscore: 70.96, time: 4.99 sec
loss: 2.9339518547058105
loss: 2.871962308883667
loss: 3.3488893508911133
epoch: 80, train_loss: 2.9038000106811523, train_acc: 78.67, train_fscore: 78.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.348900079727173, test_acc: 70.86, test_fscore: 71.25, time: 5.07 sec
              precision    recall  f1-score   support

           0     0.5095    0.7431    0.6045     144.0
           1     0.8190    0.7388    0.7768     245.0
           2     0.7291    0.6589    0.6922     384.0
           3     0.6392    0.7294    0.6813     170.0
           4     0.8667    0.7826    0.8225     299.0
           5     0.6667    0.6667    0.6667     381.0

    accuracy                         0.7104    1623.0
   macro avg     0.7050    0.7199    0.7073    1623.0
weighted avg     0.7245    0.7104    0.7141    1623.0

[[107.   5.  10.   0.  22.   0.]
 [  4. 181.  25.   0.   0.  35.]
 [ 43.  18. 253.  18.   6.  46.]
 [  0.   0.   1. 124.   0.  45.]
 [ 55.   2.   7.   0. 234.   1.]
 [  1.  15.  51.  52.   8. 254.]]
loss: 2.8366174697875977
loss: 2.995213508605957
loss: 3.3407952785491943
epoch: 81, train_loss: 2.9121999740600586, train_acc: 79.35, train_fscore: 79.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3408000469207764, test_acc: 70.73, test_fscore: 71.05, time: 5.31 sec
loss: 2.970015048980713
loss: 2.79650616645813
loss: 3.3404221534729004
epoch: 82, train_loss: 2.887200117111206, train_acc: 79.04, train_fscore: 78.89, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.340399980545044, test_acc: 70.92, test_fscore: 71.29, time: 6.18 sec
loss: 2.9182286262512207
loss: 2.852724075317383
loss: 3.3531007766723633
epoch: 83, train_loss: 2.8886001110076904, train_acc: 78.61, train_fscore: 78.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.353100061416626, test_acc: 70.98, test_fscore: 71.35, time: 6.14 sec
loss: 2.85479736328125
loss: 2.8942298889160156
loss: 3.3443233966827393
epoch: 84, train_loss: 2.8745999336242676, train_acc: 79.17, train_fscore: 79.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3443000316619873, test_acc: 70.98, test_fscore: 71.35, time: 6.14 sec
loss: 2.8270187377929688
loss: 2.888812780380249
loss: 3.3213727474212646
epoch: 85, train_loss: 2.8570001125335693, train_acc: 79.5, train_fscore: 79.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3213999271392822, test_acc: 71.1, test_fscore: 71.44, time: 6.59 sec
loss: 2.745476007461548
loss: 2.9359405040740967
loss: 3.316235065460205
epoch: 86, train_loss: 2.8352999687194824, train_acc: 79.43, train_fscore: 79.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.316200017929077, test_acc: 71.1, test_fscore: 71.45, time: 6.54 sec
loss: 2.9368767738342285
loss: 2.7134268283843994
loss: 3.340334892272949
epoch: 87, train_loss: 2.8341000080108643, train_acc: 79.59, train_fscore: 79.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3403000831604004, test_acc: 71.16, test_fscore: 71.51, time: 5.65 sec
loss: 2.7357635498046875
loss: 2.944667339324951
loss: 3.3310773372650146
epoch: 88, train_loss: 2.833199977874756, train_acc: 79.91, train_fscore: 79.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3310999870300293, test_acc: 71.41, test_fscore: 71.78, time: 6.03 sec
loss: 2.8105883598327637
loss: 2.8073344230651855
loss: 3.2983577251434326
epoch: 89, train_loss: 2.8090999126434326, train_acc: 80.21, train_fscore: 80.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2983999252319336, test_acc: 71.35, test_fscore: 71.71, time: 6.44 sec
loss: 2.84197998046875
loss: 2.758316993713379
loss: 3.293020725250244
epoch: 90, train_loss: 2.8025999069213867, train_acc: 79.72, train_fscore: 79.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2929999828338623, test_acc: 71.6, test_fscore: 71.96, time: 6.35 sec
              precision    recall  f1-score   support

           0     0.5185    0.7778    0.6222     144.0
           1     0.8304    0.7592    0.7932     245.0
           2     0.7255    0.6745    0.6991     384.0
           3     0.6368    0.7529    0.6900     170.0
           4     0.8774    0.7659    0.8179     299.0
           5     0.6813    0.6509    0.6658     381.0

    accuracy                         0.7160    1623.0
   macro avg     0.7116    0.7302    0.7147    1623.0
weighted avg     0.7313    0.7160    0.7196    1623.0

[[112.   5.   7.   0.  20.   0.]
 [  2. 186.  23.   3.   0.  31.]
 [ 44.  17. 259.  17.   4.  43.]
 [  0.   0.   1. 128.   0.  41.]
 [ 57.   2.  10.   0. 229.   1.]
 [  1.  14.  57.  53.   8. 248.]]
loss: 2.839864730834961
loss: 2.76088547706604
loss: 3.296271324157715
epoch: 91, train_loss: 2.8015999794006348, train_acc: 79.72, train_fscore: 79.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.296299934387207, test_acc: 71.41, test_fscore: 71.79, time: 5.23 sec
loss: 2.8795480728149414
loss: 2.679403781890869
loss: 3.302422523498535
epoch: 92, train_loss: 2.7860000133514404, train_acc: 79.93, train_fscore: 79.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3024001121520996, test_acc: 71.53, test_fscore: 71.91, time: 6.7 sec
loss: 2.772956609725952
loss: 2.7799265384674072
loss: 3.2933075428009033
epoch: 93, train_loss: 2.7760000228881836, train_acc: 79.67, train_fscore: 79.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.293299913406372, test_acc: 71.41, test_fscore: 71.77, time: 5.83 sec
loss: 2.7636818885803223
loss: 2.757517099380493
loss: 3.27950382232666
epoch: 94, train_loss: 2.7609000205993652, train_acc: 80.36, train_fscore: 80.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2795000076293945, test_acc: 71.23, test_fscore: 71.59, time: 6.38 sec
loss: 2.7993240356445312
loss: 2.7278308868408203
loss: 3.2947440147399902
epoch: 95, train_loss: 2.765700101852417, train_acc: 80.31, train_fscore: 80.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2946999073028564, test_acc: 70.92, test_fscore: 71.29, time: 6.49 sec
loss: 2.6383891105651855
loss: 2.839709758758545
loss: 3.3011374473571777
epoch: 96, train_loss: 2.7344000339508057, train_acc: 80.84, train_fscore: 80.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.301100015640259, test_acc: 71.23, test_fscore: 71.6, time: 6.52 sec
loss: 2.8973135948181152
loss: 2.557610511779785
loss: 3.2917516231536865
epoch: 97, train_loss: 2.7358999252319336, train_acc: 80.96, train_fscore: 80.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.291800022125244, test_acc: 71.47, test_fscore: 71.84, time: 6.43 sec
loss: 2.6928491592407227
loss: 2.7739217281341553
loss: 3.27939510345459
epoch: 98, train_loss: 2.730799913406372, train_acc: 80.91, train_fscore: 80.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.279400110244751, test_acc: 71.1, test_fscore: 71.47, time: 5.94 sec
loss: 2.613412618637085
loss: 2.8637800216674805
loss: 3.2829623222351074
epoch: 99, train_loss: 2.7276999950408936, train_acc: 80.76, train_fscore: 80.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2829999923706055, test_acc: 71.04, test_fscore: 71.39, time: 6.47 sec
loss: 2.7634267807006836
loss: 2.624507427215576
loss: 3.292696237564087
epoch: 100, train_loss: 2.700000047683716, train_acc: 80.79, train_fscore: 80.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2927000522613525, test_acc: 70.98, test_fscore: 71.39, time: 5.32 sec
              precision    recall  f1-score   support

           0     0.5185    0.7778    0.6222     144.0
           1     0.8304    0.7592    0.7932     245.0
           2     0.7255    0.6745    0.6991     384.0
           3     0.6368    0.7529    0.6900     170.0
           4     0.8774    0.7659    0.8179     299.0
           5     0.6813    0.6509    0.6658     381.0

    accuracy                         0.7160    1623.0
   macro avg     0.7116    0.7302    0.7147    1623.0
weighted avg     0.7313    0.7160    0.7196    1623.0

[[112.   5.   7.   0.  20.   0.]
 [  2. 186.  23.   3.   0.  31.]
 [ 44.  17. 259.  17.   4.  43.]
 [  0.   0.   1. 128.   0.  41.]
 [ 57.   2.  10.   0. 229.   1.]
 [  1.  14.  57.  53.   8. 248.]]
loss: 2.939643621444702
loss: 2.4282827377319336
loss: 3.302258014678955
epoch: 101, train_loss: 2.7056000232696533, train_acc: 81.31, train_fscore: 81.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.302299976348877, test_acc: 71.23, test_fscore: 71.63, time: 6.86 sec
loss: 2.6534740924835205
loss: 2.715866804122925
loss: 3.2854959964752197
epoch: 102, train_loss: 2.6826000213623047, train_acc: 81.29, train_fscore: 81.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2855000495910645, test_acc: 71.23, test_fscore: 71.57, time: 7.4 sec
loss: 2.6889266967773438
loss: 2.6568949222564697
loss: 3.2992584705352783
epoch: 103, train_loss: 2.6738998889923096, train_acc: 81.29, train_fscore: 81.16, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.299299955368042, test_acc: 71.04, test_fscore: 71.43, time: 6.15 sec
loss: 2.5982284545898438
loss: 2.7376861572265625
loss: 3.290977716445923
epoch: 104, train_loss: 2.6605000495910645, train_acc: 81.77, train_fscore: 81.66, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2909998893737793, test_acc: 71.1, test_fscore: 71.43, time: 6.12 sec
loss: 2.5958423614501953
loss: 2.7332265377044678
loss: 3.27976131439209
epoch: 105, train_loss: 2.6577000617980957, train_acc: 81.65, train_fscore: 81.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2797999382019043, test_acc: 70.79, test_fscore: 71.14, time: 6.04 sec
loss: 2.582749605178833
loss: 2.7198233604431152
loss: 3.2778122425079346
epoch: 106, train_loss: 2.647200107574463, train_acc: 81.62, train_fscore: 81.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2778000831604004, test_acc: 71.29, test_fscore: 71.65, time: 6.01 sec
loss: 2.579507350921631
loss: 2.6880860328674316
loss: 3.2697854042053223
epoch: 107, train_loss: 2.631500005722046, train_acc: 81.57, train_fscore: 81.47, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2697999477386475, test_acc: 71.04, test_fscore: 71.35, time: 6.54 sec
loss: 2.7075934410095215
loss: 2.553677558898926
loss: 3.2775003910064697
epoch: 108, train_loss: 2.6333999633789062, train_acc: 81.46, train_fscore: 81.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2774999141693115, test_acc: 71.35, test_fscore: 71.73, time: 6.15 sec
loss: 2.481806993484497
loss: 2.8049826622009277
loss: 3.2621121406555176
epoch: 109, train_loss: 2.6312999725341797, train_acc: 81.69, train_fscore: 81.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2620999813079834, test_acc: 71.16, test_fscore: 71.53, time: 6.17 sec
loss: 2.542234420776367
loss: 2.662290573120117
loss: 3.291581153869629
epoch: 110, train_loss: 2.5998001098632812, train_acc: 81.82, train_fscore: 81.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.291599988937378, test_acc: 70.98, test_fscore: 71.41, time: 6.6 sec
              precision    recall  f1-score   support

           0     0.5185    0.7778    0.6222     144.0
           1     0.8304    0.7592    0.7932     245.0
           2     0.7255    0.6745    0.6991     384.0
           3     0.6368    0.7529    0.6900     170.0
           4     0.8774    0.7659    0.8179     299.0
           5     0.6813    0.6509    0.6658     381.0

    accuracy                         0.7160    1623.0
   macro avg     0.7116    0.7302    0.7147    1623.0
weighted avg     0.7313    0.7160    0.7196    1623.0

[[112.   5.   7.   0.  20.   0.]
 [  2. 186.  23.   3.   0.  31.]
 [ 44.  17. 259.  17.   4.  43.]
 [  0.   0.   1. 128.   0.  41.]
 [ 57.   2.  10.   0. 229.   1.]
 [  1.  14.  57.  53.   8. 248.]]
loss: 2.5823841094970703
loss: 2.6475861072540283
loss: 3.32085919380188
epoch: 111, train_loss: 2.6119000911712646, train_acc: 81.94, train_fscore: 81.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3208999633789062, test_acc: 70.86, test_fscore: 71.25, time: 6.26 sec
loss: 2.5932669639587402
loss: 2.615328788757324
loss: 3.2980360984802246
epoch: 112, train_loss: 2.603300094604492, train_acc: 81.77, train_fscore: 81.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2980000972747803, test_acc: 71.23, test_fscore: 71.56, time: 6.54 sec
loss: 2.6198792457580566
loss: 2.522643566131592
loss: 3.286932945251465
epoch: 113, train_loss: 2.574700117111206, train_acc: 82.5, train_fscore: 82.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.286900043487549, test_acc: 71.23, test_fscore: 71.61, time: 6.53 sec
loss: 2.5809688568115234
loss: 2.566701650619507
loss: 3.259798049926758
epoch: 114, train_loss: 2.5739998817443848, train_acc: 82.07, train_fscore: 81.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2597999572753906, test_acc: 71.41, test_fscore: 71.75, time: 5.97 sec
loss: 2.6077661514282227
loss: 2.463552474975586
loss: 3.2494959831237793
epoch: 115, train_loss: 2.541599988937378, train_acc: 82.29, train_fscore: 82.16, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.249500036239624, test_acc: 71.6, test_fscore: 71.96, time: 6.89 sec
loss: 2.3944854736328125
loss: 2.730260133743286
loss: 3.2712056636810303
epoch: 116, train_loss: 2.5490000247955322, train_acc: 82.74, train_fscore: 82.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.271199941635132, test_acc: 71.53, test_fscore: 71.96, time: 5.57 sec
loss: 2.5234320163726807
loss: 2.6138339042663574
loss: 3.274594783782959
epoch: 117, train_loss: 2.567199945449829, train_acc: 82.72, train_fscore: 82.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.274600028991699, test_acc: 71.6, test_fscore: 71.97, time: 6.86 sec
loss: 2.53977108001709
loss: 2.5413942337036133
loss: 3.2805094718933105
epoch: 118, train_loss: 2.5404999256134033, train_acc: 83.06, train_fscore: 82.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2804999351501465, test_acc: 71.1, test_fscore: 71.46, time: 6.26 sec
loss: 2.575025796890259
loss: 2.4379873275756836
loss: 3.30964732170105
epoch: 119, train_loss: 2.5144999027252197, train_acc: 82.75, train_fscore: 82.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3096001148223877, test_acc: 71.29, test_fscore: 71.61, time: 6.65 sec
loss: 2.5427958965301514
loss: 2.5025644302368164
loss: 3.296842098236084
epoch: 120, train_loss: 2.523200035095215, train_acc: 82.84, train_fscore: 82.72, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.296799898147583, test_acc: 71.84, test_fscore: 72.21, time: 5.82 sec
              precision    recall  f1-score   support

           0     0.5234    0.7778    0.6257     144.0
           1     0.8319    0.7673    0.7983     245.0
           2     0.7328    0.6927    0.7122     384.0
           3     0.6396    0.7412    0.6866     170.0
           4     0.8706    0.7425    0.8014     299.0
           5     0.6848    0.6614    0.6729     381.0

    accuracy                         0.7184    1623.0
   macro avg     0.7138    0.7305    0.7162    1623.0
weighted avg     0.7335    0.7184    0.7221    1623.0

[[112.   4.   7.   0.  21.   0.]
 [  2. 188.  21.   1.   0.  33.]
 [ 41.  17. 266.  17.   4.  39.]
 [  0.   0.   1. 126.   0.  43.]
 [ 59.   2.  15.   0. 222.   1.]
 [  0.  15.  53.  53.   8. 252.]]
loss: 2.611785888671875
loss: 2.4034314155578613
loss: 3.290806770324707
epoch: 121, train_loss: 2.5160000324249268, train_acc: 82.96, train_fscore: 82.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.290800094604492, test_acc: 71.53, test_fscore: 71.95, time: 6.37 sec
loss: 2.561215400695801
loss: 2.4238080978393555
loss: 3.2630653381347656
epoch: 122, train_loss: 2.4976000785827637, train_acc: 83.08, train_fscore: 82.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2630999088287354, test_acc: 71.72, test_fscore: 72.07, time: 6.0 sec
loss: 2.5160531997680664
loss: 2.4653160572052
loss: 3.295532464981079
epoch: 123, train_loss: 2.4916999340057373, train_acc: 83.15, train_fscore: 83.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2955000400543213, test_acc: 71.04, test_fscore: 71.43, time: 6.49 sec
loss: 2.4473133087158203
loss: 2.5359749794006348
loss: 3.2853996753692627
epoch: 124, train_loss: 2.4900999069213867, train_acc: 83.08, train_fscore: 82.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.285399913787842, test_acc: 71.35, test_fscore: 71.7, time: 6.34 sec
loss: 2.593390464782715
loss: 2.3717846870422363
loss: 3.2922372817993164
epoch: 125, train_loss: 2.4911999702453613, train_acc: 82.99, train_fscore: 82.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2922000885009766, test_acc: 71.1, test_fscore: 71.48, time: 6.11 sec
loss: 2.4677348136901855
loss: 2.480140447616577
loss: 3.2892184257507324
epoch: 126, train_loss: 2.473599910736084, train_acc: 83.43, train_fscore: 83.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2892000675201416, test_acc: 71.1, test_fscore: 71.47, time: 5.34 sec
loss: 2.5120065212249756
loss: 2.426887273788452
loss: 3.3370840549468994
epoch: 127, train_loss: 2.474100112915039, train_acc: 83.15, train_fscore: 83.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.337100028991699, test_acc: 71.23, test_fscore: 71.6, time: 6.28 sec
loss: 2.4089083671569824
loss: 2.4735772609710693
loss: 3.3085575103759766
epoch: 128, train_loss: 2.4381000995635986, train_acc: 83.6, train_fscore: 83.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3085999488830566, test_acc: 70.79, test_fscore: 71.17, time: 7.28 sec
loss: 2.4587748050689697
loss: 2.440304756164551
loss: 3.280054807662964
epoch: 129, train_loss: 2.449899911880493, train_acc: 83.55, train_fscore: 83.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.280100107192993, test_acc: 71.35, test_fscore: 71.68, time: 6.58 sec
loss: 2.5899088382720947
loss: 2.2852001190185547
loss: 3.2853126525878906
epoch: 130, train_loss: 2.450700044631958, train_acc: 83.86, train_fscore: 83.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2853000164031982, test_acc: 71.35, test_fscore: 71.73, time: 5.54 sec
              precision    recall  f1-score   support

           0     0.5234    0.7778    0.6257     144.0
           1     0.8319    0.7673    0.7983     245.0
           2     0.7328    0.6927    0.7122     384.0
           3     0.6396    0.7412    0.6866     170.0
           4     0.8706    0.7425    0.8014     299.0
           5     0.6848    0.6614    0.6729     381.0

    accuracy                         0.7184    1623.0
   macro avg     0.7138    0.7305    0.7162    1623.0
weighted avg     0.7335    0.7184    0.7221    1623.0

[[112.   4.   7.   0.  21.   0.]
 [  2. 188.  21.   1.   0.  33.]
 [ 41.  17. 266.  17.   4.  39.]
 [  0.   0.   1. 126.   0.  43.]
 [ 59.   2.  15.   0. 222.   1.]
 [  0.  15.  53.  53.   8. 252.]]
loss: 2.520599365234375
loss: 2.3392295837402344
loss: 3.3248395919799805
epoch: 131, train_loss: 2.43969988822937, train_acc: 84.03, train_fscore: 83.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3248000144958496, test_acc: 71.6, test_fscore: 71.97, time: 6.32 sec
loss: 2.325897216796875
loss: 2.5791964530944824
loss: 3.3257739543914795
epoch: 132, train_loss: 2.4433000087738037, train_acc: 83.75, train_fscore: 83.66, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3257999420166016, test_acc: 71.66, test_fscore: 72.02, time: 6.21 sec
loss: 2.34094500541687
loss: 2.5338947772979736
loss: 3.313493490219116
epoch: 133, train_loss: 2.431299924850464, train_acc: 83.91, train_fscore: 83.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.313499927520752, test_acc: 71.29, test_fscore: 71.67, time: 6.41 sec
loss: 2.4568145275115967
loss: 2.4223694801330566
loss: 3.28853702545166
epoch: 134, train_loss: 2.4407999515533447, train_acc: 83.63, train_fscore: 83.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2885000705718994, test_acc: 71.6, test_fscore: 71.93, time: 6.7 sec
loss: 2.4124093055725098
loss: 2.4095661640167236
loss: 3.275446891784668
epoch: 135, train_loss: 2.411099910736084, train_acc: 84.29, train_fscore: 84.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.275399923324585, test_acc: 71.47, test_fscore: 71.87, time: 5.98 sec
loss: 2.490544557571411
loss: 2.316683292388916
loss: 3.2897324562072754
epoch: 136, train_loss: 2.407399892807007, train_acc: 84.39, train_fscore: 84.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2897000312805176, test_acc: 71.66, test_fscore: 72.02, time: 6.83 sec
loss: 2.4192326068878174
loss: 2.392845869064331
loss: 3.3058314323425293
epoch: 137, train_loss: 2.4065001010894775, train_acc: 84.08, train_fscore: 83.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.305799961090088, test_acc: 71.6, test_fscore: 71.96, time: 6.48 sec
loss: 2.2912557125091553
loss: 2.474829912185669
loss: 3.277815818786621
epoch: 138, train_loss: 2.3784000873565674, train_acc: 84.22, train_fscore: 84.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2778000831604004, test_acc: 71.35, test_fscore: 71.7, time: 6.92 sec
loss: 2.3690218925476074
loss: 2.3799521923065186
loss: 3.3045809268951416
epoch: 139, train_loss: 2.3740999698638916, train_acc: 84.73, train_fscore: 84.62, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3046000003814697, test_acc: 71.04, test_fscore: 71.42, time: 5.42 sec
loss: 2.35068416595459
loss: 2.4269654750823975
loss: 3.3072540760040283
epoch: 140, train_loss: 2.384200096130371, train_acc: 84.78, train_fscore: 84.7, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.307300090789795, test_acc: 71.35, test_fscore: 71.78, time: 6.62 sec
              precision    recall  f1-score   support

           0     0.5234    0.7778    0.6257     144.0
           1     0.8319    0.7673    0.7983     245.0
           2     0.7328    0.6927    0.7122     384.0
           3     0.6396    0.7412    0.6866     170.0
           4     0.8706    0.7425    0.8014     299.0
           5     0.6848    0.6614    0.6729     381.0

    accuracy                         0.7184    1623.0
   macro avg     0.7138    0.7305    0.7162    1623.0
weighted avg     0.7335    0.7184    0.7221    1623.0

[[112.   4.   7.   0.  21.   0.]
 [  2. 188.  21.   1.   0.  33.]
 [ 41.  17. 266.  17.   4.  39.]
 [  0.   0.   1. 126.   0.  43.]
 [ 59.   2.  15.   0. 222.   1.]
 [  0.  15.  53.  53.   8. 252.]]
loss: 2.3340096473693848
loss: 2.384305715560913
loss: 3.3271567821502686
epoch: 141, train_loss: 2.3575000762939453, train_acc: 84.41, train_fscore: 84.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.327199935913086, test_acc: 71.35, test_fscore: 71.75, time: 4.73 sec
loss: 2.4101402759552
loss: 2.3265633583068848
loss: 3.307401657104492
epoch: 142, train_loss: 2.370500087738037, train_acc: 84.51, train_fscore: 84.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3073999881744385, test_acc: 71.35, test_fscore: 71.71, time: 5.92 sec
loss: 2.3817296028137207
loss: 2.3350181579589844
loss: 3.3017241954803467
epoch: 143, train_loss: 2.359299898147583, train_acc: 84.49, train_fscore: 84.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3017001152038574, test_acc: 71.53, test_fscore: 71.9, time: 6.48 sec
loss: 2.2680468559265137
loss: 2.4571282863616943
loss: 3.306964874267578
epoch: 144, train_loss: 2.3578999042510986, train_acc: 84.65, train_fscore: 84.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.306999921798706, test_acc: 71.6, test_fscore: 71.98, time: 5.19 sec
loss: 2.3539681434631348
loss: 2.315479278564453
loss: 3.318798065185547
epoch: 145, train_loss: 2.335700035095215, train_acc: 85.7, train_fscore: 85.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3187999725341797, test_acc: 71.41, test_fscore: 71.78, time: 6.9 sec
loss: 2.4359428882598877
loss: 2.1907639503479004
loss: 3.3191046714782715
epoch: 146, train_loss: 2.3229000568389893, train_acc: 84.73, train_fscore: 84.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3190999031066895, test_acc: 71.35, test_fscore: 71.67, time: 6.92 sec
loss: 2.2264957427978516
loss: 2.4454479217529297
loss: 3.31622576713562
epoch: 147, train_loss: 2.333699941635132, train_acc: 84.91, train_fscore: 84.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.316200017929077, test_acc: 71.29, test_fscore: 71.64, time: 5.53 sec
loss: 2.2422308921813965
loss: 2.3764684200286865
loss: 3.315725326538086
epoch: 148, train_loss: 2.3059000968933105, train_acc: 85.22, train_fscore: 85.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.315700054168701, test_acc: 71.53, test_fscore: 71.92, time: 5.51 sec
loss: 2.3308730125427246
loss: 2.2575106620788574
loss: 3.3010129928588867
epoch: 149, train_loss: 2.2962000370025635, train_acc: 84.94, train_fscore: 84.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3010001182556152, test_acc: 71.41, test_fscore: 71.81, time: 6.36 sec
loss: 2.3361682891845703
loss: 2.2706615924835205
loss: 3.3390302658081055
epoch: 150, train_loss: 2.303999900817871, train_acc: 85.09, train_fscore: 85.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3389999866485596, test_acc: 71.6, test_fscore: 71.97, time: 5.88 sec
              precision    recall  f1-score   support

           0     0.5234    0.7778    0.6257     144.0
           1     0.8319    0.7673    0.7983     245.0
           2     0.7328    0.6927    0.7122     384.0
           3     0.6396    0.7412    0.6866     170.0
           4     0.8706    0.7425    0.8014     299.0
           5     0.6848    0.6614    0.6729     381.0

    accuracy                         0.7184    1623.0
   macro avg     0.7138    0.7305    0.7162    1623.0
weighted avg     0.7335    0.7184    0.7221    1623.0

[[112.   4.   7.   0.  21.   0.]
 [  2. 188.  21.   1.   0.  33.]
 [ 41.  17. 266.  17.   4.  39.]
 [  0.   0.   1. 126.   0.  43.]
 [ 59.   2.  15.   0. 222.   1.]
 [  0.  15.  53.  53.   8. 252.]]
Test performance..
F-Score: 72.21
F-Score-index: 120
              precision    recall  f1-score   support

           0     0.5234    0.7778    0.6257     144.0
           1     0.8319    0.7673    0.7983     245.0
           2     0.7328    0.6927    0.7122     384.0
           3     0.6396    0.7412    0.6866     170.0
           4     0.8706    0.7425    0.8014     299.0
           5     0.6848    0.6614    0.6729     381.0

    accuracy                         0.7184    1623.0
   macro avg     0.7138    0.7305    0.7162    1623.0
weighted avg     0.7335    0.7184    0.7221    1623.0

[[112.   4.   7.   0.  21.   0.]
 [  2. 188.  21.   1.   0.  33.]
 [ 41.  17. 266.  17.   4.  39.]
 [  0.   0.   1. 126.   0.  43.]
 [ 59.   2.  15.   0. 222.   1.]
 [  0.  15.  53.  53.   8. 252.]]
--- 5 ---
Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.5, batch_size=64, hidden_dim=1024, n_head=8, epochs=150, temp=2, tensorboard=False, class_weight=True, Dataset='IEMOCAP')
Running on GPU
temp 2
total parameters: 79687704
training parameters: 79687704
loss: 7.97725772857666
loss: 10.005555152893066
loss: 7.8763508796691895
epoch: 1, train_loss: 8.903800010681152, train_acc: 19.47, train_fscore: 19.49, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.876399993896484, test_acc: 25.26, test_fscore: 19.22, time: 7.67 sec
loss: 8.019752502441406
loss: 8.738605499267578
loss: 7.535660743713379
epoch: 2, train_loss: 8.377699851989746, train_acc: 25.97, train_fscore: 22.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.535699844360352, test_acc: 30.75, test_fscore: 28.22, time: 7.74 sec
loss: 7.850730895996094
loss: 7.219778537750244
loss: 7.059142589569092
epoch: 3, train_loss: 7.5609002113342285, train_acc: 38.43, train_fscore: 36.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.059100151062012, test_acc: 36.97, test_fscore: 31.51, time: 6.51 sec
loss: 7.264469146728516
loss: 7.415182113647461
loss: 7.101583003997803
epoch: 4, train_loss: 7.337699890136719, train_acc: 40.71, train_fscore: 32.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.101600170135498, test_acc: 42.45, test_fscore: 35.29, time: 6.0 sec
loss: 7.29318904876709
loss: 6.949786186218262
loss: 6.6527605056762695
epoch: 5, train_loss: 7.134399890899658, train_acc: 45.11, train_fscore: 37.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.6528000831604, test_acc: 44.3, test_fscore: 42.74, time: 6.32 sec
loss: 6.73000431060791
loss: 6.700611591339111
loss: 6.659824371337891
epoch: 6, train_loss: 6.716100215911865, train_acc: 52.08, train_fscore: 52.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.659800052642822, test_acc: 43.07, test_fscore: 44.49, time: 6.42 sec
loss: 6.684762001037598
loss: 6.590549945831299
loss: 6.5682477951049805
epoch: 7, train_loss: 6.638400077819824, train_acc: 50.26, train_fscore: 50.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.56820011138916, test_acc: 44.42, test_fscore: 45.37, time: 6.59 sec
loss: 6.515212535858154
loss: 6.441856384277344
loss: 6.403359413146973
epoch: 8, train_loss: 6.480999946594238, train_acc: 50.74, train_fscore: 49.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.40339994430542, test_acc: 48.24, test_fscore: 47.04, time: 5.73 sec
loss: 6.192704200744629
loss: 6.306697845458984
loss: 6.1894307136535645
epoch: 9, train_loss: 6.248700141906738, train_acc: 53.41, train_fscore: 51.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.1894001960754395, test_acc: 52.62, test_fscore: 52.19, time: 6.08 sec
loss: 6.084167957305908
loss: 5.962349891662598
loss: 6.029196739196777
epoch: 10, train_loss: 6.029200077056885, train_acc: 58.66, train_fscore: 57.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.029200077056885, test_acc: 55.27, test_fscore: 54.45, time: 6.1 sec
              precision    recall  f1-score   support

           0     0.3462    0.3125    0.3285     144.0
           1     0.6052    0.7633    0.6751     245.0
           2     0.5149    0.5391    0.5267     384.0
           3     0.4498    0.8176    0.5804     170.0
           4     0.7255    0.6187    0.6679     299.0
           5     0.6147    0.3517    0.4474     381.0

    accuracy                         0.5527    1623.0
   macro avg     0.5427    0.5672    0.5377    1623.0
weighted avg     0.5690    0.5527    0.5445    1623.0

[[ 45.  20.  21.   8.  50.   0.]
 [  2. 187.  33.   4.   2.  17.]
 [ 33.  58. 207.  33.  12.  41.]
 [  0.   1.  13. 139.   1.  16.]
 [ 43.   6.  47.   8. 185.  10.]
 [  7.  37.  81. 117.   5. 134.]]
loss: 5.859100341796875
loss: 5.812800884246826
loss: 5.7696533203125
epoch: 11, train_loss: 5.8368000984191895, train_acc: 61.46, train_fscore: 59.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.769700050354004, test_acc: 57.12, test_fscore: 56.2, time: 5.79 sec
loss: 5.774098873138428
loss: 5.328923225402832
loss: 5.4908037185668945
epoch: 12, train_loss: 5.561299800872803, train_acc: 62.29, train_fscore: 60.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.490799903869629, test_acc: 58.78, test_fscore: 57.91, time: 6.27 sec
loss: 5.472050189971924
loss: 5.191192150115967
loss: 5.326223850250244
epoch: 13, train_loss: 5.348100185394287, train_acc: 63.53, train_fscore: 61.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.326200008392334, test_acc: 61.68, test_fscore: 60.77, time: 5.92 sec
loss: 5.185687065124512
loss: 5.276535987854004
loss: 5.178600311279297
epoch: 14, train_loss: 5.226399898529053, train_acc: 61.89, train_fscore: 59.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.178599834442139, test_acc: 62.97, test_fscore: 62.13, time: 6.01 sec
loss: 5.077118396759033
loss: 5.068325042724609
loss: 5.026468276977539
epoch: 15, train_loss: 5.0731000900268555, train_acc: 63.49, train_fscore: 61.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.026500225067139, test_acc: 64.08, test_fscore: 63.59, time: 6.13 sec
loss: 5.115832805633545
loss: 4.77731466293335
loss: 4.916154861450195
epoch: 16, train_loss: 4.954800128936768, train_acc: 64.85, train_fscore: 63.51, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.916200160980225, test_acc: 61.68, test_fscore: 62.25, time: 6.3 sec
loss: 4.887526035308838
loss: 4.817548751831055
loss: 4.821441650390625
epoch: 17, train_loss: 4.854499816894531, train_acc: 64.78, train_fscore: 63.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.821400165557861, test_acc: 61.18, test_fscore: 61.97, time: 6.0 sec
loss: 4.88742208480835
loss: 4.613773345947266
loss: 4.70425271987915
epoch: 18, train_loss: 4.761099815368652, train_acc: 64.53, train_fscore: 63.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.7042999267578125, test_acc: 62.91, test_fscore: 63.55, time: 6.78 sec
loss: 4.679110527038574
loss: 4.601505756378174
loss: 4.637185573577881
epoch: 19, train_loss: 4.64300012588501, train_acc: 65.25, train_fscore: 64.49, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.637199878692627, test_acc: 62.85, test_fscore: 63.76, time: 6.96 sec
loss: 4.574823379516602
loss: 4.605628490447998
loss: 4.585603713989258
epoch: 20, train_loss: 4.588699817657471, train_acc: 65.85, train_fscore: 65.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.585599899291992, test_acc: 64.33, test_fscore: 64.62, time: 5.57 sec
              precision    recall  f1-score   support

           0     0.3579    0.4722    0.4072     144.0
           1     0.7500    0.6857    0.7164     245.0
           2     0.6128    0.6224    0.6176     384.0
           3     0.5867    0.7765    0.6684     170.0
           4     0.8147    0.7793    0.7966     299.0
           5     0.6623    0.5354    0.5922     381.0

    accuracy                         0.6433    1623.0
   macro avg     0.6307    0.6453    0.6330    1623.0
weighted avg     0.6570    0.6433    0.6462    1623.0

[[ 68.  11.  14.   5.  45.   1.]
 [  6. 168.  44.   1.   0.  26.]
 [ 54.  25. 239.  20.   3.  43.]
 [  0.   0.   5. 132.   0.  33.]
 [ 58.   0.   7.   0. 233.   1.]
 [  4.  20.  81.  67.   5. 204.]]
loss: 4.554630279541016
loss: 4.510840892791748
loss: 4.484450817108154
epoch: 21, train_loss: 4.534800052642822, train_acc: 66.02, train_fscore: 65.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.484499931335449, test_acc: 64.51, test_fscore: 64.81, time: 6.41 sec
loss: 4.550875186920166
loss: 4.299600124359131
loss: 4.406541347503662
epoch: 22, train_loss: 4.441299915313721, train_acc: 65.82, train_fscore: 64.9, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.406499862670898, test_acc: 64.51, test_fscore: 64.64, time: 6.09 sec
loss: 4.613930702209473
loss: 4.1564788818359375
loss: 4.402467727661133
epoch: 23, train_loss: 4.407100200653076, train_acc: 65.83, train_fscore: 64.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.402500152587891, test_acc: 63.89, test_fscore: 64.27, time: 6.66 sec
loss: 4.372120380401611
loss: 4.27092170715332
loss: 4.416358947753906
epoch: 24, train_loss: 4.3231000900268555, train_acc: 66.61, train_fscore: 65.75, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.416399955749512, test_acc: 63.4, test_fscore: 63.95, time: 6.55 sec
loss: 4.390989303588867
loss: 4.1773295402526855
loss: 4.372878551483154
epoch: 25, train_loss: 4.290200233459473, train_acc: 67.99, train_fscore: 67.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.372900009155273, test_acc: 64.39, test_fscore: 64.62, time: 5.74 sec
loss: 4.200258255004883
loss: 4.2730584144592285
loss: 4.304835796356201
epoch: 26, train_loss: 4.2342000007629395, train_acc: 67.59, train_fscore: 66.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.304800033569336, test_acc: 65.43, test_fscore: 65.88, time: 6.99 sec
loss: 4.296205520629883
loss: 4.058774471282959
loss: 4.248049259185791
epoch: 27, train_loss: 4.18179988861084, train_acc: 68.43, train_fscore: 67.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.248000144958496, test_acc: 65.74, test_fscore: 66.25, time: 5.32 sec
loss: 4.278571128845215
loss: 3.9645004272460938
loss: 4.200250148773193
epoch: 28, train_loss: 4.140600204467773, train_acc: 68.62, train_fscore: 68.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.200200080871582, test_acc: 65.62, test_fscore: 65.92, time: 6.35 sec
loss: 4.063521862030029
loss: 4.105022430419922
loss: 4.181392192840576
epoch: 29, train_loss: 4.082300186157227, train_acc: 68.8, train_fscore: 68.09, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.181399822235107, test_acc: 65.8, test_fscore: 66.04, time: 5.96 sec
loss: 4.050379753112793
loss: 4.056839942932129
loss: 4.149320602416992
epoch: 30, train_loss: 4.053400039672852, train_acc: 68.88, train_fscore: 68.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.1493000984191895, test_acc: 65.99, test_fscore: 66.41, time: 6.68 sec
              precision    recall  f1-score   support

           0     0.4000    0.5556    0.4651     144.0
           1     0.7344    0.7224    0.7284     245.0
           2     0.6737    0.5807    0.6238     384.0
           3     0.5896    0.7353    0.6545     170.0
           4     0.8433    0.7559    0.7972     299.0
           5     0.6469    0.6299    0.6383     381.0

    accuracy                         0.6599    1623.0
   macro avg     0.6480    0.6633    0.6512    1623.0
weighted avg     0.6747    0.6599    0.6641    1623.0

[[ 80.   9.  14.   4.  35.   2.]
 [  3. 177.  33.   0.   0.  32.]
 [ 50.  31. 223.  24.   3.  53.]
 [  0.   0.   2. 125.   0.  43.]
 [ 64.   0.   8.   0. 226.   1.]
 [  3.  24.  51.  59.   4. 240.]]
loss: 3.8178296089172363
loss: 4.141814231872559
loss: 4.109927177429199
epoch: 31, train_loss: 3.981100082397461, train_acc: 69.6, train_fscore: 69.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.109899997711182, test_acc: 66.36, test_fscore: 66.84, time: 6.45 sec
loss: 3.811878204345703
loss: 4.13138484954834
loss: 4.080713748931885
epoch: 32, train_loss: 3.9590001106262207, train_acc: 69.5, train_fscore: 69.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.080699920654297, test_acc: 65.62, test_fscore: 66.21, time: 6.21 sec
loss: 3.989105463027954
loss: 3.8355095386505127
loss: 4.028712749481201
epoch: 33, train_loss: 3.9205000400543213, train_acc: 70.22, train_fscore: 69.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.02869987487793, test_acc: 66.48, test_fscore: 66.97, time: 6.07 sec
loss: 3.99300479888916
loss: 3.7098891735076904
loss: 4.010316848754883
epoch: 34, train_loss: 3.8657000064849854, train_acc: 70.81, train_fscore: 70.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.010300159454346, test_acc: 66.3, test_fscore: 67.0, time: 5.57 sec
loss: 3.624330997467041
loss: 4.011473655700684
loss: 3.9787039756774902
epoch: 35, train_loss: 3.8166000843048096, train_acc: 71.1, train_fscore: 70.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9786999225616455, test_acc: 66.85, test_fscore: 67.28, time: 6.34 sec
loss: 3.8433752059936523
loss: 3.740507125854492
loss: 3.917034149169922
epoch: 36, train_loss: 3.7985999584198, train_acc: 70.88, train_fscore: 70.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9170000553131104, test_acc: 67.47, test_fscore: 67.77, time: 6.8 sec
loss: 3.6279354095458984
loss: 3.8793625831604004
loss: 3.8846306800842285
epoch: 37, train_loss: 3.7462000846862793, train_acc: 71.33, train_fscore: 70.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8845999240875244, test_acc: 67.53, test_fscore: 68.01, time: 6.22 sec
loss: 3.758511781692505
loss: 3.676011800765991
loss: 3.8508899211883545
epoch: 38, train_loss: 3.7207000255584717, train_acc: 71.96, train_fscore: 71.59, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8508999347686768, test_acc: 68.08, test_fscore: 68.24, time: 6.14 sec
loss: 3.7196125984191895
loss: 3.6545560359954834
loss: 3.860236167907715
epoch: 39, train_loss: 3.6898000240325928, train_acc: 72.29, train_fscore: 71.9, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8601999282836914, test_acc: 67.9, test_fscore: 68.45, time: 6.3 sec
loss: 3.833942413330078
loss: 3.4087703227996826
loss: 3.826582193374634
epoch: 40, train_loss: 3.629699945449829, train_acc: 72.75, train_fscore: 72.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8266000747680664, test_acc: 67.84, test_fscore: 68.41, time: 6.35 sec
              precision    recall  f1-score   support

           0     0.4227    0.6458    0.5110     144.0
           1     0.7695    0.7633    0.7664     245.0
           2     0.6971    0.6172    0.6547     384.0
           3     0.6349    0.7059    0.6685     170.0
           4     0.8740    0.7191    0.7890     299.0
           5     0.6494    0.6562    0.6527     381.0

    accuracy                         0.6790    1623.0
   macro avg     0.6746    0.6846    0.6737    1623.0
weighted avg     0.6985    0.6790    0.6845    1623.0

[[ 93.   9.  15.   2.  24.   1.]
 [  3. 187.  20.   0.   0.  35.]
 [ 45.  27. 237.  22.   3.  50.]
 [  0.   0.   2. 120.   0.  48.]
 [ 75.   0.   8.   0. 215.   1.]
 [  4.  20.  58.  45.   4. 250.]]
loss: 3.6088099479675293
loss: 3.6719002723693848
loss: 3.8012263774871826
epoch: 41, train_loss: 3.6375999450683594, train_acc: 72.58, train_fscore: 72.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8011999130249023, test_acc: 68.02, test_fscore: 68.43, time: 5.89 sec
loss: 3.5674848556518555
loss: 3.5804200172424316
loss: 3.8176794052124023
epoch: 42, train_loss: 3.5734000205993652, train_acc: 73.25, train_fscore: 72.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.817699909210205, test_acc: 67.9, test_fscore: 68.5, time: 6.14 sec
loss: 3.4852218627929688
loss: 3.6407310962677
loss: 3.779609441757202
epoch: 43, train_loss: 3.5520999431610107, train_acc: 73.55, train_fscore: 73.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.779599905014038, test_acc: 68.08, test_fscore: 68.47, time: 6.09 sec
loss: 3.342658042907715
loss: 3.74847149848938
loss: 3.750807762145996
epoch: 44, train_loss: 3.531100034713745, train_acc: 73.44, train_fscore: 73.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7507998943328857, test_acc: 68.64, test_fscore: 69.07, time: 6.21 sec
loss: 3.3454864025115967
loss: 3.680436849594116
loss: 3.738250970840454
epoch: 45, train_loss: 3.5099000930786133, train_acc: 73.36, train_fscore: 73.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.738300085067749, test_acc: 68.33, test_fscore: 68.86, time: 5.76 sec
loss: 3.5554111003875732
loss: 3.372070789337158
loss: 3.7039406299591064
epoch: 46, train_loss: 3.4721999168395996, train_acc: 73.77, train_fscore: 73.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.703900098800659, test_acc: 68.64, test_fscore: 69.08, time: 7.2 sec
loss: 3.348670244216919
loss: 3.545865535736084
loss: 3.713608503341675
epoch: 47, train_loss: 3.4428999423980713, train_acc: 73.87, train_fscore: 73.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.713599920272827, test_acc: 68.45, test_fscore: 68.91, time: 5.47 sec
loss: 3.3016157150268555
loss: 3.5226101875305176
loss: 3.6951794624328613
epoch: 48, train_loss: 3.4072999954223633, train_acc: 74.44, train_fscore: 74.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.695199966430664, test_acc: 68.76, test_fscore: 69.2, time: 6.44 sec
loss: 3.5643115043640137
loss: 3.1951608657836914
loss: 3.6608269214630127
epoch: 49, train_loss: 3.392199993133545, train_acc: 74.01, train_fscore: 73.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.660799980163574, test_acc: 68.7, test_fscore: 69.13, time: 6.21 sec
loss: 3.4018898010253906
loss: 3.340656280517578
loss: 3.6499860286712646
epoch: 50, train_loss: 3.3743999004364014, train_acc: 74.66, train_fscore: 74.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6500000953674316, test_acc: 68.7, test_fscore: 69.17, time: 6.09 sec
              precision    recall  f1-score   support

           0     0.4483    0.6319    0.5245     144.0
           1     0.7826    0.7347    0.7579     245.0
           2     0.6980    0.6380    0.6667     384.0
           3     0.6093    0.7706    0.6805     170.0
           4     0.8664    0.7592    0.8093     299.0
           5     0.6685    0.6352    0.6514     381.0

    accuracy                         0.6876    1623.0
   macro avg     0.6789    0.6949    0.6817    1623.0
weighted avg     0.7034    0.6876    0.6920    1623.0

[[ 91.   9.  16.   2.  26.   0.]
 [  5. 180.  24.   0.   0.  36.]
 [ 45.  22. 245.  23.   3.  46.]
 [  0.   0.   2. 131.   0.  37.]
 [ 62.   1.   8.   0. 227.   1.]
 [  0.  18.  56.  59.   6. 242.]]
loss: 3.307509660720825
loss: 3.3940634727478027
loss: 3.665360450744629
epoch: 51, train_loss: 3.3468000888824463, train_acc: 74.77, train_fscore: 74.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6654000282287598, test_acc: 68.76, test_fscore: 69.19, time: 5.71 sec
loss: 3.2198143005371094
loss: 3.496320962905884
loss: 3.5908689498901367
epoch: 52, train_loss: 3.351300001144409, train_acc: 75.13, train_fscore: 74.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.59089994430542, test_acc: 68.95, test_fscore: 69.35, time: 6.72 sec
loss: 3.3495235443115234
loss: 3.3101584911346436
loss: 3.558969497680664
epoch: 53, train_loss: 3.330699920654297, train_acc: 75.39, train_fscore: 75.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.559000015258789, test_acc: 69.07, test_fscore: 69.56, time: 5.33 sec
loss: 3.2352709770202637
loss: 3.3522796630859375
loss: 3.5775506496429443
epoch: 54, train_loss: 3.2913999557495117, train_acc: 75.03, train_fscore: 74.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5776000022888184, test_acc: 69.07, test_fscore: 69.6, time: 6.17 sec
loss: 3.2247226238250732
loss: 3.3243892192840576
loss: 3.539520025253296
epoch: 55, train_loss: 3.270400047302246, train_acc: 75.25, train_fscore: 75.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5394999980926514, test_acc: 69.07, test_fscore: 69.46, time: 6.3 sec
loss: 3.362759828567505
loss: 3.119199752807617
loss: 3.5521247386932373
epoch: 56, train_loss: 3.244499921798706, train_acc: 75.54, train_fscore: 75.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5520999431610107, test_acc: 69.07, test_fscore: 69.49, time: 6.2 sec
loss: 3.2563085556030273
loss: 3.2220163345336914
loss: 3.532118797302246
epoch: 57, train_loss: 3.2407000064849854, train_acc: 75.68, train_fscore: 75.49, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.532099962234497, test_acc: 69.01, test_fscore: 69.42, time: 5.92 sec
loss: 3.237727165222168
loss: 3.179917812347412
loss: 3.524569034576416
epoch: 58, train_loss: 3.2105000019073486, train_acc: 76.11, train_fscore: 75.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.524600028991699, test_acc: 68.88, test_fscore: 69.36, time: 6.33 sec
loss: 3.3335070610046387
loss: 3.0682497024536133
loss: 3.527012825012207
epoch: 59, train_loss: 3.2005999088287354, train_acc: 75.89, train_fscore: 75.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5269999504089355, test_acc: 68.76, test_fscore: 69.34, time: 5.95 sec
loss: 3.2447879314422607
loss: 3.1191811561584473
loss: 3.5223171710968018
epoch: 60, train_loss: 3.1900999546051025, train_acc: 76.59, train_fscore: 76.43, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5223000049591064, test_acc: 69.19, test_fscore: 69.63, time: 6.48 sec
              precision    recall  f1-score   support

           0     0.4634    0.6597    0.5444     144.0
           1     0.7965    0.7510    0.7731     245.0
           2     0.7139    0.6302    0.6694     384.0
           3     0.6207    0.7412    0.6756     170.0
           4     0.8697    0.7592    0.8107     299.0
           5     0.6484    0.6535    0.6510     381.0

    accuracy                         0.6919    1623.0
   macro avg     0.6854    0.6991    0.6874    1623.0
weighted avg     0.7077    0.6919    0.6963    1623.0

[[ 95.   9.  14.   0.  25.   1.]
 [  3. 184.  21.   0.   0.  37.]
 [ 45.  20. 242.  21.   3.  53.]
 [  0.   0.   1. 126.   0.  43.]
 [ 61.   2.   8.   0. 227.   1.]
 [  1.  16.  53.  56.   6. 249.]]
loss: 3.2475385665893555
loss: 3.0829789638519287
loss: 3.524754524230957
epoch: 61, train_loss: 3.1738998889923096, train_acc: 76.23, train_fscore: 76.06, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5248000621795654, test_acc: 69.01, test_fscore: 69.5, time: 6.11 sec
loss: 3.225174903869629
loss: 3.013840675354004
loss: 3.5019826889038086
epoch: 62, train_loss: 3.1266000270843506, train_acc: 76.9, train_fscore: 76.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.502000093460083, test_acc: 69.56, test_fscore: 70.06, time: 5.8 sec
loss: 3.0578219890594482
loss: 3.2018485069274902
loss: 3.4800989627838135
epoch: 63, train_loss: 3.123500108718872, train_acc: 77.11, train_fscore: 76.94, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.48009991645813, test_acc: 69.13, test_fscore: 69.55, time: 5.93 sec
loss: 3.133730411529541
loss: 3.0551373958587646
loss: 3.472317934036255
epoch: 64, train_loss: 3.0959999561309814, train_acc: 76.73, train_fscore: 76.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4723000526428223, test_acc: 69.38, test_fscore: 69.89, time: 5.45 sec
loss: 3.131176471710205
loss: 3.0391173362731934
loss: 3.4729576110839844
epoch: 65, train_loss: 3.0861001014709473, train_acc: 76.92, train_fscore: 76.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4730000495910645, test_acc: 69.62, test_fscore: 70.04, time: 6.28 sec
loss: 3.0666840076446533
loss: 3.1205148696899414
loss: 3.446152925491333
epoch: 66, train_loss: 3.0913000106811523, train_acc: 77.33, train_fscore: 77.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.446199893951416, test_acc: 69.81, test_fscore: 70.24, time: 6.33 sec
loss: 2.892521381378174
loss: 3.2577147483825684
loss: 3.4383223056793213
epoch: 67, train_loss: 3.0673999786376953, train_acc: 77.44, train_fscore: 77.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4382998943328857, test_acc: 69.56, test_fscore: 70.0, time: 6.14 sec
loss: 3.1312928199768066
loss: 2.9828577041625977
loss: 3.4238121509552
epoch: 68, train_loss: 3.0601999759674072, train_acc: 77.59, train_fscore: 77.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.423799991607666, test_acc: 69.87, test_fscore: 70.3, time: 6.2 sec
loss: 3.0056331157684326
loss: 3.1211977005004883
loss: 3.43633770942688
epoch: 69, train_loss: 3.0585999488830566, train_acc: 77.83, train_fscore: 77.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.436300039291382, test_acc: 70.12, test_fscore: 70.51, time: 6.47 sec
loss: 3.1278235912323
loss: 2.9144296646118164
loss: 3.4134695529937744
epoch: 70, train_loss: 3.02620005607605, train_acc: 77.69, train_fscore: 77.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4135000705718994, test_acc: 70.12, test_fscore: 70.5, time: 5.68 sec
              precision    recall  f1-score   support

           0     0.4907    0.7361    0.5889     144.0
           1     0.8009    0.7551    0.7773     245.0
           2     0.7074    0.6484    0.6766     384.0
           3     0.6250    0.7647    0.6878     170.0
           4     0.8867    0.7592    0.8180     299.0
           5     0.6694    0.6325    0.6505     381.0

    accuracy                         0.7012    1623.0
   macro avg     0.6967    0.7160    0.6999    1623.0
weighted avg     0.7178    0.7012    0.7051    1623.0

[[106.   7.  12.   0.  19.   0.]
 [  3. 185.  24.   0.   0.  33.]
 [ 47.  19. 249.  20.   3.  46.]
 [  0.   0.   1. 130.   0.  39.]
 [ 60.   2.   9.   0. 227.   1.]
 [  0.  18.  57.  58.   7. 241.]]
loss: 3.140676498413086
loss: 2.8844246864318848
loss: 3.3822524547576904
epoch: 71, train_loss: 3.021399974822998, train_acc: 78.26, train_fscore: 78.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3822999000549316, test_acc: 70.12, test_fscore: 70.58, time: 6.92 sec
loss: 2.887561082839966
loss: 3.1003572940826416
loss: 3.4008541107177734
epoch: 72, train_loss: 2.9918999671936035, train_acc: 78.11, train_fscore: 77.97, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.400899887084961, test_acc: 70.12, test_fscore: 70.6, time: 6.26 sec
loss: 2.8060436248779297
loss: 3.185863971710205
loss: 3.416767120361328
epoch: 73, train_loss: 2.9823999404907227, train_acc: 78.61, train_fscore: 78.5, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.416800022125244, test_acc: 69.93, test_fscore: 70.34, time: 5.64 sec
loss: 2.9085488319396973
loss: 3.0291919708251953
loss: 3.4075512886047363
epoch: 74, train_loss: 2.9649999141693115, train_acc: 78.43, train_fscore: 78.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.407599925994873, test_acc: 69.87, test_fscore: 70.31, time: 5.67 sec
loss: 2.963613748550415
loss: 2.9392032623291016
loss: 3.3918356895446777
epoch: 75, train_loss: 2.95169997215271, train_acc: 78.83, train_fscore: 78.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3917999267578125, test_acc: 70.24, test_fscore: 70.65, time: 6.44 sec
loss: 3.0256948471069336
loss: 2.8284900188446045
loss: 3.3734469413757324
epoch: 76, train_loss: 2.9314000606536865, train_acc: 78.36, train_fscore: 78.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3733999729156494, test_acc: 69.81, test_fscore: 70.31, time: 5.59 sec
loss: 3.092545509338379
loss: 2.7431552410125732
loss: 3.3711581230163574
epoch: 77, train_loss: 2.9296998977661133, train_acc: 78.95, train_fscore: 78.85, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3712000846862793, test_acc: 70.18, test_fscore: 70.58, time: 6.45 sec
loss: 2.9213621616363525
loss: 2.885023355484009
loss: 3.3694608211517334
epoch: 78, train_loss: 2.905100107192993, train_acc: 78.93, train_fscore: 78.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.369499921798706, test_acc: 70.55, test_fscore: 70.89, time: 5.93 sec
loss: 2.982261896133423
loss: 2.776442527770996
loss: 3.4025793075561523
epoch: 79, train_loss: 2.884399890899658, train_acc: 79.28, train_fscore: 79.15, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.402600049972534, test_acc: 69.56, test_fscore: 70.08, time: 6.35 sec
loss: 2.846611738204956
loss: 2.923182964324951
loss: 3.3509681224823
epoch: 80, train_loss: 2.8835999965667725, train_acc: 79.38, train_fscore: 79.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3510000705718994, test_acc: 70.49, test_fscore: 70.87, time: 6.12 sec
              precision    recall  f1-score   support

           0     0.5098    0.7222    0.5977     144.0
           1     0.8097    0.7469    0.7771     245.0
           2     0.7251    0.6458    0.6832     384.0
           3     0.6114    0.7588    0.6772     170.0
           4     0.8704    0.7860    0.8260     299.0
           5     0.6649    0.6457    0.6551     381.0

    accuracy                         0.7055    1623.0
   macro avg     0.6985    0.7176    0.7027    1623.0
weighted avg     0.7195    0.7055    0.7089    1623.0

[[104.   7.  10.   0.  23.   0.]
 [  2. 183.  23.   3.   0.  34.]
 [ 45.  18. 248.  20.   4.  49.]
 [  0.   0.   1. 129.   0.  40.]
 [ 53.   2.   8.   0. 235.   1.]
 [  0.  16.  52.  59.   8. 246.]]
loss: 2.8647007942199707
loss: 2.882603168487549
loss: 3.355316638946533
epoch: 81, train_loss: 2.8729000091552734, train_acc: 79.41, train_fscore: 79.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.355299949645996, test_acc: 70.3, test_fscore: 70.69, time: 4.92 sec
loss: 3.040048599243164
loss: 2.671327590942383
loss: 3.359286308288574
epoch: 82, train_loss: 2.8608999252319336, train_acc: 79.43, train_fscore: 79.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.359299898147583, test_acc: 69.87, test_fscore: 70.3, time: 6.27 sec
loss: 2.75870418548584
loss: 2.9804348945617676
loss: 3.3493196964263916
epoch: 83, train_loss: 2.857599973678589, train_acc: 79.66, train_fscore: 79.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.349299907684326, test_acc: 70.36, test_fscore: 70.74, time: 6.0 sec
loss: 2.902496576309204
loss: 2.7835021018981934
loss: 3.347694158554077
epoch: 84, train_loss: 2.8457999229431152, train_acc: 79.35, train_fscore: 79.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3477001190185547, test_acc: 69.93, test_fscore: 70.36, time: 5.94 sec
loss: 2.983363389968872
loss: 2.6308889389038086
loss: 3.329735040664673
epoch: 85, train_loss: 2.8190999031066895, train_acc: 79.79, train_fscore: 79.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.329699993133545, test_acc: 70.67, test_fscore: 71.05, time: 6.25 sec
loss: 2.770981788635254
loss: 2.852071762084961
loss: 3.3322677612304688
epoch: 86, train_loss: 2.8108999729156494, train_acc: 80.21, train_fscore: 80.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3322999477386475, test_acc: 70.43, test_fscore: 70.81, time: 7.07 sec
loss: 2.8237268924713135
loss: 2.7680091857910156
loss: 3.352696418762207
epoch: 87, train_loss: 2.79830002784729, train_acc: 79.64, train_fscore: 79.5, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3526999950408936, test_acc: 70.67, test_fscore: 71.04, time: 5.88 sec
loss: 2.843876838684082
loss: 2.763226270675659
loss: 3.3208935260772705
epoch: 88, train_loss: 2.809499979019165, train_acc: 80.09, train_fscore: 79.97, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3208999633789062, test_acc: 70.67, test_fscore: 71.04, time: 6.09 sec
loss: 2.76871395111084
loss: 2.7796502113342285
loss: 3.314473867416382
epoch: 89, train_loss: 2.773699998855591, train_acc: 80.17, train_fscore: 80.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.314500093460083, test_acc: 70.61, test_fscore: 71.0, time: 6.12 sec
loss: 2.6739742755889893
loss: 2.897108554840088
loss: 3.337151050567627
epoch: 90, train_loss: 2.773400068283081, train_acc: 80.65, train_fscore: 80.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3371999263763428, test_acc: 70.92, test_fscore: 71.33, time: 6.09 sec
              precision    recall  f1-score   support

           0     0.5045    0.7778    0.6120     144.0
           1     0.8238    0.7633    0.7924     245.0
           2     0.7147    0.6589    0.6856     384.0
           3     0.6497    0.7529    0.6975     170.0
           4     0.8911    0.7391    0.8080     299.0
           5     0.6667    0.6562    0.6614     381.0

    accuracy                         0.7092    1623.0
   macro avg     0.7084    0.7247    0.7095    1623.0
weighted avg     0.7269    0.7092    0.7133    1623.0

[[112.   5.   9.   0.  18.   0.]
 [  2. 187.  20.   2.   0.  34.]
 [ 45.  18. 253.  16.   3.  49.]
 [  0.   0.   1. 128.   0.  41.]
 [ 63.   2.  12.   0. 221.   1.]
 [  0.  15.  59.  51.   6. 250.]]
loss: 2.782626152038574
loss: 2.7509636878967285
loss: 3.321493148803711
epoch: 91, train_loss: 2.7688000202178955, train_acc: 80.33, train_fscore: 80.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.321500062942505, test_acc: 70.73, test_fscore: 71.12, time: 5.74 sec
loss: 2.728257656097412
loss: 2.7585301399230957
loss: 3.311288833618164
epoch: 92, train_loss: 2.742000102996826, train_acc: 81.05, train_fscore: 80.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.311300039291382, test_acc: 71.1, test_fscore: 71.49, time: 6.91 sec
loss: 2.7106165885925293
loss: 2.7414603233337402
loss: 3.306140184402466
epoch: 93, train_loss: 2.724299907684326, train_acc: 80.74, train_fscore: 80.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3060998916625977, test_acc: 70.61, test_fscore: 71.01, time: 5.32 sec
loss: 2.742703914642334
loss: 2.706089496612549
loss: 3.309631586074829
epoch: 94, train_loss: 2.7246999740600586, train_acc: 80.26, train_fscore: 80.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3096001148223877, test_acc: 71.35, test_fscore: 71.71, time: 6.78 sec
loss: 2.725113868713379
loss: 2.7448973655700684
loss: 3.3279764652252197
epoch: 95, train_loss: 2.7344000339508057, train_acc: 80.62, train_fscore: 80.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.328000068664551, test_acc: 70.55, test_fscore: 70.94, time: 6.23 sec
loss: 2.7485365867614746
loss: 2.637746572494507
loss: 3.294128894805908
epoch: 96, train_loss: 2.6979000568389893, train_acc: 80.84, train_fscore: 80.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.294100046157837, test_acc: 71.29, test_fscore: 71.65, time: 5.64 sec
loss: 2.7613449096679688
loss: 2.641226053237915
loss: 3.2887678146362305
epoch: 97, train_loss: 2.7063000202178955, train_acc: 80.95, train_fscore: 80.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.288800001144409, test_acc: 71.1, test_fscore: 71.46, time: 6.35 sec
loss: 2.6492574214935303
loss: 2.7339890003204346
loss: 3.309539556503296
epoch: 98, train_loss: 2.690999984741211, train_acc: 81.65, train_fscore: 81.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.309499979019165, test_acc: 71.35, test_fscore: 71.74, time: 5.84 sec
loss: 2.6980080604553223
loss: 2.6548972129821777
loss: 3.2739667892456055
epoch: 99, train_loss: 2.6779000759124756, train_acc: 81.6, train_fscore: 81.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2739999294281006, test_acc: 71.35, test_fscore: 71.69, time: 6.26 sec
loss: 2.6035497188568115
loss: 2.749319553375244
loss: 3.282944440841675
epoch: 100, train_loss: 2.672499895095825, train_acc: 80.84, train_fscore: 80.7, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.282900094985962, test_acc: 71.41, test_fscore: 71.8, time: 5.57 sec
              precision    recall  f1-score   support

           0     0.5160    0.7847    0.6226     144.0
           1     0.8326    0.7714    0.8008     245.0
           2     0.7361    0.6536    0.6924     384.0
           3     0.6172    0.7588    0.6807     170.0
           4     0.8876    0.7659    0.8223     299.0
           5     0.6721    0.6509    0.6613     381.0

    accuracy                         0.7141    1623.0
   macro avg     0.7103    0.7309    0.7134    1623.0
weighted avg     0.7316    0.7141    0.7180    1623.0

[[113.   4.   8.   0.  19.   0.]
 [  2. 189.  18.   3.   0.  33.]
 [ 46.  18. 251.  19.   3.  47.]
 [  0.   0.   1. 129.   0.  40.]
 [ 58.   1.  10.   0. 229.   1.]
 [  0.  15.  53.  58.   7. 248.]]
loss: 2.655404567718506
loss: 2.681347370147705
loss: 3.3161613941192627
epoch: 101, train_loss: 2.66729998588562, train_acc: 81.31, train_fscore: 81.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.316200017929077, test_acc: 71.16, test_fscore: 71.55, time: 6.13 sec
loss: 2.8485214710235596
loss: 2.4202399253845215
loss: 3.319969415664673
epoch: 102, train_loss: 2.647200107574463, train_acc: 82.0, train_fscore: 81.9, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.319999933242798, test_acc: 71.16, test_fscore: 71.54, time: 6.52 sec
loss: 2.560624837875366
loss: 2.7154011726379395
loss: 3.309352159500122
epoch: 103, train_loss: 2.6319000720977783, train_acc: 81.45, train_fscore: 81.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3094000816345215, test_acc: 71.29, test_fscore: 71.67, time: 6.16 sec
loss: 2.569873094558716
loss: 2.6783249378204346
loss: 3.2946486473083496
epoch: 104, train_loss: 2.620300054550171, train_acc: 81.81, train_fscore: 81.7, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.294600009918213, test_acc: 71.6, test_fscore: 71.99, time: 4.89 sec
loss: 2.4596853256225586
loss: 2.7590787410736084
loss: 3.2797656059265137
epoch: 105, train_loss: 2.601099967956543, train_acc: 81.91, train_fscore: 81.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2797999382019043, test_acc: 71.04, test_fscore: 71.4, time: 3.07 sec
loss: 2.628037452697754
loss: 2.580717086791992
loss: 3.2995991706848145
epoch: 106, train_loss: 2.6059000492095947, train_acc: 82.12, train_fscore: 82.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2995998859405518, test_acc: 71.23, test_fscore: 71.62, time: 4.06 sec
loss: 2.5596206188201904
loss: 2.6985840797424316
loss: 3.270691394805908
epoch: 107, train_loss: 2.6231000423431396, train_acc: 82.24, train_fscore: 82.16, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.270699977874756, test_acc: 71.16, test_fscore: 71.52, time: 4.75 sec
loss: 2.5103135108947754
loss: 2.7241616249084473
loss: 3.2526278495788574
epoch: 108, train_loss: 2.605299949645996, train_acc: 82.03, train_fscore: 81.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2525999546051025, test_acc: 72.03, test_fscore: 72.35, time: 5.31 sec
loss: 2.6624767780303955
loss: 2.4844532012939453
loss: 3.2702605724334717
epoch: 109, train_loss: 2.577399969100952, train_acc: 82.46, train_fscore: 82.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2702999114990234, test_acc: 71.16, test_fscore: 71.52, time: 6.2 sec
loss: 2.584848642349243
loss: 2.551301956176758
loss: 3.2992396354675293
epoch: 110, train_loss: 2.5690999031066895, train_acc: 81.98, train_fscore: 81.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2992000579833984, test_acc: 71.35, test_fscore: 71.71, time: 5.24 sec
              precision    recall  f1-score   support

           0     0.5308    0.7778    0.6310     144.0
           1     0.8268    0.7796    0.8025     245.0
           2     0.7214    0.6745    0.6972     384.0
           3     0.6649    0.7471    0.7036     170.0
           4     0.8837    0.7625    0.8187     299.0
           5     0.6756    0.6614    0.6684     381.0

    accuracy                         0.7203    1623.0
   macro avg     0.7172    0.7338    0.7202    1623.0
weighted avg     0.7337    0.7203    0.7235    1623.0

[[112.   4.   7.   0.  21.   0.]
 [  2. 191.  18.   1.   0.  33.]
 [ 43.  19. 259.  15.   3.  45.]
 [  0.   0.   1. 127.   0.  42.]
 [ 54.   2.  14.   0. 228.   1.]
 [  0.  15.  60.  48.   6. 252.]]
loss: 2.6093249320983887
loss: 2.5492935180664062
loss: 3.3092072010040283
epoch: 111, train_loss: 2.580399990081787, train_acc: 82.34, train_fscore: 82.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3092000484466553, test_acc: 71.41, test_fscore: 71.78, time: 5.58 sec
loss: 2.7417044639587402
loss: 2.3738293647766113
loss: 3.2768983840942383
epoch: 112, train_loss: 2.5673999786376953, train_acc: 82.44, train_fscore: 82.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.276900053024292, test_acc: 71.04, test_fscore: 71.4, time: 6.12 sec
loss: 2.6127166748046875
loss: 2.47080659866333
loss: 3.293715000152588
epoch: 113, train_loss: 2.5497000217437744, train_acc: 82.99, train_fscore: 82.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2936999797821045, test_acc: 71.47, test_fscore: 71.8, time: 5.99 sec
loss: 2.5862317085266113
loss: 2.4944612979888916
loss: 3.3331618309020996
epoch: 114, train_loss: 2.541300058364868, train_acc: 82.63, train_fscore: 82.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.333199977874756, test_acc: 70.73, test_fscore: 71.13, time: 6.31 sec
loss: 2.5116569995880127
loss: 2.5679714679718018
loss: 3.2982945442199707
epoch: 115, train_loss: 2.538800001144409, train_acc: 82.7, train_fscore: 82.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.29830002784729, test_acc: 71.35, test_fscore: 71.69, time: 6.1 sec
loss: 2.403280735015869
loss: 2.640549421310425
loss: 3.2656617164611816
epoch: 116, train_loss: 2.507699966430664, train_acc: 83.12, train_fscore: 83.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.265700101852417, test_acc: 71.6, test_fscore: 71.95, time: 6.75 sec
loss: 2.374497890472412
loss: 2.672680139541626
loss: 3.2660269737243652
epoch: 117, train_loss: 2.5130999088287354, train_acc: 82.79, train_fscore: 82.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2660000324249268, test_acc: 71.84, test_fscore: 72.15, time: 6.11 sec
loss: 2.5725903511047363
loss: 2.444507360458374
loss: 3.304807662963867
epoch: 118, train_loss: 2.513700008392334, train_acc: 83.46, train_fscore: 83.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.304800033569336, test_acc: 71.78, test_fscore: 72.12, time: 5.78 sec
loss: 2.504152297973633
loss: 2.493804693222046
loss: 3.317204713821411
epoch: 119, train_loss: 2.4992001056671143, train_acc: 83.36, train_fscore: 83.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.317199945449829, test_acc: 71.47, test_fscore: 71.87, time: 6.05 sec
loss: 2.5220065116882324
loss: 2.490326404571533
loss: 3.302283763885498
epoch: 120, train_loss: 2.5069000720977783, train_acc: 82.86, train_fscore: 82.75, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.302299976348877, test_acc: 71.1, test_fscore: 71.52, time: 6.51 sec
              precision    recall  f1-score   support

           0     0.5308    0.7778    0.6310     144.0
           1     0.8268    0.7796    0.8025     245.0
           2     0.7214    0.6745    0.6972     384.0
           3     0.6649    0.7471    0.7036     170.0
           4     0.8837    0.7625    0.8187     299.0
           5     0.6756    0.6614    0.6684     381.0

    accuracy                         0.7203    1623.0
   macro avg     0.7172    0.7338    0.7202    1623.0
weighted avg     0.7337    0.7203    0.7235    1623.0

[[112.   4.   7.   0.  21.   0.]
 [  2. 191.  18.   1.   0.  33.]
 [ 43.  19. 259.  15.   3.  45.]
 [  0.   0.   1. 127.   0.  42.]
 [ 54.   2.  14.   0. 228.   1.]
 [  0.  15.  60.  48.   6. 252.]]
loss: 2.2940704822540283
loss: 2.6981749534606934
loss: 3.28737211227417
epoch: 121, train_loss: 2.486599922180176, train_acc: 82.84, train_fscore: 82.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.287400007247925, test_acc: 71.53, test_fscore: 71.85, time: 6.5 sec
loss: 2.533848285675049
loss: 2.4115540981292725
loss: 3.274742603302002
epoch: 122, train_loss: 2.4769999980926514, train_acc: 83.25, train_fscore: 83.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2746999263763428, test_acc: 71.6, test_fscore: 71.94, time: 6.23 sec
loss: 2.52642560005188
loss: 2.4182937145233154
loss: 3.292168140411377
epoch: 123, train_loss: 2.474100112915039, train_acc: 83.48, train_fscore: 83.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2922000885009766, test_acc: 71.6, test_fscore: 71.93, time: 6.37 sec
loss: 2.626181125640869
loss: 2.3034005165100098
loss: 3.31042742729187
epoch: 124, train_loss: 2.474299907684326, train_acc: 83.92, train_fscore: 83.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3104000091552734, test_acc: 71.53, test_fscore: 71.96, time: 6.18 sec
loss: 2.489135265350342
loss: 2.442528247833252
loss: 3.315638303756714
epoch: 125, train_loss: 2.4665000438690186, train_acc: 83.44, train_fscore: 83.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3155999183654785, test_acc: 71.04, test_fscore: 71.44, time: 6.37 sec
loss: 2.450151205062866
loss: 2.4484357833862305
loss: 3.3447976112365723
epoch: 126, train_loss: 2.4493000507354736, train_acc: 83.36, train_fscore: 83.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3447999954223633, test_acc: 71.6, test_fscore: 71.88, time: 6.29 sec
loss: 2.3821609020233154
loss: 2.5338218212127686
loss: 3.2991106510162354
epoch: 127, train_loss: 2.455899953842163, train_acc: 84.32, train_fscore: 84.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.299099922180176, test_acc: 71.66, test_fscore: 72.06, time: 5.53 sec
loss: 2.426481246948242
loss: 2.4924845695495605
loss: 3.273218870162964
epoch: 128, train_loss: 2.4576001167297363, train_acc: 83.43, train_fscore: 83.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.273200035095215, test_acc: 71.78, test_fscore: 72.11, time: 6.75 sec
loss: 2.452021598815918
loss: 2.410144805908203
loss: 3.27756929397583
epoch: 129, train_loss: 2.4321000576019287, train_acc: 84.29, train_fscore: 84.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.277600049972534, test_acc: 72.09, test_fscore: 72.44, time: 6.33 sec
loss: 2.362313985824585
loss: 2.477830648422241
loss: 3.3271842002868652
epoch: 130, train_loss: 2.413599967956543, train_acc: 84.13, train_fscore: 84.06, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.327199935913086, test_acc: 70.79, test_fscore: 71.19, time: 6.18 sec
              precision    recall  f1-score   support

           0     0.5337    0.7708    0.6307     144.0
           1     0.8444    0.7755    0.8085     245.0
           2     0.7139    0.6953    0.7045     384.0
           3     0.6396    0.7412    0.6866     170.0
           4     0.8824    0.7525    0.8123     299.0
           5     0.6896    0.6588    0.6738     381.0

    accuracy                         0.7209    1623.0
   macro avg     0.7173    0.7324    0.7194    1623.0
weighted avg     0.7352    0.7209    0.7244    1623.0

[[111.   4.   7.   0.  22.   0.]
 [  2. 190.  20.   3.   0.  30.]
 [ 40.  18. 267.  17.   3.  39.]
 [  0.   0.   1. 126.   0.  43.]
 [ 54.   0.  19.   0. 225.   1.]
 [  1.  13.  60.  51.   5. 251.]]
loss: 2.42305850982666
loss: 2.396862030029297
loss: 3.303551435470581
epoch: 131, train_loss: 2.410799980163574, train_acc: 84.18, train_fscore: 84.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3036000728607178, test_acc: 71.53, test_fscore: 71.86, time: 5.82 sec
loss: 2.413846254348755
loss: 2.3919358253479004
loss: 3.294146776199341
epoch: 132, train_loss: 2.4031999111175537, train_acc: 84.46, train_fscore: 84.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.294100046157837, test_acc: 71.41, test_fscore: 71.77, time: 6.05 sec
loss: 2.361891746520996
loss: 2.4392237663269043
loss: 3.2910797595977783
epoch: 133, train_loss: 2.3966000080108643, train_acc: 83.92, train_fscore: 83.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.291100025177002, test_acc: 71.41, test_fscore: 71.79, time: 5.49 sec
loss: 2.362328052520752
loss: 2.4185149669647217
loss: 3.305990695953369
epoch: 134, train_loss: 2.3879001140594482, train_acc: 83.98, train_fscore: 83.9, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.305999994277954, test_acc: 71.47, test_fscore: 71.74, time: 6.38 sec
loss: 2.4679243564605713
loss: 2.289857864379883
loss: 3.3304989337921143
epoch: 135, train_loss: 2.3873000144958496, train_acc: 84.46, train_fscore: 84.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3304998874664307, test_acc: 71.53, test_fscore: 71.93, time: 6.41 sec
loss: 2.3983099460601807
loss: 2.3710150718688965
loss: 3.324018955230713
epoch: 136, train_loss: 2.3852999210357666, train_acc: 84.08, train_fscore: 84.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3239998817443848, test_acc: 71.47, test_fscore: 71.77, time: 6.06 sec
loss: 2.4245076179504395
loss: 2.2943382263183594
loss: 3.298037052154541
epoch: 137, train_loss: 2.3640999794006348, train_acc: 84.34, train_fscore: 84.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2980000972747803, test_acc: 71.72, test_fscore: 72.04, time: 6.53 sec
loss: 2.212606191635132
loss: 2.5510528087615967
loss: 3.343200206756592
epoch: 138, train_loss: 2.3752999305725098, train_acc: 84.85, train_fscore: 84.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3431999683380127, test_acc: 71.04, test_fscore: 71.44, time: 5.09 sec
loss: 2.3968307971954346
loss: 2.302678346633911
loss: 3.3092572689056396
epoch: 139, train_loss: 2.352099895477295, train_acc: 84.66, train_fscore: 84.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.309299945831299, test_acc: 72.15, test_fscore: 72.51, time: 6.75 sec
loss: 2.2617483139038086
loss: 2.41790771484375
loss: 3.2819368839263916
epoch: 140, train_loss: 2.336400032043457, train_acc: 84.66, train_fscore: 84.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.281899929046631, test_acc: 71.53, test_fscore: 71.87, time: 6.02 sec
              precision    recall  f1-score   support

           0     0.5324    0.7986    0.6389     144.0
           1     0.8348    0.7837    0.8084     245.0
           2     0.7225    0.6849    0.7032     384.0
           3     0.6462    0.7412    0.6904     170.0
           4     0.8911    0.7391    0.8080     299.0
           5     0.6865    0.6667    0.6764     381.0

    accuracy                         0.7215    1623.0
   macro avg     0.7189    0.7357    0.7209    1623.0
weighted avg     0.7372    0.7215    0.7251    1623.0

[[115.   4.   6.   0.  19.   0.]
 [  2. 192.  20.   3.   0.  28.]
 [ 40.  19. 263.  15.   3.  44.]
 [  0.   0.   1. 126.   0.  43.]
 [ 59.   1.  17.   0. 221.   1.]
 [  0.  14.  57.  51.   5. 254.]]
loss: 2.448066234588623
loss: 2.240081787109375
loss: 3.3107850551605225
epoch: 141, train_loss: 2.3485000133514404, train_acc: 84.66, train_fscore: 84.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.310800075531006, test_acc: 71.04, test_fscore: 71.42, time: 6.37 sec
loss: 2.383885622024536
loss: 2.265453815460205
loss: 3.337219476699829
epoch: 142, train_loss: 2.331199884414673, train_acc: 85.04, train_fscore: 84.94, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3371999263763428, test_acc: 71.29, test_fscore: 71.59, time: 6.44 sec
loss: 2.3739113807678223
loss: 2.252316951751709
loss: 3.327714681625366
epoch: 143, train_loss: 2.3178999423980713, train_acc: 85.37, train_fscore: 85.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.327699899673462, test_acc: 71.1, test_fscore: 71.45, time: 6.07 sec
loss: 2.3345398902893066
loss: 2.293224334716797
loss: 3.315925359725952
epoch: 144, train_loss: 2.3143999576568604, train_acc: 85.03, train_fscore: 84.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3159000873565674, test_acc: 71.23, test_fscore: 71.58, time: 5.55 sec
loss: 2.374664068222046
loss: 2.2390618324279785
loss: 3.3018393516540527
epoch: 145, train_loss: 2.313199996948242, train_acc: 84.97, train_fscore: 84.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.301800012588501, test_acc: 71.72, test_fscore: 72.04, time: 6.57 sec
loss: 2.318329334259033
loss: 2.293728828430176
loss: 3.337311267852783
epoch: 146, train_loss: 2.306999921798706, train_acc: 85.42, train_fscore: 85.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3373000621795654, test_acc: 71.53, test_fscore: 71.85, time: 6.01 sec
loss: 2.349301815032959
loss: 2.202296257019043
loss: 3.310786724090576
epoch: 147, train_loss: 2.278700113296509, train_acc: 86.13, train_fscore: 86.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.310800075531006, test_acc: 71.47, test_fscore: 71.79, time: 6.66 sec
loss: 2.2399606704711914
loss: 2.3527300357818604
loss: 3.316859722137451
epoch: 148, train_loss: 2.291100025177002, train_acc: 85.47, train_fscore: 85.37, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3169000148773193, test_acc: 71.41, test_fscore: 71.72, time: 6.28 sec
loss: 2.489234447479248
loss: 2.1002285480499268
loss: 3.3331916332244873
epoch: 149, train_loss: 2.3032000064849854, train_acc: 85.28, train_fscore: 85.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.333199977874756, test_acc: 71.72, test_fscore: 72.06, time: 5.81 sec
loss: 2.2930588722229004
loss: 2.309004306793213
loss: 3.3334991931915283
epoch: 150, train_loss: 2.3004000186920166, train_acc: 85.15, train_fscore: 85.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3334999084472656, test_acc: 71.53, test_fscore: 71.85, time: 6.49 sec
              precision    recall  f1-score   support

           0     0.5324    0.7986    0.6389     144.0
           1     0.8348    0.7837    0.8084     245.0
           2     0.7225    0.6849    0.7032     384.0
           3     0.6462    0.7412    0.6904     170.0
           4     0.8911    0.7391    0.8080     299.0
           5     0.6865    0.6667    0.6764     381.0

    accuracy                         0.7215    1623.0
   macro avg     0.7189    0.7357    0.7209    1623.0
weighted avg     0.7372    0.7215    0.7251    1623.0

[[115.   4.   6.   0.  19.   0.]
 [  2. 192.  20.   3.   0.  28.]
 [ 40.  19. 263.  15.   3.  44.]
 [  0.   0.   1. 126.   0.  43.]
 [ 59.   1.  17.   0. 221.   1.]
 [  0.  14.  57.  51.   5. 254.]]
Test performance..
F-Score: 72.51
F-Score-index: 139
              precision    recall  f1-score   support

           0     0.5324    0.7986    0.6389     144.0
           1     0.8348    0.7837    0.8084     245.0
           2     0.7225    0.6849    0.7032     384.0
           3     0.6462    0.7412    0.6904     170.0
           4     0.8911    0.7391    0.8080     299.0
           5     0.6865    0.6667    0.6764     381.0

    accuracy                         0.7215    1623.0
   macro avg     0.7189    0.7357    0.7209    1623.0
weighted avg     0.7372    0.7215    0.7251    1623.0

[[115.   4.   6.   0.  19.   0.]
 [  2. 192.  20.   3.   0.  28.]
 [ 40.  19. 263.  15.   3.  44.]
 [  0.   0.   1. 126.   0.  43.]
 [ 59.   1.  17.   0. 221.   1.]
 [  0.  14.  57.  51.   5. 254.]]
--- 6 ---
Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.5, batch_size=64, hidden_dim=1024, n_head=8, epochs=150, temp=2, tensorboard=False, class_weight=True, Dataset='IEMOCAP')
Running on GPU
temp 2
total parameters: 79687704
training parameters: 79687704
loss: 7.760219573974609
loss: 11.675048828125
loss: 7.554005146026611
epoch: 1, train_loss: 9.657699584960938, train_acc: 19.29, train_fscore: 15.75, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.553999900817871, test_acc: 19.16, test_fscore: 7.38, time: 7.82 sec
loss: 7.955230712890625
loss: 8.865257263183594
loss: 8.418652534484863
epoch: 2, train_loss: 8.402099609375, train_acc: 23.3, train_fscore: 20.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.418700218200684, test_acc: 32.35, test_fscore: 24.9, time: 6.18 sec
loss: 8.851035118103027
loss: 7.973794460296631
loss: 7.091753005981445
epoch: 3, train_loss: 8.459500312805176, train_acc: 39.47, train_fscore: 32.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.091800212860107, test_acc: 38.63, test_fscore: 35.47, time: 6.85 sec
loss: 7.40382194519043
loss: 7.417600631713867
loss: 7.195302963256836
epoch: 4, train_loss: 7.4105000495910645, train_acc: 34.66, train_fscore: 30.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.195300102233887, test_acc: 43.81, test_fscore: 37.08, time: 6.34 sec
loss: 7.342339515686035
loss: 7.296953201293945
loss: 7.071120262145996
epoch: 5, train_loss: 7.321599960327148, train_acc: 42.34, train_fscore: 38.16, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.071100234985352, test_acc: 39.68, test_fscore: 36.39, time: 7.0 sec
loss: 7.008379936218262
loss: 7.011991024017334
loss: 6.682219505310059
epoch: 6, train_loss: 7.0100998878479, train_acc: 47.06, train_fscore: 45.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.682199954986572, test_acc: 43.87, test_fscore: 40.87, time: 5.76 sec
loss: 6.6107683181762695
loss: 6.829723834991455
loss: 6.633299350738525
epoch: 7, train_loss: 6.711299896240234, train_acc: 48.88, train_fscore: 42.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.633299827575684, test_acc: 52.13, test_fscore: 45.37, time: 6.08 sec
loss: 6.621673583984375
loss: 6.581811904907227
loss: 6.516025543212891
epoch: 8, train_loss: 6.6031999588012695, train_acc: 53.24, train_fscore: 47.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.515999794006348, test_acc: 50.4, test_fscore: 49.46, time: 6.88 sec
loss: 6.426337242126465
loss: 6.436062335968018
loss: 6.355283737182617
epoch: 9, train_loss: 6.4309000968933105, train_acc: 54.42, train_fscore: 52.51, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.355299949645996, test_acc: 49.78, test_fscore: 49.94, time: 6.21 sec
loss: 6.388421058654785
loss: 6.053114414215088
loss: 6.199312210083008
epoch: 10, train_loss: 6.244900226593018, train_acc: 55.78, train_fscore: 55.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.1992998123168945, test_acc: 53.79, test_fscore: 54.02, time: 6.62 sec
              precision    recall  f1-score   support

           0     0.2598    0.4583    0.3317     144.0
           1     0.7655    0.7061    0.7346     245.0
           2     0.4188    0.3021    0.3510     384.0
           3     0.6188    0.5824    0.6000     170.0
           4     0.6713    0.6488    0.6599     299.0
           5     0.5396    0.5906    0.5639     381.0

    accuracy                         0.5379    1623.0
   macro avg     0.5456    0.5480    0.5402    1623.0
weighted avg     0.5528    0.5379    0.5402    1623.0

[[ 66.   4.  18.   3.  51.   2.]
 [ 27. 173.  16.   2.   3.  24.]
 [106.  26. 116.   5.  20. 111.]
 [  0.   3.  16.  99.   9.  43.]
 [ 42.   3.  43.   5. 194.  12.]
 [ 13.  17.  68.  46.  12. 225.]]
loss: 6.142367362976074
loss: 5.936469554901123
loss: 6.020272254943848
epoch: 11, train_loss: 6.05079984664917, train_acc: 59.69, train_fscore: 59.15, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.020299911499023, test_acc: 54.65, test_fscore: 53.55, time: 5.66 sec
loss: 5.917481899261475
loss: 5.753170967102051
loss: 5.809020519256592
epoch: 12, train_loss: 5.841100215911865, train_acc: 58.97, train_fscore: 57.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.809000015258789, test_acc: 58.23, test_fscore: 57.46, time: 6.6 sec
loss: 5.8499650955200195
loss: 5.407125473022461
loss: 5.609279632568359
epoch: 13, train_loss: 5.644499778747559, train_acc: 60.64, train_fscore: 58.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.609300136566162, test_acc: 59.52, test_fscore: 58.77, time: 6.33 sec
loss: 5.448709011077881
loss: 5.461919784545898
loss: 5.4017486572265625
epoch: 14, train_loss: 5.455100059509277, train_acc: 60.76, train_fscore: 58.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.401700019836426, test_acc: 60.07, test_fscore: 59.37, time: 5.52 sec
loss: 5.287521839141846
loss: 5.284409523010254
loss: 5.228339195251465
epoch: 15, train_loss: 5.286099910736084, train_acc: 62.7, train_fscore: 61.09, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.228300094604492, test_acc: 61.31, test_fscore: 60.65, time: 6.59 sec
loss: 5.085156440734863
loss: 5.23819637298584
loss: 5.094330787658691
epoch: 16, train_loss: 5.156300067901611, train_acc: 63.6, train_fscore: 62.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.094299793243408, test_acc: 62.11, test_fscore: 61.6, time: 6.58 sec
loss: 4.997047424316406
loss: 5.0787224769592285
loss: 4.958456516265869
epoch: 17, train_loss: 5.036399841308594, train_acc: 64.61, train_fscore: 63.15, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.958499908447266, test_acc: 61.61, test_fscore: 62.08, time: 6.02 sec
loss: 4.8188652992248535
loss: 4.989383697509766
loss: 4.854791164398193
epoch: 18, train_loss: 4.898600101470947, train_acc: 64.92, train_fscore: 63.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.854800224304199, test_acc: 64.26, test_fscore: 64.79, time: 6.58 sec
loss: 4.736274719238281
loss: 4.882386207580566
loss: 4.745997428894043
epoch: 19, train_loss: 4.80679988861084, train_acc: 65.89, train_fscore: 65.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.745999813079834, test_acc: 63.59, test_fscore: 64.26, time: 6.01 sec
loss: 4.639676570892334
loss: 4.8152337074279785
loss: 4.633821964263916
epoch: 20, train_loss: 4.722599983215332, train_acc: 65.83, train_fscore: 65.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.633800029754639, test_acc: 63.4, test_fscore: 64.0, time: 7.46 sec
              precision    recall  f1-score   support

           0     0.3636    0.5833    0.4480     144.0
           1     0.7511    0.7265    0.7386     245.0
           2     0.6622    0.5104    0.5765     384.0
           3     0.6029    0.7235    0.6578     170.0
           4     0.8372    0.7224    0.7756     299.0
           5     0.6196    0.6457    0.6324     381.0

    accuracy                         0.6426    1623.0
   macro avg     0.6394    0.6520    0.6381    1623.0
weighted avg     0.6652    0.6426    0.6479    1623.0

[[ 84.  11.   8.   3.  36.   2.]
 [  3. 178.  32.   3.   0.  29.]
 [ 62.  27. 196.  21.   2.  76.]
 [  0.   0.   5. 123.   0.  42.]
 [ 73.   0.   8.   0. 216.   2.]
 [  9.  21.  47.  54.   4. 246.]]
loss: 4.872241973876953
loss: 4.382340431213379
loss: 4.574381351470947
epoch: 21, train_loss: 4.646299839019775, train_acc: 65.92, train_fscore: 65.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.574399948120117, test_acc: 63.65, test_fscore: 64.11, time: 6.04 sec
loss: 4.563780784606934
loss: 4.611710548400879
loss: 4.522671222686768
epoch: 22, train_loss: 4.58519983291626, train_acc: 66.32, train_fscore: 65.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.52269983291626, test_acc: 63.71, test_fscore: 63.82, time: 6.3 sec
loss: 4.404641628265381
loss: 4.6146063804626465
loss: 4.442070484161377
epoch: 23, train_loss: 4.503900051116943, train_acc: 64.44, train_fscore: 63.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.4421000480651855, test_acc: 64.2, test_fscore: 64.47, time: 6.02 sec
loss: 4.400393009185791
loss: 4.501163482666016
loss: 4.372848987579346
epoch: 24, train_loss: 4.448299884796143, train_acc: 65.68, train_fscore: 64.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.372799873352051, test_acc: 65.62, test_fscore: 65.87, time: 5.66 sec
loss: 4.442533493041992
loss: 4.320058345794678
loss: 4.345375061035156
epoch: 25, train_loss: 4.388299942016602, train_acc: 66.95, train_fscore: 66.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.345399856567383, test_acc: 65.25, test_fscore: 65.64, time: 6.65 sec
loss: 4.309928894042969
loss: 4.324836254119873
loss: 4.345220565795898
epoch: 26, train_loss: 4.316699981689453, train_acc: 67.52, train_fscore: 66.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.345200061798096, test_acc: 65.25, test_fscore: 65.85, time: 5.69 sec
loss: 4.295289516448975
loss: 4.280917167663574
loss: 4.338738441467285
epoch: 27, train_loss: 4.288700103759766, train_acc: 66.87, train_fscore: 66.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.338699817657471, test_acc: 65.43, test_fscore: 65.62, time: 6.84 sec
loss: 4.387575149536133
loss: 4.065506458282471
loss: 4.274063587188721
epoch: 28, train_loss: 4.237500190734863, train_acc: 67.47, train_fscore: 66.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.274099826812744, test_acc: 65.8, test_fscore: 66.15, time: 6.84 sec
loss: 4.251358509063721
loss: 4.1377692222595215
loss: 4.222912311553955
epoch: 29, train_loss: 4.199100017547607, train_acc: 67.54, train_fscore: 66.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.222899913787842, test_acc: 65.93, test_fscore: 66.27, time: 6.73 sec
loss: 4.035819053649902
loss: 4.295406341552734
loss: 4.204469680786133
epoch: 30, train_loss: 4.160299777984619, train_acc: 68.18, train_fscore: 67.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.204500198364258, test_acc: 65.68, test_fscore: 65.99, time: 5.9 sec
              precision    recall  f1-score   support

           0     0.4089    0.5764    0.4784     144.0
           1     0.7291    0.7469    0.7379     245.0
           2     0.6646    0.5521    0.6031     384.0
           3     0.5787    0.7353    0.6477     170.0
           4     0.8550    0.7692    0.8099     299.0
           5     0.6493    0.6220    0.6354     381.0

    accuracy                         0.6593    1623.0
   macro avg     0.6476    0.6670    0.6521    1623.0
weighted avg     0.6741    0.6593    0.6627    1623.0

[[ 83.  10.  12.   5.  33.   1.]
 [  2. 183.  31.   0.   0.  29.]
 [ 55.  33. 212.  26.   2.  56.]
 [  0.   0.   4. 125.   0.  41.]
 [ 60.   0.   8.   0. 230.   1.]
 [  3.  25.  52.  60.   4. 237.]]
loss: 4.080750465393066
loss: 4.124728202819824
loss: 4.209070205688477
epoch: 31, train_loss: 4.10230016708374, train_acc: 69.36, train_fscore: 68.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.209099769592285, test_acc: 65.13, test_fscore: 65.68, time: 6.3 sec
loss: 3.9855380058288574
loss: 4.149628162384033
loss: 4.158095359802246
epoch: 32, train_loss: 4.061299800872803, train_acc: 69.0, train_fscore: 68.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.158100128173828, test_acc: 65.99, test_fscore: 66.46, time: 7.04 sec
loss: 3.968291759490967
loss: 4.024546146392822
loss: 4.113831996917725
epoch: 33, train_loss: 3.9941000938415527, train_acc: 70.14, train_fscore: 69.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.113800048828125, test_acc: 66.36, test_fscore: 66.9, time: 6.08 sec
loss: 4.020448207855225
loss: 3.895754337310791
loss: 4.083484649658203
epoch: 34, train_loss: 3.963900089263916, train_acc: 70.46, train_fscore: 70.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.083499908447266, test_acc: 66.85, test_fscore: 67.35, time: 6.29 sec
loss: 3.9313931465148926
loss: 3.9256749153137207
loss: 4.046046257019043
epoch: 35, train_loss: 3.9286999702453613, train_acc: 70.33, train_fscore: 69.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.046000003814697, test_acc: 66.91, test_fscore: 67.39, time: 6.48 sec
loss: 4.094647407531738
loss: 3.7052743434906006
loss: 4.00936222076416
epoch: 36, train_loss: 3.9184000492095947, train_acc: 70.95, train_fscore: 70.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.009399890899658, test_acc: 67.22, test_fscore: 67.71, time: 6.16 sec
loss: 3.8553831577301025
loss: 3.8372130393981934
loss: 3.9784159660339355
epoch: 37, train_loss: 3.847100019454956, train_acc: 71.69, train_fscore: 71.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9783999919891357, test_acc: 67.04, test_fscore: 67.54, time: 6.32 sec
loss: 3.880894660949707
loss: 3.765031099319458
loss: 3.949432849884033
epoch: 38, train_loss: 3.82669997215271, train_acc: 71.22, train_fscore: 70.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.949399948120117, test_acc: 67.16, test_fscore: 67.65, time: 5.34 sec
loss: 3.882871150970459
loss: 3.6507301330566406
loss: 3.9225594997406006
epoch: 39, train_loss: 3.774199962615967, train_acc: 71.2, train_fscore: 70.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.922600030899048, test_acc: 67.59, test_fscore: 68.07, time: 7.3 sec
loss: 3.7539210319519043
loss: 3.7475686073303223
loss: 3.900721549987793
epoch: 40, train_loss: 3.750699996948242, train_acc: 72.13, train_fscore: 71.72, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.900700092315674, test_acc: 68.08, test_fscore: 68.5, time: 6.75 sec
              precision    recall  f1-score   support

           0     0.4527    0.6319    0.5275     144.0
           1     0.7490    0.7306    0.7397     245.0
           2     0.6957    0.6250    0.6584     384.0
           3     0.6142    0.7118    0.6594     170.0
           4     0.8664    0.7592    0.8093     299.0
           5     0.6517    0.6483    0.6500     381.0

    accuracy                         0.6808    1623.0
   macro avg     0.6716    0.6845    0.6741    1623.0
weighted avg     0.6948    0.6808    0.6850    1623.0

[[ 91.   9.  13.   3.  27.   1.]
 [  3. 179.  25.   1.   0.  37.]
 [ 45.  27. 240.  22.   4.  46.]
 [  0.   0.   2. 121.   0.  47.]
 [ 61.   1.   9.   0. 227.   1.]
 [  1.  23.  56.  50.   4. 247.]]
loss: 3.711738109588623
loss: 3.7190001010894775
loss: 3.8740036487579346
epoch: 41, train_loss: 3.7151999473571777, train_acc: 72.2, train_fscore: 71.89, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.874000072479248, test_acc: 67.53, test_fscore: 68.09, time: 6.5 sec
loss: 3.754378318786621
loss: 3.6119577884674072
loss: 3.836961269378662
epoch: 42, train_loss: 3.6937999725341797, train_acc: 72.38, train_fscore: 72.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8369998931884766, test_acc: 68.15, test_fscore: 68.45, time: 5.98 sec
loss: 3.758892297744751
loss: 3.5444445610046387
loss: 3.8131799697875977
epoch: 43, train_loss: 3.6542000770568848, train_acc: 72.17, train_fscore: 71.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.813199996948242, test_acc: 68.27, test_fscore: 68.64, time: 6.18 sec
loss: 3.663738489151001
loss: 3.5307202339172363
loss: 3.8175551891326904
epoch: 44, train_loss: 3.605299949645996, train_acc: 72.53, train_fscore: 72.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8176000118255615, test_acc: 68.02, test_fscore: 68.55, time: 5.79 sec
loss: 3.63645601272583
loss: 3.551229238510132
loss: 3.7912209033966064
epoch: 45, train_loss: 3.5947999954223633, train_acc: 73.2, train_fscore: 72.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7911999225616455, test_acc: 68.7, test_fscore: 69.05, time: 6.06 sec
loss: 3.628607749938965
loss: 3.513241767883301
loss: 3.8029041290283203
epoch: 46, train_loss: 3.5725998878479004, train_acc: 73.65, train_fscore: 73.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8029000759124756, test_acc: 67.9, test_fscore: 68.51, time: 6.42 sec
loss: 3.3917999267578125
loss: 3.719938278198242
loss: 3.7494699954986572
epoch: 47, train_loss: 3.553800106048584, train_acc: 73.25, train_fscore: 73.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.749500036239624, test_acc: 68.33, test_fscore: 68.76, time: 6.4 sec
loss: 3.389042377471924
loss: 3.639127731323242
loss: 3.7178823947906494
epoch: 48, train_loss: 3.5058000087738037, train_acc: 73.98, train_fscore: 73.64, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.717900037765503, test_acc: 68.95, test_fscore: 69.36, time: 6.12 sec
loss: 3.292487144470215
loss: 3.6539926528930664
loss: 3.724597215652466
epoch: 49, train_loss: 3.470599889755249, train_acc: 73.65, train_fscore: 73.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.724600076675415, test_acc: 68.88, test_fscore: 69.41, time: 6.3 sec
loss: 3.5915608406066895
loss: 3.2918286323547363
loss: 3.710054874420166
epoch: 50, train_loss: 3.4546000957489014, train_acc: 74.29, train_fscore: 74.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.710099935531616, test_acc: 68.64, test_fscore: 69.04, time: 6.01 sec
              precision    recall  f1-score   support

           0     0.4585    0.7292    0.5630     144.0
           1     0.7605    0.7388    0.7495     245.0
           2     0.7085    0.6328    0.6685     384.0
           3     0.6276    0.7235    0.6721     170.0
           4     0.8963    0.7224    0.8000     299.0
           5     0.6649    0.6562    0.6605     381.0

    accuracy                         0.6888    1623.0
   macro avg     0.6860    0.7005    0.6856    1623.0
weighted avg     0.7100    0.6888    0.6941    1623.0

[[105.   9.  12.   2.  16.   0.]
 [  4. 181.  24.   1.   0.  35.]
 [ 47.  24. 243.  22.   3.  45.]
 [  0.   0.   2. 123.   0.  45.]
 [ 73.   1.   8.   0. 216.   1.]
 [  0.  23.  54.  48.   6. 250.]]
loss: 3.3318827152252197
loss: 3.5446200370788574
loss: 3.6882269382476807
epoch: 51, train_loss: 3.4293999671936035, train_acc: 74.77, train_fscore: 74.5, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.688199996948242, test_acc: 68.52, test_fscore: 68.93, time: 6.7 sec
loss: 3.398371934890747
loss: 3.4022178649902344
loss: 3.675462007522583
epoch: 52, train_loss: 3.4003000259399414, train_acc: 75.11, train_fscore: 74.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.67549991607666, test_acc: 68.27, test_fscore: 68.78, time: 5.62 sec
loss: 3.295865535736084
loss: 3.505262851715088
loss: 3.6239020824432373
epoch: 53, train_loss: 3.390399932861328, train_acc: 74.99, train_fscore: 74.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6238999366760254, test_acc: 69.19, test_fscore: 69.63, time: 6.72 sec
loss: 3.34306263923645
loss: 3.37969970703125
loss: 3.614920139312744
epoch: 54, train_loss: 3.360300064086914, train_acc: 74.65, train_fscore: 74.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6149001121520996, test_acc: 69.13, test_fscore: 69.61, time: 5.92 sec
loss: 3.315167188644409
loss: 3.3460469245910645
loss: 3.613833427429199
epoch: 55, train_loss: 3.3301000595092773, train_acc: 75.58, train_fscore: 75.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.613800048828125, test_acc: 69.13, test_fscore: 69.65, time: 6.56 sec
loss: 3.1886181831359863
loss: 3.4058475494384766
loss: 3.6025469303131104
epoch: 56, train_loss: 3.2946999073028564, train_acc: 75.49, train_fscore: 75.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6024999618530273, test_acc: 69.38, test_fscore: 69.76, time: 6.99 sec
loss: 3.3124263286590576
loss: 3.3189618587493896
loss: 3.5644421577453613
epoch: 57, train_loss: 3.315500020980835, train_acc: 75.54, train_fscore: 75.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5643999576568604, test_acc: 69.62, test_fscore: 70.07, time: 7.04 sec
loss: 3.332167148590088
loss: 3.1542999744415283
loss: 3.551103353500366
epoch: 58, train_loss: 3.2507998943328857, train_acc: 76.49, train_fscore: 76.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.551100015640259, test_acc: 69.38, test_fscore: 69.87, time: 5.68 sec
loss: 3.0692739486694336
loss: 3.45558762550354
loss: 3.542677402496338
epoch: 59, train_loss: 3.251499891281128, train_acc: 75.54, train_fscore: 75.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5427000522613525, test_acc: 69.87, test_fscore: 70.32, time: 6.33 sec
loss: 3.229755401611328
loss: 3.2676820755004883
loss: 3.5254974365234375
epoch: 60, train_loss: 3.2471001148223877, train_acc: 75.9, train_fscore: 75.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5255000591278076, test_acc: 69.25, test_fscore: 69.69, time: 6.62 sec
              precision    recall  f1-score   support

           0     0.4769    0.7153    0.5722     144.0
           1     0.7731    0.7510    0.7619     245.0
           2     0.7123    0.6510    0.6803     384.0
           3     0.6373    0.7235    0.6777     170.0
           4     0.8937    0.7592    0.8210     299.0
           5     0.6658    0.6483    0.6569     381.0

    accuracy                         0.6987    1623.0
   macro avg     0.6932    0.7081    0.6950    1623.0
weighted avg     0.7152    0.6987    0.7032    1623.0

[[103.   9.  14.   0.  18.   0.]
 [  4. 184.  21.   0.   0.  36.]
 [ 47.  22. 250.  20.   3.  42.]
 [  0.   0.   2. 123.   0.  45.]
 [ 62.   1.   8.   0. 227.   1.]
 [  0.  22.  56.  50.   6. 247.]]
loss: 3.1922740936279297
loss: 3.222661018371582
loss: 3.5113155841827393
epoch: 61, train_loss: 3.2070000171661377, train_acc: 76.4, train_fscore: 76.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5113000869750977, test_acc: 69.69, test_fscore: 70.16, time: 4.03 sec
loss: 3.2279458045959473
loss: 3.186042308807373
loss: 3.4907402992248535
epoch: 62, train_loss: 3.2083001136779785, train_acc: 76.64, train_fscore: 76.5, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4907000064849854, test_acc: 69.62, test_fscore: 70.07, time: 6.25 sec
loss: 3.073281764984131
loss: 3.2916061878204346
loss: 3.4755120277404785
epoch: 63, train_loss: 3.174499988555908, train_acc: 76.97, train_fscore: 76.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4755001068115234, test_acc: 69.93, test_fscore: 70.42, time: 7.02 sec
loss: 3.183568000793457
loss: 3.1307566165924072
loss: 3.472752809524536
epoch: 64, train_loss: 3.159899950027466, train_acc: 76.52, train_fscore: 76.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4728000164031982, test_acc: 69.25, test_fscore: 69.74, time: 6.04 sec
loss: 3.1620922088623047
loss: 3.1238203048706055
loss: 3.4787790775299072
epoch: 65, train_loss: 3.1449999809265137, train_acc: 76.45, train_fscore: 76.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.478800058364868, test_acc: 69.56, test_fscore: 70.02, time: 6.45 sec
loss: 3.1386559009552
loss: 3.108391523361206
loss: 3.463331937789917
epoch: 66, train_loss: 3.1261000633239746, train_acc: 76.94, train_fscore: 76.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4632999897003174, test_acc: 69.56, test_fscore: 70.03, time: 5.83 sec
loss: 3.123234987258911
loss: 3.129425048828125
loss: 3.4393081665039062
epoch: 67, train_loss: 3.126199960708618, train_acc: 77.11, train_fscore: 76.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.439300060272217, test_acc: 70.36, test_fscore: 70.81, time: 5.83 sec
loss: 3.194796323776245
loss: 2.975759744644165
loss: 3.432953119277954
epoch: 68, train_loss: 3.0940001010894775, train_acc: 77.37, train_fscore: 77.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.433000087738037, test_acc: 70.06, test_fscore: 70.54, time: 5.42 sec
loss: 3.177532911300659
loss: 2.968552589416504
loss: 3.44606351852417
epoch: 69, train_loss: 3.0820000171661377, train_acc: 77.21, train_fscore: 77.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4460999965667725, test_acc: 69.99, test_fscore: 70.42, time: 6.22 sec
loss: 3.146085262298584
loss: 2.981189250946045
loss: 3.4433794021606445
epoch: 70, train_loss: 3.0689001083374023, train_acc: 77.37, train_fscore: 77.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4433999061584473, test_acc: 70.36, test_fscore: 70.78, time: 7.06 sec
              precision    recall  f1-score   support

           0     0.4840    0.7361    0.5840     144.0
           1     0.7897    0.7510    0.7699     245.0
           2     0.7291    0.6589    0.6922     384.0
           3     0.6425    0.7294    0.6832     170.0
           4     0.8867    0.7592    0.8180     299.0
           5     0.6613    0.6509    0.6561     381.0

    accuracy                         0.7036    1623.0
   macro avg     0.6989    0.7143    0.7006    1623.0
weighted avg     0.7206    0.7036    0.7081    1623.0

[[106.   9.  11.   0.  18.   0.]
 [  3. 184.  20.   0.   0.  38.]
 [ 47.  19. 253.  19.   3.  43.]
 [  0.   0.   1. 124.   0.  45.]
 [ 62.   1.   8.   0. 227.   1.]
 [  1.  20.  54.  50.   8. 248.]]
loss: 3.1144821643829346
loss: 2.9740376472473145
loss: 3.4317281246185303
epoch: 71, train_loss: 3.0487000942230225, train_acc: 77.49, train_fscore: 77.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4316999912261963, test_acc: 69.93, test_fscore: 70.39, time: 5.85 sec
loss: 3.0527098178863525
loss: 3.0372097492218018
loss: 3.4258787631988525
epoch: 72, train_loss: 3.0455000400543213, train_acc: 77.61, train_fscore: 77.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4258999824523926, test_acc: 70.43, test_fscore: 70.88, time: 6.47 sec
loss: 3.242349624633789
loss: 2.7297065258026123
loss: 3.4377756118774414
epoch: 73, train_loss: 3.0195999145507812, train_acc: 77.88, train_fscore: 77.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4377999305725098, test_acc: 70.55, test_fscore: 70.98, time: 7.25 sec
loss: 3.065276861190796
loss: 2.9260566234588623
loss: 3.4225668907165527
epoch: 74, train_loss: 3.0037999153137207, train_acc: 78.57, train_fscore: 78.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.422600030899048, test_acc: 69.99, test_fscore: 70.43, time: 5.38 sec
loss: 2.9682013988494873
loss: 3.04929518699646
loss: 3.3931703567504883
epoch: 75, train_loss: 3.0078001022338867, train_acc: 78.47, train_fscore: 78.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.393199920654297, test_acc: 70.3, test_fscore: 70.74, time: 6.39 sec
loss: 3.177553653717041
loss: 2.7592098712921143
loss: 3.408379077911377
epoch: 76, train_loss: 2.990999937057495, train_acc: 78.02, train_fscore: 77.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.408400058746338, test_acc: 69.93, test_fscore: 70.39, time: 6.25 sec
loss: 3.019789934158325
loss: 2.9322190284729004
loss: 3.4043691158294678
epoch: 77, train_loss: 2.9788999557495117, train_acc: 78.57, train_fscore: 78.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.404400110244751, test_acc: 70.36, test_fscore: 70.82, time: 6.94 sec
loss: 2.8388590812683105
loss: 3.1079936027526855
loss: 3.404794216156006
epoch: 78, train_loss: 2.9642999172210693, train_acc: 78.45, train_fscore: 78.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4047999382019043, test_acc: 70.43, test_fscore: 70.91, time: 6.24 sec
loss: 2.993607759475708
loss: 2.8784143924713135
loss: 3.4164886474609375
epoch: 79, train_loss: 2.939199924468994, train_acc: 78.64, train_fscore: 78.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4165000915527344, test_acc: 70.36, test_fscore: 70.85, time: 5.73 sec
loss: 2.8297536373138428
loss: 3.0828073024749756
loss: 3.3913257122039795
epoch: 80, train_loss: 2.9498000144958496, train_acc: 78.97, train_fscore: 78.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3912999629974365, test_acc: 70.36, test_fscore: 70.81, time: 6.03 sec
              precision    recall  f1-score   support

           0     0.4930    0.7361    0.5905     144.0
           1     0.7974    0.7551    0.7757     245.0
           2     0.7246    0.6510    0.6859     384.0
           3     0.6392    0.7294    0.6813     170.0
           4     0.8941    0.7625    0.8231     299.0
           5     0.6597    0.6614    0.6606     381.0

    accuracy                         0.7055    1623.0
   macro avg     0.7013    0.7159    0.7028    1623.0
weighted avg     0.7221    0.7055    0.7098    1623.0

[[106.   9.  11.   0.  18.   0.]
 [  2. 185.  20.   0.   0.  38.]
 [ 47.  19. 250.  19.   3.  46.]
 [  0.   0.   1. 124.   0.  45.]
 [ 59.   2.   9.   0. 228.   1.]
 [  1.  17.  54.  51.   6. 252.]]
loss: 2.971682548522949
loss: 2.8459434509277344
loss: 3.371584415435791
epoch: 81, train_loss: 2.9105000495910645, train_acc: 79.14, train_fscore: 79.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3715999126434326, test_acc: 70.61, test_fscore: 71.09, time: 5.83 sec
loss: 3.047555446624756
loss: 2.7229251861572266
loss: 3.370485305786133
epoch: 82, train_loss: 2.9028000831604004, train_acc: 78.92, train_fscore: 78.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.370500087738037, test_acc: 70.86, test_fscore: 71.35, time: 5.46 sec
loss: 2.7148032188415527
loss: 3.103309154510498
loss: 3.3579535484313965
epoch: 83, train_loss: 2.896399974822998, train_acc: 79.45, train_fscore: 79.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3580000400543213, test_acc: 70.79, test_fscore: 71.23, time: 6.66 sec
loss: 2.966003656387329
loss: 2.778463840484619
loss: 3.3631722927093506
epoch: 84, train_loss: 2.8814001083374023, train_acc: 79.69, train_fscore: 79.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3631999492645264, test_acc: 71.29, test_fscore: 71.66, time: 7.15 sec
loss: 2.990734577178955
loss: 2.767655849456787
loss: 3.3699028491973877
epoch: 85, train_loss: 2.887200117111206, train_acc: 79.43, train_fscore: 79.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3698999881744385, test_acc: 70.86, test_fscore: 71.31, time: 5.83 sec
loss: 2.6732289791107178
loss: 3.1014459133148193
loss: 3.3743152618408203
epoch: 86, train_loss: 2.8724000453948975, train_acc: 79.43, train_fscore: 79.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.374300003051758, test_acc: 70.86, test_fscore: 71.32, time: 6.83 sec
loss: 2.849069595336914
loss: 2.846553325653076
loss: 3.3545310497283936
epoch: 87, train_loss: 2.847899913787842, train_acc: 80.03, train_fscore: 79.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3545000553131104, test_acc: 70.86, test_fscore: 71.27, time: 7.07 sec
loss: 2.7144923210144043
loss: 3.013848304748535
loss: 3.34564471244812
epoch: 88, train_loss: 2.8492000102996826, train_acc: 80.1, train_fscore: 80.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.345599889755249, test_acc: 71.1, test_fscore: 71.46, time: 5.67 sec
loss: 2.665463924407959
loss: 3.042515754699707
loss: 3.337484359741211
epoch: 89, train_loss: 2.8431999683380127, train_acc: 80.12, train_fscore: 80.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3375000953674316, test_acc: 71.1, test_fscore: 71.46, time: 6.39 sec
loss: 2.8687539100646973
loss: 2.7324647903442383
loss: 3.3420603275299072
epoch: 90, train_loss: 2.808199882507324, train_acc: 80.09, train_fscore: 79.94, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.342099905014038, test_acc: 71.41, test_fscore: 71.85, time: 6.27 sec
              precision    recall  f1-score   support

           0     0.5022    0.7847    0.6125     144.0
           1     0.8025    0.7796    0.7909     245.0
           2     0.7590    0.6562    0.7039     384.0
           3     0.6402    0.7118    0.6741     170.0
           4     0.8956    0.7458    0.8139     299.0
           5     0.6641    0.6798    0.6719     381.0

    accuracy                         0.7141    1623.0
   macro avg     0.7106    0.7263    0.7112    1623.0
weighted avg     0.7332    0.7141    0.7185    1623.0

[[113.   8.   7.   0.  16.   0.]
 [  3. 191.  15.   0.   0.  36.]
 [ 45.  21. 252.  17.   3.  46.]
 [  0.   0.   1. 121.   0.  48.]
 [ 64.   2.   9.   0. 223.   1.]
 [  0.  16.  48.  51.   7. 259.]]
loss: 2.753319025039673
loss: 2.870133399963379
loss: 3.345701217651367
epoch: 91, train_loss: 2.8096001148223877, train_acc: 80.0, train_fscore: 79.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3457000255584717, test_acc: 71.29, test_fscore: 71.68, time: 6.24 sec
loss: 2.8926758766174316
loss: 2.68721604347229
loss: 3.3397135734558105
epoch: 92, train_loss: 2.799999952316284, train_acc: 80.59, train_fscore: 80.49, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3396999835968018, test_acc: 71.1, test_fscore: 71.47, time: 6.11 sec
loss: 2.688279390335083
loss: 2.9496214389801025
loss: 3.3149030208587646
epoch: 93, train_loss: 2.803800106048584, train_acc: 80.22, train_fscore: 80.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3148999214172363, test_acc: 71.04, test_fscore: 71.41, time: 6.45 sec
loss: 2.6582953929901123
loss: 2.8491079807281494
loss: 3.346024990081787
epoch: 94, train_loss: 2.7506000995635986, train_acc: 80.69, train_fscore: 80.57, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3459999561309814, test_acc: 70.43, test_fscore: 70.81, time: 7.0 sec
loss: 2.9351468086242676
loss: 2.5694210529327393
loss: 3.3554677963256836
epoch: 95, train_loss: 2.770699977874756, train_acc: 80.86, train_fscore: 80.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3554999828338623, test_acc: 70.61, test_fscore: 70.99, time: 5.55 sec
loss: 2.8018860816955566
loss: 2.6902756690979004
loss: 3.321796417236328
epoch: 96, train_loss: 2.7500998973846436, train_acc: 80.65, train_fscore: 80.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3217999935150146, test_acc: 71.53, test_fscore: 71.92, time: 6.79 sec
loss: 2.653000593185425
loss: 2.855520009994507
loss: 3.305467128753662
epoch: 97, train_loss: 2.7500998973846436, train_acc: 80.64, train_fscore: 80.51, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.305500030517578, test_acc: 71.53, test_fscore: 71.94, time: 6.3 sec
loss: 2.6779074668884277
loss: 2.7529420852661133
loss: 3.3196096420288086
epoch: 98, train_loss: 2.7123000621795654, train_acc: 80.91, train_fscore: 80.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3196001052856445, test_acc: 71.41, test_fscore: 71.8, time: 6.32 sec
loss: 2.68214750289917
loss: 2.7417452335357666
loss: 3.3167338371276855
epoch: 99, train_loss: 2.709199905395508, train_acc: 81.03, train_fscore: 80.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.316699981689453, test_acc: 71.6, test_fscore: 71.96, time: 6.0 sec
loss: 2.7681546211242676
loss: 2.650210380554199
loss: 3.3279104232788086
epoch: 100, train_loss: 2.714099884033203, train_acc: 80.81, train_fscore: 80.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.327899932861328, test_acc: 70.79, test_fscore: 71.2, time: 6.5 sec
              precision    recall  f1-score   support

           0     0.5253    0.7917    0.6316     144.0
           1     0.8112    0.7714    0.7908     245.0
           2     0.7404    0.6536    0.6943     384.0
           3     0.6524    0.7176    0.6835     170.0
           4     0.8945    0.7659    0.8252     299.0
           5     0.6573    0.6745    0.6658     381.0

    accuracy                         0.7160    1623.0
   macro avg     0.7135    0.7291    0.7152    1623.0
weighted avg     0.7317    0.7160    0.7196    1623.0

[[114.   6.   7.   0.  17.   0.]
 [  3. 189.  15.   0.   0.  38.]
 [ 44.  20. 251.  17.   4.  48.]
 [  0.   0.   1. 122.   0.  47.]
 [ 55.   2.  12.   0. 229.   1.]
 [  1.  16.  53.  48.   6. 257.]]
loss: 2.741619110107422
loss: 2.6496481895446777
loss: 3.351552724838257
epoch: 101, train_loss: 2.7014000415802, train_acc: 81.02, train_fscore: 80.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.351599931716919, test_acc: 71.23, test_fscore: 71.59, time: 6.76 sec
loss: 2.7141847610473633
loss: 2.6610465049743652
loss: 3.304203748703003
epoch: 102, train_loss: 2.690700054168701, train_acc: 80.79, train_fscore: 80.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3041999340057373, test_acc: 71.47, test_fscore: 71.84, time: 5.89 sec
loss: 2.657203197479248
loss: 2.6983108520507812
loss: 3.3069162368774414
epoch: 103, train_loss: 2.676500082015991, train_acc: 80.98, train_fscore: 80.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3069000244140625, test_acc: 71.41, test_fscore: 71.78, time: 6.28 sec
loss: 2.660484790802002
loss: 2.691556930541992
loss: 3.3139102458953857
epoch: 104, train_loss: 2.675299882888794, train_acc: 82.17, train_fscore: 82.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3138999938964844, test_acc: 71.66, test_fscore: 72.03, time: 6.01 sec
loss: 2.7785041332244873
loss: 2.5211148262023926
loss: 3.31870174407959
epoch: 105, train_loss: 2.6528000831604004, train_acc: 81.96, train_fscore: 81.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.318700075149536, test_acc: 71.53, test_fscore: 71.9, time: 6.09 sec
loss: 2.697350263595581
loss: 2.6002037525177
loss: 3.3267130851745605
epoch: 106, train_loss: 2.651700019836426, train_acc: 82.01, train_fscore: 81.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.32669997215271, test_acc: 71.78, test_fscore: 72.14, time: 6.17 sec
loss: 2.4308719635009766
loss: 2.863320827484131
loss: 3.322035789489746
epoch: 107, train_loss: 2.6387999057769775, train_acc: 81.84, train_fscore: 81.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.322000026702881, test_acc: 71.78, test_fscore: 72.14, time: 6.09 sec
loss: 2.6418113708496094
loss: 2.626894950866699
loss: 3.3238632678985596
epoch: 108, train_loss: 2.634999990463257, train_acc: 81.84, train_fscore: 81.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.323899984359741, test_acc: 71.6, test_fscore: 71.94, time: 6.56 sec
loss: 2.6779513359069824
loss: 2.595177412033081
loss: 3.319417953491211
epoch: 109, train_loss: 2.6380999088287354, train_acc: 82.08, train_fscore: 81.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3194000720977783, test_acc: 71.47, test_fscore: 71.83, time: 5.44 sec
loss: 2.668107509613037
loss: 2.5509705543518066
loss: 3.3081789016723633
epoch: 110, train_loss: 2.6120998859405518, train_acc: 82.22, train_fscore: 82.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.308199882507324, test_acc: 71.53, test_fscore: 71.97, time: 6.07 sec
              precision    recall  f1-score   support

           0     0.5231    0.7847    0.6278     144.0
           1     0.8182    0.7714    0.7941     245.0
           2     0.7358    0.6745    0.7038     384.0
           3     0.6447    0.7471    0.6921     170.0
           4     0.8867    0.7592    0.8180     299.0
           5     0.6739    0.6562    0.6649     381.0

    accuracy                         0.7178    1623.0
   macro avg     0.7137    0.7322    0.7168    1623.0
weighted avg     0.7331    0.7178    0.7214    1623.0

[[113.   6.   7.   0.  18.   0.]
 [  3. 189.  15.   1.   0.  37.]
 [ 42.  19. 259.  18.   5.  41.]
 [  0.   0.   1. 127.   0.  42.]
 [ 57.   2.  12.   0. 227.   1.]
 [  1.  15.  58.  51.   6. 250.]]
loss: 2.603017568588257
loss: 2.597348213195801
loss: 3.2965340614318848
epoch: 111, train_loss: 2.600399971008301, train_acc: 82.08, train_fscore: 81.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2964999675750732, test_acc: 72.27, test_fscore: 72.6, time: 5.68 sec
loss: 2.609023332595825
loss: 2.5770890712738037
loss: 3.305968761444092
epoch: 112, train_loss: 2.5945000648498535, train_acc: 82.12, train_fscore: 81.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.305999994277954, test_acc: 71.66, test_fscore: 72.01, time: 6.57 sec
loss: 2.5367395877838135
loss: 2.645415782928467
loss: 3.3077504634857178
epoch: 113, train_loss: 2.5882999897003174, train_acc: 82.41, train_fscore: 82.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.307800054550171, test_acc: 71.6, test_fscore: 71.92, time: 6.4 sec
loss: 2.703434944152832
loss: 2.440082550048828
loss: 3.3022942543029785
epoch: 114, train_loss: 2.582900047302246, train_acc: 82.39, train_fscore: 82.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.302299976348877, test_acc: 71.78, test_fscore: 72.13, time: 6.42 sec
loss: 2.5945489406585693
loss: 2.5370540618896484
loss: 3.299466133117676
epoch: 115, train_loss: 2.5666000843048096, train_acc: 82.56, train_fscore: 82.47, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.299499988555908, test_acc: 71.6, test_fscore: 71.96, time: 5.58 sec
loss: 2.503216505050659
loss: 2.635850429534912
loss: 3.2981197834014893
epoch: 116, train_loss: 2.5648000240325928, train_acc: 82.77, train_fscore: 82.7, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.298099994659424, test_acc: 71.53, test_fscore: 71.9, time: 6.46 sec
loss: 2.550825834274292
loss: 2.5294032096862793
loss: 3.3123421669006348
epoch: 117, train_loss: 2.5409998893737793, train_acc: 82.82, train_fscore: 82.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.312299966812134, test_acc: 71.84, test_fscore: 72.2, time: 6.83 sec
loss: 2.459127902984619
loss: 2.654473304748535
loss: 3.297447919845581
epoch: 118, train_loss: 2.5464000701904297, train_acc: 83.1, train_fscore: 83.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2973999977111816, test_acc: 71.47, test_fscore: 71.83, time: 6.41 sec
loss: 2.5297741889953613
loss: 2.5407721996307373
loss: 3.330016613006592
epoch: 119, train_loss: 2.5350000858306885, train_acc: 82.98, train_fscore: 82.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3299999237060547, test_acc: 71.47, test_fscore: 71.79, time: 6.11 sec
loss: 2.3993492126464844
loss: 2.6656064987182617
loss: 3.3335156440734863
epoch: 120, train_loss: 2.5216000080108643, train_acc: 83.15, train_fscore: 83.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3334999084472656, test_acc: 71.72, test_fscore: 72.09, time: 6.34 sec
              precision    recall  f1-score   support

           0     0.5352    0.7917    0.6387     144.0
           1     0.8261    0.7755    0.8000     245.0
           2     0.7457    0.6719    0.7068     384.0
           3     0.6337    0.7529    0.6882     170.0
           4     0.8864    0.7826    0.8313     299.0
           5     0.6766    0.6535    0.6649     381.0

    accuracy                         0.7227    1623.0
   macro avg     0.7173    0.7380    0.7216    1623.0
weighted avg     0.7371    0.7227    0.7260    1623.0

[[114.   5.   7.   0.  18.   0.]
 [  3. 190.  15.   1.   0.  36.]
 [ 43.  19. 258.  18.   5.  41.]
 [  0.   0.   1. 128.   0.  41.]
 [ 52.   2.  10.   0. 234.   1.]
 [  1.  14.  55.  55.   7. 249.]]
loss: 2.4531350135803223
loss: 2.58829927444458
loss: 3.3037922382354736
epoch: 121, train_loss: 2.5137999057769775, train_acc: 82.65, train_fscore: 82.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.303800106048584, test_acc: 71.41, test_fscore: 71.78, time: 6.24 sec
loss: 2.5908703804016113
loss: 2.4245800971984863
loss: 3.3186869621276855
epoch: 122, train_loss: 2.5114998817443848, train_acc: 83.22, train_fscore: 83.12, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.318700075149536, test_acc: 71.9, test_fscore: 72.26, time: 6.58 sec
loss: 2.4111976623535156
loss: 2.5753138065338135
loss: 3.2834272384643555
epoch: 123, train_loss: 2.4930999279022217, train_acc: 83.06, train_fscore: 82.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.283400058746338, test_acc: 71.84, test_fscore: 72.15, time: 6.71 sec
loss: 2.5653696060180664
loss: 2.4210124015808105
loss: 3.3260107040405273
epoch: 124, train_loss: 2.5000998973846436, train_acc: 83.75, train_fscore: 83.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3259999752044678, test_acc: 71.6, test_fscore: 71.93, time: 6.15 sec
loss: 2.4324986934661865
loss: 2.537501573562622
loss: 3.3436830043792725
epoch: 125, train_loss: 2.4830000400543213, train_acc: 83.65, train_fscore: 83.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3436999320983887, test_acc: 71.66, test_fscore: 72.0, time: 6.64 sec
loss: 2.5578982830047607
loss: 2.3650436401367188
loss: 3.2889037132263184
epoch: 126, train_loss: 2.4700000286102295, train_acc: 83.1, train_fscore: 82.97, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2888998985290527, test_acc: 71.84, test_fscore: 72.19, time: 5.91 sec
loss: 2.41995906829834
loss: 2.531630754470825
loss: 3.2989931106567383
epoch: 127, train_loss: 2.4725000858306885, train_acc: 83.7, train_fscore: 83.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2990000247955322, test_acc: 71.53, test_fscore: 71.94, time: 6.17 sec
loss: 2.3833696842193604
loss: 2.530327796936035
loss: 3.312680721282959
epoch: 128, train_loss: 2.4511001110076904, train_acc: 83.75, train_fscore: 83.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.312700033187866, test_acc: 71.66, test_fscore: 72.02, time: 6.24 sec
loss: 2.4761414527893066
loss: 2.4261865615844727
loss: 3.3297858238220215
epoch: 129, train_loss: 2.4519999027252197, train_acc: 83.73, train_fscore: 83.64, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3297998905181885, test_acc: 71.29, test_fscore: 71.66, time: 6.65 sec
loss: 2.274620294570923
loss: 2.5927586555480957
loss: 3.3278591632843018
epoch: 130, train_loss: 2.4316999912261963, train_acc: 83.96, train_fscore: 83.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.327899932861328, test_acc: 71.9, test_fscore: 72.25, time: 4.8 sec
              precision    recall  f1-score   support

           0     0.5352    0.7917    0.6387     144.0
           1     0.8261    0.7755    0.8000     245.0
           2     0.7457    0.6719    0.7068     384.0
           3     0.6337    0.7529    0.6882     170.0
           4     0.8864    0.7826    0.8313     299.0
           5     0.6766    0.6535    0.6649     381.0

    accuracy                         0.7227    1623.0
   macro avg     0.7173    0.7380    0.7216    1623.0
weighted avg     0.7371    0.7227    0.7260    1623.0

[[114.   5.   7.   0.  18.   0.]
 [  3. 190.  15.   1.   0.  36.]
 [ 43.  19. 258.  18.   5.  41.]
 [  0.   0.   1. 128.   0.  41.]
 [ 52.   2.  10.   0. 234.   1.]
 [  1.  14.  55.  55.   7. 249.]]
loss: 2.4523329734802246
loss: 2.4666695594787598
loss: 3.2871766090393066
epoch: 131, train_loss: 2.459199905395508, train_acc: 83.67, train_fscore: 83.57, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2871999740600586, test_acc: 71.84, test_fscore: 72.18, time: 1.75 sec
loss: 2.487996816635132
loss: 2.361988067626953
loss: 3.330496311187744
epoch: 132, train_loss: 2.4296998977661133, train_acc: 83.79, train_fscore: 83.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3304998874664307, test_acc: 71.6, test_fscore: 72.0, time: 2.62 sec
loss: 2.454599380493164
loss: 2.396883726119995
loss: 3.310659885406494
epoch: 133, train_loss: 2.4279000759124756, train_acc: 83.6, train_fscore: 83.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.310699939727783, test_acc: 71.53, test_fscore: 71.89, time: 4.81 sec
loss: 2.3466885089874268
loss: 2.4576122760772705
loss: 3.308554172515869
epoch: 134, train_loss: 2.3980000019073486, train_acc: 84.04, train_fscore: 83.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3085999488830566, test_acc: 71.66, test_fscore: 71.99, time: 4.5 sec
loss: 2.4954729080200195
loss: 2.31012225151062
loss: 3.328372001647949
epoch: 135, train_loss: 2.4112000465393066, train_acc: 84.27, train_fscore: 84.16, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.328399896621704, test_acc: 71.53, test_fscore: 71.87, time: 4.57 sec
loss: 2.2684969902038574
loss: 2.5476057529449463
loss: 3.308504581451416
epoch: 136, train_loss: 2.4042999744415283, train_acc: 84.58, train_fscore: 84.5, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.308500051498413, test_acc: 71.35, test_fscore: 71.73, time: 6.4 sec
loss: 2.445211410522461
loss: 2.33362078666687
loss: 3.320345878601074
epoch: 137, train_loss: 2.3921000957489014, train_acc: 84.44, train_fscore: 84.37, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3203001022338867, test_acc: 71.16, test_fscore: 71.56, time: 5.79 sec
loss: 2.3589038848876953
loss: 2.411738872528076
loss: 3.3373234272003174
epoch: 138, train_loss: 2.384000062942505, train_acc: 84.32, train_fscore: 84.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3373000621795654, test_acc: 70.98, test_fscore: 71.32, time: 6.64 sec
loss: 2.318579912185669
loss: 2.418424606323242
loss: 3.32151460647583
epoch: 139, train_loss: 2.365799903869629, train_acc: 84.39, train_fscore: 84.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.321500062942505, test_acc: 71.16, test_fscore: 71.55, time: 5.81 sec
loss: 2.3721985816955566
loss: 2.3639044761657715
loss: 3.307302236557007
epoch: 140, train_loss: 2.3684000968933105, train_acc: 84.22, train_fscore: 84.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.307300090789795, test_acc: 71.04, test_fscore: 71.46, time: 6.95 sec
              precision    recall  f1-score   support

           0     0.5352    0.7917    0.6387     144.0
           1     0.8261    0.7755    0.8000     245.0
           2     0.7457    0.6719    0.7068     384.0
           3     0.6337    0.7529    0.6882     170.0
           4     0.8864    0.7826    0.8313     299.0
           5     0.6766    0.6535    0.6649     381.0

    accuracy                         0.7227    1623.0
   macro avg     0.7173    0.7380    0.7216    1623.0
weighted avg     0.7371    0.7227    0.7260    1623.0

[[114.   5.   7.   0.  18.   0.]
 [  3. 190.  15.   1.   0.  36.]
 [ 43.  19. 258.  18.   5.  41.]
 [  0.   0.   1. 128.   0.  41.]
 [ 52.   2.  10.   0. 234.   1.]
 [  1.  14.  55.  55.   7. 249.]]
loss: 2.466053009033203
loss: 2.248725652694702
loss: 3.302917718887329
epoch: 141, train_loss: 2.3668999671936035, train_acc: 84.46, train_fscore: 84.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3029000759124756, test_acc: 71.35, test_fscore: 71.74, time: 6.65 sec
loss: 2.406214475631714
loss: 2.3274447917938232
loss: 3.339031219482422
epoch: 142, train_loss: 2.371000051498413, train_acc: 84.46, train_fscore: 84.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3389999866485596, test_acc: 72.15, test_fscore: 72.47, time: 6.24 sec
loss: 2.3030433654785156
loss: 2.4253525733947754
loss: 3.31072998046875
epoch: 143, train_loss: 2.3587000370025635, train_acc: 84.65, train_fscore: 84.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.310699939727783, test_acc: 72.03, test_fscore: 72.36, time: 5.28 sec
loss: 2.2808918952941895
loss: 2.368901014328003
loss: 3.2950782775878906
epoch: 144, train_loss: 2.322999954223633, train_acc: 84.91, train_fscore: 84.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.295099973678589, test_acc: 71.6, test_fscore: 71.98, time: 6.58 sec
loss: 2.3148915767669678
loss: 2.3419883251190186
loss: 3.3147034645080566
epoch: 145, train_loss: 2.3278000354766846, train_acc: 85.08, train_fscore: 84.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.31469988822937, test_acc: 71.97, test_fscore: 72.29, time: 6.21 sec
loss: 2.264397621154785
loss: 2.3681528568267822
loss: 3.3355631828308105
epoch: 146, train_loss: 2.312299966812134, train_acc: 84.97, train_fscore: 84.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.335599899291992, test_acc: 71.04, test_fscore: 71.45, time: 5.87 sec
loss: 2.3869190216064453
loss: 2.2588419914245605
loss: 3.346932888031006
epoch: 147, train_loss: 2.325500011444092, train_acc: 84.84, train_fscore: 84.75, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.34689998626709, test_acc: 71.72, test_fscore: 72.11, time: 6.6 sec
loss: 2.321951150894165
loss: 2.3170406818389893
loss: 3.335747003555298
epoch: 148, train_loss: 2.3196001052856445, train_acc: 85.15, train_fscore: 85.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.335700035095215, test_acc: 71.66, test_fscore: 71.97, time: 6.13 sec
loss: 2.2435336112976074
loss: 2.4038069248199463
loss: 3.3523361682891846
epoch: 149, train_loss: 2.3192999362945557, train_acc: 84.96, train_fscore: 84.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.352299928665161, test_acc: 70.98, test_fscore: 71.43, time: 6.47 sec
loss: 2.3811299800872803
loss: 2.2485501766204834
loss: 3.3477635383605957
epoch: 150, train_loss: 2.3185999393463135, train_acc: 85.15, train_fscore: 85.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3478000164031982, test_acc: 71.1, test_fscore: 71.46, time: 6.22 sec
              precision    recall  f1-score   support

           0     0.5352    0.7917    0.6387     144.0
           1     0.8261    0.7755    0.8000     245.0
           2     0.7457    0.6719    0.7068     384.0
           3     0.6337    0.7529    0.6882     170.0
           4     0.8864    0.7826    0.8313     299.0
           5     0.6766    0.6535    0.6649     381.0

    accuracy                         0.7227    1623.0
   macro avg     0.7173    0.7380    0.7216    1623.0
weighted avg     0.7371    0.7227    0.7260    1623.0

[[114.   5.   7.   0.  18.   0.]
 [  3. 190.  15.   1.   0.  36.]
 [ 43.  19. 258.  18.   5.  41.]
 [  0.   0.   1. 128.   0.  41.]
 [ 52.   2.  10.   0. 234.   1.]
 [  1.  14.  55.  55.   7. 249.]]
Test performance..
F-Score: 72.6
F-Score-index: 111
              precision    recall  f1-score   support

           0     0.5352    0.7917    0.6387     144.0
           1     0.8261    0.7755    0.8000     245.0
           2     0.7457    0.6719    0.7068     384.0
           3     0.6337    0.7529    0.6882     170.0
           4     0.8864    0.7826    0.8313     299.0
           5     0.6766    0.6535    0.6649     381.0

    accuracy                         0.7227    1623.0
   macro avg     0.7173    0.7380    0.7216    1623.0
weighted avg     0.7371    0.7227    0.7260    1623.0

[[114.   5.   7.   0.  18.   0.]
 [  3. 190.  15.   1.   0.  36.]
 [ 43.  19. 258.  18.   5.  41.]
 [  0.   0.   1. 128.   0.  41.]
 [ 52.   2.  10.   0. 234.   1.]
 [  1.  14.  55.  55.   7. 249.]]
--- 7 ---
Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.5, batch_size=64, hidden_dim=1024, n_head=8, epochs=150, temp=2, tensorboard=False, class_weight=True, Dataset='IEMOCAP')
Running on GPU
temp 2
total parameters: 79687704
training parameters: 79687704
loss: 8.018653869628906
loss: 10.896413803100586
loss: 7.971586227416992
epoch: 1, train_loss: 9.423399925231934, train_acc: 19.09, train_fscore: 16.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.97160005569458, test_acc: 25.02, test_fscore: 16.92, time: 6.96 sec
loss: 8.14654541015625
loss: 8.577951431274414
loss: 8.290260314941406
epoch: 2, train_loss: 8.345800399780273, train_acc: 19.54, train_fscore: 16.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.290300369262695, test_acc: 23.17, test_fscore: 21.56, time: 6.42 sec
loss: 8.423197746276855
loss: 7.856412887573242
loss: 6.941154956817627
epoch: 3, train_loss: 8.152700424194336, train_acc: 27.01, train_fscore: 24.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.941199779510498, test_acc: 32.41, test_fscore: 28.97, time: 6.88 sec
loss: 7.310808181762695
loss: 7.186525821685791
loss: 7.190695285797119
epoch: 4, train_loss: 7.252099990844727, train_acc: 40.46, train_fscore: 38.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.190700054168701, test_acc: 34.5, test_fscore: 28.71, time: 5.63 sec
loss: 7.351795673370361
loss: 7.329984188079834
loss: 6.854850769042969
epoch: 5, train_loss: 7.341899871826172, train_acc: 40.77, train_fscore: 36.72, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.854899883270264, test_acc: 42.64, test_fscore: 41.03, time: 6.95 sec
loss: 6.97997522354126
loss: 6.743159294128418
loss: 6.591065883636475
epoch: 6, train_loss: 6.871399879455566, train_acc: 44.82, train_fscore: 41.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.591100215911865, test_acc: 49.85, test_fscore: 44.34, time: 5.99 sec
loss: 6.64702844619751
loss: 6.637392044067383
loss: 6.636298656463623
epoch: 7, train_loss: 6.642499923706055, train_acc: 50.26, train_fscore: 44.49, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.636300086975098, test_acc: 49.54, test_fscore: 44.58, time: 6.92 sec
loss: 6.711570739746094
loss: 6.5031962394714355
loss: 6.4966301918029785
epoch: 8, train_loss: 6.6153998374938965, train_acc: 52.1, train_fscore: 47.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.496600151062012, test_acc: 48.49, test_fscore: 46.67, time: 6.33 sec
loss: 6.412952423095703
loss: 6.339105606079102
loss: 6.243443489074707
epoch: 9, train_loss: 6.38040018081665, train_acc: 56.94, train_fscore: 55.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.2434000968933105, test_acc: 53.48, test_fscore: 52.81, time: 6.53 sec
loss: 6.147315502166748
loss: 6.132174968719482
loss: 6.13236141204834
epoch: 10, train_loss: 6.139999866485596, train_acc: 58.97, train_fscore: 56.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.132400035858154, test_acc: 53.05, test_fscore: 52.24, time: 6.84 sec
              precision    recall  f1-score   support

           0     0.2835    0.2500    0.2657     144.0
           1     0.6713    0.7918    0.7266     245.0
           2     0.4420    0.5156    0.4760     384.0
           3     0.4691    0.7588    0.5798     170.0
           4     0.6589    0.5686    0.6104     299.0
           5     0.6239    0.3701    0.4646     381.0

    accuracy                         0.5348    1623.0
   macro avg     0.5248    0.5425    0.5205    1623.0
weighted avg     0.5480    0.5348    0.5281    1623.0

[[ 36.  14.  28.   8.  57.   1.]
 [  5. 194.  27.   4.   1.  14.]
 [ 55.  48. 198.  19.  18.  46.]
 [  0.   2.  19. 129.   4.  16.]
 [ 25.   6.  76.  14. 170.   8.]
 [  6.  25. 100. 101.   8. 141.]]
loss: 6.078711032867432
loss: 5.937177658081055
loss: 5.967428684234619
epoch: 11, train_loss: 6.014100074768066, train_acc: 57.33, train_fscore: 54.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.967400074005127, test_acc: 56.07, test_fscore: 55.83, time: 6.57 sec
loss: 5.905391693115234
loss: 5.687694072723389
loss: 5.677322864532471
epoch: 12, train_loss: 5.808000087738037, train_acc: 59.02, train_fscore: 57.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.677299976348877, test_acc: 57.98, test_fscore: 57.6, time: 6.16 sec
loss: 5.642459869384766
loss: 5.371245861053467
loss: 5.449822425842285
epoch: 13, train_loss: 5.516600131988525, train_acc: 62.55, train_fscore: 61.15, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.44980001449585, test_acc: 59.64, test_fscore: 59.3, time: 6.99 sec
loss: 5.240986347198486
loss: 5.464458465576172
loss: 5.326200008392334
epoch: 14, train_loss: 5.3460001945495605, train_acc: 62.91, train_fscore: 61.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.326200008392334, test_acc: 57.86, test_fscore: 57.4, time: 7.1 sec
loss: 5.282421588897705
loss: 5.130553245544434
loss: 5.1640214920043945
epoch: 15, train_loss: 5.208399772644043, train_acc: 62.91, train_fscore: 61.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.164000034332275, test_acc: 59.77, test_fscore: 59.51, time: 6.49 sec
loss: 4.952304363250732
loss: 5.216919422149658
loss: 5.0349273681640625
epoch: 16, train_loss: 5.088799953460693, train_acc: 63.48, train_fscore: 61.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.034900188446045, test_acc: 61.18, test_fscore: 61.42, time: 5.92 sec
loss: 5.032576560974121
loss: 4.885369300842285
loss: 4.942256450653076
epoch: 17, train_loss: 4.9695000648498535, train_acc: 63.96, train_fscore: 62.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.942299842834473, test_acc: 62.66, test_fscore: 62.78, time: 6.29 sec
loss: 4.910030364990234
loss: 4.782527446746826
loss: 4.781895160675049
epoch: 18, train_loss: 4.850599765777588, train_acc: 64.34, train_fscore: 63.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.781899929046631, test_acc: 64.2, test_fscore: 64.4, time: 5.44 sec
loss: 4.736185550689697
loss: 4.737820148468018
loss: 4.687477111816406
epoch: 19, train_loss: 4.736999988555908, train_acc: 65.4, train_fscore: 64.49, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.6875, test_acc: 63.71, test_fscore: 64.54, time: 6.36 sec
loss: 4.549490928649902
loss: 4.804081916809082
loss: 4.6172075271606445
epoch: 20, train_loss: 4.662899971008301, train_acc: 65.9, train_fscore: 65.37, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.617199897766113, test_acc: 64.08, test_fscore: 64.63, time: 6.72 sec
              precision    recall  f1-score   support

           0     0.3463    0.4931    0.4069     144.0
           1     0.7457    0.7061    0.7254     245.0
           2     0.6607    0.5781    0.6167     384.0
           3     0.5683    0.7588    0.6499     170.0
           4     0.8077    0.7023    0.7513     299.0
           5     0.6474    0.6168    0.6317     381.0

    accuracy                         0.6408    1623.0
   macro avg     0.6294    0.6425    0.6303    1623.0
weighted avg     0.6599    0.6408    0.6463    1623.0

[[ 71.  13.  14.   4.  41.   1.]
 [  3. 173.  36.   2.   0.  31.]
 [ 48.  25. 222.  25.   6.  58.]
 [  0.   0.   4. 129.   0.  37.]
 [ 80.   0.   8.   0. 210.   1.]
 [  3.  21.  52.  67.   3. 235.]]
loss: 4.669638633728027
loss: 4.471432685852051
loss: 4.531941890716553
epoch: 21, train_loss: 4.579699993133545, train_acc: 65.77, train_fscore: 65.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.531899929046631, test_acc: 64.88, test_fscore: 65.08, time: 6.15 sec
loss: 4.57973051071167
loss: 4.484492301940918
loss: 4.477621078491211
epoch: 22, train_loss: 4.53439998626709, train_acc: 65.92, train_fscore: 65.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.47760009765625, test_acc: 63.65, test_fscore: 63.85, time: 5.83 sec
loss: 4.53423547744751
loss: 4.316095352172852
loss: 4.423214912414551
epoch: 23, train_loss: 4.439599990844727, train_acc: 65.85, train_fscore: 64.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.4232001304626465, test_acc: 64.26, test_fscore: 64.46, time: 6.41 sec
loss: 4.400761127471924
loss: 4.404922962188721
loss: 4.39012336730957
epoch: 24, train_loss: 4.4028000831604, train_acc: 66.61, train_fscore: 65.75, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.390100002288818, test_acc: 64.2, test_fscore: 64.52, time: 6.22 sec
loss: 4.322891712188721
loss: 4.3696818351745605
loss: 4.3863115310668945
epoch: 25, train_loss: 4.345099925994873, train_acc: 66.85, train_fscore: 66.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.386300086975098, test_acc: 64.26, test_fscore: 64.76, time: 6.13 sec
loss: 4.34994649887085
loss: 4.181193828582764
loss: 4.352190017700195
epoch: 26, train_loss: 4.270400047302246, train_acc: 67.49, train_fscore: 66.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.352200031280518, test_acc: 64.88, test_fscore: 65.09, time: 6.27 sec
loss: 4.206986427307129
loss: 4.237548828125
loss: 4.300191879272461
epoch: 27, train_loss: 4.221399784088135, train_acc: 68.4, train_fscore: 67.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.30019998550415, test_acc: 65.13, test_fscore: 65.49, time: 6.83 sec
loss: 4.053771018981934
loss: 4.3134613037109375
loss: 4.270029067993164
epoch: 28, train_loss: 4.181600093841553, train_acc: 68.19, train_fscore: 67.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.269999980926514, test_acc: 65.06, test_fscore: 65.44, time: 5.18 sec
loss: 4.158456325531006
loss: 4.0915093421936035
loss: 4.221153736114502
epoch: 29, train_loss: 4.128399848937988, train_acc: 69.04, train_fscore: 68.47, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.221199989318848, test_acc: 66.05, test_fscore: 66.09, time: 6.74 sec
loss: 4.192943096160889
loss: 3.9697988033294678
loss: 4.201642990112305
epoch: 30, train_loss: 4.090400218963623, train_acc: 68.52, train_fscore: 67.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.201600074768066, test_acc: 65.0, test_fscore: 65.4, time: 6.43 sec
              precision    recall  f1-score   support

           0     0.4110    0.4653    0.4365     144.0
           1     0.7250    0.7102    0.7175     245.0
           2     0.6657    0.5755    0.6173     384.0
           3     0.5727    0.7412    0.6462     170.0
           4     0.8137    0.8328    0.8231     299.0
           5     0.6492    0.6168    0.6326     381.0

    accuracy                         0.6605    1623.0
   macro avg     0.6396    0.6570    0.6455    1623.0
weighted avg     0.6657    0.6605    0.6609    1623.0

[[ 67.  11.  14.   5.  45.   2.]
 [  3. 174.  36.   0.   0.  32.]
 [ 50.  29. 221.  27.   6.  51.]
 [  0.   0.   3. 126.   0.  41.]
 [ 41.   0.   8.   0. 249.   1.]
 [  2.  26.  50.  62.   6. 235.]]
loss: 4.067998886108398
loss: 4.010807037353516
loss: 4.15912389755249
epoch: 31, train_loss: 4.041100025177002, train_acc: 69.57, train_fscore: 69.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.15910005569458, test_acc: 65.5, test_fscore: 65.64, time: 6.64 sec
loss: 4.117117404937744
loss: 3.8595781326293945
loss: 4.131591320037842
epoch: 32, train_loss: 4.0019001960754395, train_acc: 69.09, train_fscore: 68.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.1315999031066895, test_acc: 65.13, test_fscore: 65.62, time: 6.27 sec
loss: 3.899238109588623
loss: 4.001162528991699
loss: 4.0829644203186035
epoch: 33, train_loss: 3.947999954223633, train_acc: 70.19, train_fscore: 69.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.083000183105469, test_acc: 65.99, test_fscore: 66.49, time: 6.14 sec
loss: 3.952101230621338
loss: 3.8443336486816406
loss: 4.039297103881836
epoch: 34, train_loss: 3.905400037765503, train_acc: 70.45, train_fscore: 69.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.039299964904785, test_acc: 66.61, test_fscore: 66.87, time: 5.99 sec
loss: 3.8225486278533936
loss: 3.9304089546203613
loss: 4.001004695892334
epoch: 35, train_loss: 3.873500108718872, train_acc: 70.55, train_fscore: 70.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.000999927520752, test_acc: 66.54, test_fscore: 66.86, time: 6.92 sec
loss: 3.806225299835205
loss: 3.8285317420959473
loss: 3.969167709350586
epoch: 36, train_loss: 3.8164000511169434, train_acc: 72.03, train_fscore: 71.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9691998958587646, test_acc: 67.47, test_fscore: 67.77, time: 6.43 sec
loss: 3.8160958290100098
loss: 3.756502151489258
loss: 3.9519808292388916
epoch: 37, train_loss: 3.7890000343322754, train_acc: 71.64, train_fscore: 71.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9519999027252197, test_acc: 67.41, test_fscore: 67.56, time: 5.63 sec
loss: 3.8731307983398438
loss: 3.600877285003662
loss: 3.9165232181549072
epoch: 38, train_loss: 3.7435998916625977, train_acc: 71.69, train_fscore: 71.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9165000915527344, test_acc: 67.53, test_fscore: 67.92, time: 6.85 sec
loss: 3.6498703956604004
loss: 3.7778992652893066
loss: 3.8724420070648193
epoch: 39, train_loss: 3.7100000381469727, train_acc: 71.88, train_fscore: 71.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8724000453948975, test_acc: 67.71, test_fscore: 68.03, time: 6.07 sec
loss: 3.7599339485168457
loss: 3.593677282333374
loss: 3.8538286685943604
epoch: 40, train_loss: 3.681299924850464, train_acc: 72.43, train_fscore: 72.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.853800058364868, test_acc: 67.96, test_fscore: 68.33, time: 6.27 sec
              precision    recall  f1-score   support

           0     0.4388    0.5972    0.5059     144.0
           1     0.7672    0.7265    0.7463     245.0
           2     0.6880    0.6432    0.6649     384.0
           3     0.6124    0.7529    0.6755     170.0
           4     0.8413    0.7625    0.8000     299.0
           5     0.6629    0.6194    0.6404     381.0

    accuracy                         0.6796    1623.0
   macro avg     0.6685    0.6836    0.6722    1623.0
weighted avg     0.6923    0.6796    0.6833    1623.0

[[ 86.   9.  16.   2.  31.   0.]
 [  5. 178.  25.   0.   0.  37.]
 [ 43.  25. 247.  22.   5.  42.]
 [  0.   0.   2. 128.   0.  40.]
 [ 62.   0.   8.   0. 228.   1.]
 [  0.  20.  61.  57.   7. 236.]]
loss: 3.762157440185547
loss: 3.496886968612671
loss: 3.846195936203003
epoch: 41, train_loss: 3.6366000175476074, train_acc: 72.98, train_fscore: 72.72, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8461999893188477, test_acc: 68.02, test_fscore: 68.33, time: 5.89 sec
loss: 3.687849998474121
loss: 3.5383858680725098
loss: 3.8193087577819824
epoch: 42, train_loss: 3.6226000785827637, train_acc: 72.7, train_fscore: 72.37, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8192999362945557, test_acc: 68.39, test_fscore: 68.62, time: 5.95 sec
loss: 3.5076074600219727
loss: 3.6367688179016113
loss: 3.790127754211426
epoch: 43, train_loss: 3.5685999393463135, train_acc: 72.93, train_fscore: 72.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.79010009765625, test_acc: 68.52, test_fscore: 68.88, time: 5.85 sec
loss: 3.4711482524871826
loss: 3.647120714187622
loss: 3.7437915802001953
epoch: 44, train_loss: 3.5536999702453613, train_acc: 73.32, train_fscore: 73.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.743799924850464, test_acc: 68.15, test_fscore: 68.44, time: 5.78 sec
loss: 3.3657376766204834
loss: 3.6726675033569336
loss: 3.743957042694092
epoch: 45, train_loss: 3.5107998847961426, train_acc: 73.39, train_fscore: 73.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.74399995803833, test_acc: 68.21, test_fscore: 68.58, time: 6.59 sec
loss: 3.5666351318359375
loss: 3.4726979732513428
loss: 3.7439615726470947
epoch: 46, train_loss: 3.523900032043457, train_acc: 73.48, train_fscore: 73.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.74399995803833, test_acc: 68.76, test_fscore: 69.16, time: 6.29 sec
loss: 3.4474830627441406
loss: 3.499342441558838
loss: 3.7205088138580322
epoch: 47, train_loss: 3.470900058746338, train_acc: 73.94, train_fscore: 73.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7204999923706055, test_acc: 68.58, test_fscore: 68.91, time: 5.35 sec
loss: 3.598799228668213
loss: 3.3153300285339355
loss: 3.698122262954712
epoch: 48, train_loss: 3.4677999019622803, train_acc: 74.23, train_fscore: 74.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6981000900268555, test_acc: 68.64, test_fscore: 69.08, time: 6.4 sec
loss: 3.5044708251953125
loss: 3.346890926361084
loss: 3.6837055683135986
epoch: 49, train_loss: 3.4319000244140625, train_acc: 73.82, train_fscore: 73.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6837000846862793, test_acc: 68.64, test_fscore: 69.0, time: 5.6 sec
loss: 3.287834644317627
loss: 3.490352153778076
loss: 3.668795585632324
epoch: 50, train_loss: 3.3866000175476074, train_acc: 74.06, train_fscore: 73.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.668800115585327, test_acc: 68.7, test_fscore: 69.02, time: 6.28 sec
              precision    recall  f1-score   support

           0     0.4527    0.6319    0.5275     144.0
           1     0.7679    0.7429    0.7552     245.0
           2     0.6991    0.6354    0.6658     384.0
           3     0.6207    0.7412    0.6756     170.0
           4     0.8598    0.7592    0.8064     299.0
           5     0.6667    0.6457    0.6560     381.0

    accuracy                         0.6876    1623.0
   macro avg     0.6778    0.6927    0.6811    1623.0
weighted avg     0.7014    0.6876    0.6916    1623.0

[[ 91.   9.  17.   1.  26.   0.]
 [  4. 182.  23.   0.   0.  36.]
 [ 44.  24. 244.  23.   5.  44.]
 [  0.   0.   2. 126.   0.  42.]
 [ 62.   1.   8.   0. 227.   1.]
 [  0.  21.  55.  53.   6. 246.]]
loss: 3.5215296745300293
loss: 3.179879665374756
loss: 3.663818120956421
epoch: 51, train_loss: 3.367300033569336, train_acc: 74.25, train_fscore: 73.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.663800001144409, test_acc: 67.9, test_fscore: 68.27, time: 6.35 sec
loss: 3.28463077545166
loss: 3.4305615425109863
loss: 3.64501690864563
epoch: 52, train_loss: 3.3513998985290527, train_acc: 74.18, train_fscore: 73.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6449999809265137, test_acc: 68.45, test_fscore: 68.81, time: 6.14 sec
loss: 3.553018093109131
loss: 3.069458246231079
loss: 3.6010220050811768
epoch: 53, train_loss: 3.32669997215271, train_acc: 74.63, train_fscore: 74.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6010000705718994, test_acc: 69.13, test_fscore: 69.56, time: 6.21 sec
loss: 3.2859318256378174
loss: 3.3195102214813232
loss: 3.583404064178467
epoch: 54, train_loss: 3.3010001182556152, train_acc: 75.22, train_fscore: 74.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.583400011062622, test_acc: 69.13, test_fscore: 69.56, time: 5.82 sec
loss: 3.3097033500671387
loss: 3.283932685852051
loss: 3.5790798664093018
epoch: 55, train_loss: 3.297600030899048, train_acc: 75.3, train_fscore: 75.12, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5790998935699463, test_acc: 69.32, test_fscore: 69.64, time: 6.86 sec
loss: 3.179293632507324
loss: 3.3854410648345947
loss: 3.604888677597046
epoch: 56, train_loss: 3.276900053024292, train_acc: 75.27, train_fscore: 75.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6048998832702637, test_acc: 68.76, test_fscore: 69.08, time: 6.28 sec
loss: 3.232248306274414
loss: 3.2691152095794678
loss: 3.5576791763305664
epoch: 57, train_loss: 3.2488999366760254, train_acc: 75.94, train_fscore: 75.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5576999187469482, test_acc: 69.07, test_fscore: 69.43, time: 6.09 sec
loss: 3.1301467418670654
loss: 3.3377723693847656
loss: 3.5341100692749023
epoch: 58, train_loss: 3.222899913787842, train_acc: 75.78, train_fscore: 75.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.53410005569458, test_acc: 69.38, test_fscore: 69.82, time: 6.21 sec
loss: 3.283153533935547
loss: 3.1206607818603516
loss: 3.545963764190674
epoch: 59, train_loss: 3.2081000804901123, train_acc: 76.13, train_fscore: 75.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5460000038146973, test_acc: 69.38, test_fscore: 69.74, time: 6.91 sec
loss: 3.244948625564575
loss: 3.177321434020996
loss: 3.528304100036621
epoch: 60, train_loss: 3.2132999897003174, train_acc: 76.27, train_fscore: 76.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5283000469207764, test_acc: 69.69, test_fscore: 70.06, time: 6.68 sec
              precision    recall  f1-score   support

           0     0.4750    0.6597    0.5523     144.0
           1     0.7897    0.7510    0.7699     245.0
           2     0.7135    0.6484    0.6794     384.0
           3     0.6184    0.7529    0.6790     170.0
           4     0.8619    0.7726    0.8148     299.0
           5     0.6667    0.6404    0.6533     381.0

    accuracy                         0.6969    1623.0
   macro avg     0.6875    0.7042    0.6915    1623.0
weighted avg     0.7102    0.6969    0.7006    1623.0

[[ 95.   9.  15.   0.  25.   0.]
 [  3. 184.  21.   0.   0.  37.]
 [ 44.  20. 249.  22.   6.  43.]
 [  0.   0.   1. 128.   0.  41.]
 [ 57.   2.   8.   0. 231.   1.]
 [  1.  18.  55.  57.   6. 244.]]
loss: 3.108458995819092
loss: 3.255382537841797
loss: 3.507063388824463
epoch: 61, train_loss: 3.179500102996826, train_acc: 76.32, train_fscore: 76.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5071001052856445, test_acc: 69.62, test_fscore: 70.0, time: 6.12 sec
loss: 3.0639352798461914
loss: 3.267500877380371
loss: 3.5305542945861816
epoch: 62, train_loss: 3.1624999046325684, train_acc: 76.16, train_fscore: 75.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.530600070953369, test_acc: 69.5, test_fscore: 69.96, time: 6.02 sec
loss: 3.211820125579834
loss: 3.085014581680298
loss: 3.5125606060028076
epoch: 63, train_loss: 3.1524999141693115, train_acc: 76.49, train_fscore: 76.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5125999450683594, test_acc: 69.38, test_fscore: 69.91, time: 6.27 sec
loss: 3.166353464126587
loss: 3.088742733001709
loss: 3.5007781982421875
epoch: 64, train_loss: 3.131200075149536, train_acc: 76.92, train_fscore: 76.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5007998943328857, test_acc: 69.38, test_fscore: 69.72, time: 5.57 sec
loss: 3.187533140182495
loss: 3.0438005924224854
loss: 3.5077381134033203
epoch: 65, train_loss: 3.121000051498413, train_acc: 76.92, train_fscore: 76.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.507699966430664, test_acc: 69.56, test_fscore: 69.92, time: 6.24 sec
loss: 3.0749411582946777
loss: 3.104257345199585
loss: 3.4504449367523193
epoch: 66, train_loss: 3.0889999866485596, train_acc: 76.87, train_fscore: 76.7, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4504001140594482, test_acc: 69.93, test_fscore: 70.39, time: 6.15 sec
loss: 2.9651784896850586
loss: 3.1873483657836914
loss: 3.4391698837280273
epoch: 67, train_loss: 3.0724000930786133, train_acc: 77.02, train_fscore: 76.85, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.439199924468994, test_acc: 70.24, test_fscore: 70.64, time: 5.68 sec
loss: 3.0067930221557617
loss: 3.152170181274414
loss: 3.4778635501861572
epoch: 68, train_loss: 3.0697999000549316, train_acc: 77.4, train_fscore: 77.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4779000282287598, test_acc: 69.87, test_fscore: 70.39, time: 6.42 sec
loss: 3.061734914779663
loss: 3.056955337524414
loss: 3.4340994358062744
epoch: 69, train_loss: 3.0594000816345215, train_acc: 77.61, train_fscore: 77.5, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4340999126434326, test_acc: 69.87, test_fscore: 70.2, time: 5.94 sec
loss: 2.932037115097046
loss: 3.1433472633361816
loss: 3.4233133792877197
epoch: 70, train_loss: 3.033099889755249, train_acc: 77.59, train_fscore: 77.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.42330002784729, test_acc: 70.86, test_fscore: 71.21, time: 6.17 sec
              precision    recall  f1-score   support

           0     0.5000    0.7153    0.5886     144.0
           1     0.8079    0.7551    0.7806     245.0
           2     0.7238    0.6484    0.6841     384.0
           3     0.6337    0.7529    0.6882     170.0
           4     0.8657    0.7759    0.8183     299.0
           5     0.6765    0.6640    0.6702     381.0

    accuracy                         0.7086    1623.0
   macro avg     0.7013    0.7186    0.7050    1623.0
weighted avg     0.7222    0.7086    0.7121    1623.0

[[103.   8.  11.   0.  22.   0.]
 [  2. 185.  23.   1.   0.  34.]
 [ 44.  18. 249.  21.   7.  45.]
 [  0.   0.   1. 128.   0.  41.]
 [ 56.   2.   8.   0. 232.   1.]
 [  1.  16.  52.  52.   7. 253.]]
loss: 3.117253065109253
loss: 2.8725640773773193
loss: 3.4174585342407227
epoch: 71, train_loss: 3.009700059890747, train_acc: 77.76, train_fscore: 77.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4175000190734863, test_acc: 70.43, test_fscore: 70.9, time: 6.06 sec
loss: 2.8707494735717773
loss: 3.1417832374572754
loss: 3.4115004539489746
epoch: 72, train_loss: 3.007999897003174, train_acc: 78.19, train_fscore: 78.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4114999771118164, test_acc: 70.92, test_fscore: 71.21, time: 6.47 sec
loss: 3.1204051971435547
loss: 2.866213798522949
loss: 3.435115337371826
epoch: 73, train_loss: 3.0046000480651855, train_acc: 78.28, train_fscore: 78.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4351000785827637, test_acc: 70.3, test_fscore: 70.76, time: 6.22 sec
loss: 2.9393465518951416
loss: 3.036768913269043
loss: 3.430579900741577
epoch: 74, train_loss: 2.9869000911712646, train_acc: 78.54, train_fscore: 78.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4305999279022217, test_acc: 70.61, test_fscore: 70.99, time: 6.43 sec
loss: 3.067612648010254
loss: 2.828700542449951
loss: 3.416414976119995
epoch: 75, train_loss: 2.9551000595092773, train_acc: 78.61, train_fscore: 78.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4163999557495117, test_acc: 70.98, test_fscore: 71.36, time: 5.99 sec
loss: 2.963775157928467
loss: 2.9342689514160156
loss: 3.3844895362854004
epoch: 76, train_loss: 2.9500999450683594, train_acc: 78.35, train_fscore: 78.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.384500026702881, test_acc: 70.86, test_fscore: 71.23, time: 6.51 sec
loss: 2.8347244262695312
loss: 3.0540871620178223
loss: 3.4024126529693604
epoch: 77, train_loss: 2.9342000484466553, train_acc: 78.52, train_fscore: 78.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.402400016784668, test_acc: 70.79, test_fscore: 71.1, time: 5.73 sec
loss: 2.8682751655578613
loss: 2.9637463092803955
loss: 3.41658353805542
epoch: 78, train_loss: 2.913599967956543, train_acc: 78.67, train_fscore: 78.51, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.416599988937378, test_acc: 70.98, test_fscore: 71.36, time: 6.71 sec
loss: 2.987502336502075
loss: 2.860696792602539
loss: 3.3728439807891846
epoch: 79, train_loss: 2.9272000789642334, train_acc: 78.9, train_fscore: 78.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.37280011177063, test_acc: 70.92, test_fscore: 71.32, time: 6.17 sec
loss: 2.882375717163086
loss: 2.884857177734375
loss: 3.3764569759368896
epoch: 80, train_loss: 2.883500099182129, train_acc: 79.48, train_fscore: 79.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.376499891281128, test_acc: 71.16, test_fscore: 71.54, time: 6.61 sec
              precision    recall  f1-score   support

           0     0.5116    0.7639    0.6128     144.0
           1     0.8158    0.7592    0.7865     245.0
           2     0.7232    0.6667    0.6938     384.0
           3     0.6275    0.7529    0.6845     170.0
           4     0.8842    0.7659    0.8208     299.0
           5     0.6777    0.6457    0.6613     381.0

    accuracy                         0.7116    1623.0
   macro avg     0.7066    0.7257    0.7099    1623.0
weighted avg     0.7273    0.7116    0.7154    1623.0

[[110.   6.   9.   0.  19.   0.]
 [  2. 186.  22.   1.   0.  34.]
 [ 45.  18. 256.  20.   4.  41.]
 [  0.   0.   1. 128.   0.  41.]
 [ 57.   2.  10.   0. 229.   1.]
 [  1.  16.  56.  55.   7. 246.]]
loss: 2.8773581981658936
loss: 2.9188766479492188
loss: 3.385495662689209
epoch: 81, train_loss: 2.89739990234375, train_acc: 79.29, train_fscore: 79.15, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.385499954223633, test_acc: 70.92, test_fscore: 71.31, time: 6.34 sec
loss: 2.884646415710449
loss: 2.8536860942840576
loss: 3.3849573135375977
epoch: 82, train_loss: 2.8696000576019287, train_acc: 79.45, train_fscore: 79.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.384999990463257, test_acc: 70.61, test_fscore: 71.02, time: 6.91 sec
loss: 2.880280017852783
loss: 2.79468035697937
loss: 3.37117862701416
epoch: 83, train_loss: 2.842099905014038, train_acc: 79.33, train_fscore: 79.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3712000846862793, test_acc: 70.3, test_fscore: 70.73, time: 5.55 sec
loss: 2.8313682079315186
loss: 2.8397140502929688
loss: 3.3662660121917725
epoch: 84, train_loss: 2.835400104522705, train_acc: 79.59, train_fscore: 79.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.366300106048584, test_acc: 71.04, test_fscore: 71.38, time: 6.62 sec
loss: 2.7918994426727295
loss: 2.921877145767212
loss: 3.3364124298095703
epoch: 85, train_loss: 2.851900100708008, train_acc: 79.78, train_fscore: 79.63, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.336400032043457, test_acc: 71.23, test_fscore: 71.54, time: 6.08 sec
loss: 2.965017557144165
loss: 2.668855667114258
loss: 3.3452577590942383
epoch: 86, train_loss: 2.8224000930786133, train_acc: 80.12, train_fscore: 79.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3452999591827393, test_acc: 71.29, test_fscore: 71.63, time: 6.59 sec
loss: 2.772108554840088
loss: 2.8390631675720215
loss: 3.351634979248047
epoch: 87, train_loss: 2.801500082015991, train_acc: 79.88, train_fscore: 79.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.351599931716919, test_acc: 71.35, test_fscore: 71.68, time: 5.75 sec
loss: 2.816601276397705
loss: 2.7585949897766113
loss: 3.341655731201172
epoch: 88, train_loss: 2.7899999618530273, train_acc: 80.19, train_fscore: 80.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3417000770568848, test_acc: 70.92, test_fscore: 71.2, time: 6.67 sec
loss: 2.886442184448242
loss: 2.6407411098480225
loss: 3.3309335708618164
epoch: 89, train_loss: 2.77239990234375, train_acc: 80.1, train_fscore: 79.97, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.330899953842163, test_acc: 70.98, test_fscore: 71.28, time: 6.47 sec
loss: 2.6983261108398438
loss: 2.888472557067871
loss: 3.3553214073181152
epoch: 90, train_loss: 2.782399892807007, train_acc: 80.24, train_fscore: 80.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.355299949645996, test_acc: 71.29, test_fscore: 71.63, time: 6.73 sec
              precision    recall  f1-score   support

           0     0.5164    0.7639    0.6162     144.0
           1     0.8112    0.7714    0.7908     245.0
           2     0.7335    0.6667    0.6985     384.0
           3     0.6305    0.7529    0.6863     170.0
           4     0.8702    0.7625    0.8128     299.0
           5     0.6804    0.6483    0.6640     381.0

    accuracy                         0.7135    1623.0
   macro avg     0.7071    0.7276    0.7114    1623.0
weighted avg     0.7279    0.7135    0.7168    1623.0

[[110.   7.   8.   0.  19.   0.]
 [  2. 189.  20.   2.   0.  32.]
 [ 42.  19. 256.  18.   7.  42.]
 [  0.   0.   1. 128.   0.  41.]
 [ 58.   2.  10.   0. 228.   1.]
 [  1.  16.  54.  55.   8. 247.]]
loss: 2.7662134170532227
loss: 2.7648797035217285
loss: 3.344421863555908
epoch: 91, train_loss: 2.7655999660491943, train_acc: 80.86, train_fscore: 80.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.344399929046631, test_acc: 71.1, test_fscore: 71.45, time: 6.12 sec
loss: 2.8039517402648926
loss: 2.7227776050567627
loss: 3.314694404602051
epoch: 92, train_loss: 2.7676000595092773, train_acc: 80.71, train_fscore: 80.59, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.31469988822937, test_acc: 71.29, test_fscore: 71.6, time: 6.18 sec
loss: 2.7260727882385254
loss: 2.768786907196045
loss: 3.339371681213379
epoch: 93, train_loss: 2.7462000846862793, train_acc: 80.81, train_fscore: 80.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.339400053024292, test_acc: 70.61, test_fscore: 71.06, time: 6.2 sec
loss: 2.631171941757202
loss: 2.850851535797119
loss: 3.321809768676758
epoch: 94, train_loss: 2.735599994659424, train_acc: 81.0, train_fscore: 80.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3217999935150146, test_acc: 71.16, test_fscore: 71.44, time: 6.24 sec
loss: 2.8837406635284424
loss: 2.5697286128997803
loss: 3.3394827842712402
epoch: 95, train_loss: 2.7451000213623047, train_acc: 81.22, train_fscore: 81.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3394999504089355, test_acc: 71.16, test_fscore: 71.5, time: 6.11 sec
loss: 2.801039457321167
loss: 2.5882949829101562
loss: 3.3223395347595215
epoch: 96, train_loss: 2.6981000900268555, train_acc: 81.36, train_fscore: 81.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3222999572753906, test_acc: 71.23, test_fscore: 71.59, time: 6.39 sec
loss: 2.584718942642212
loss: 2.848649501800537
loss: 3.3038930892944336
epoch: 97, train_loss: 2.7081000804901123, train_acc: 81.19, train_fscore: 81.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3039000034332275, test_acc: 71.35, test_fscore: 71.61, time: 5.59 sec
loss: 2.627725601196289
loss: 2.757260322570801
loss: 3.3345491886138916
epoch: 98, train_loss: 2.691999912261963, train_acc: 81.24, train_fscore: 81.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3345000743865967, test_acc: 70.61, test_fscore: 71.01, time: 6.46 sec
loss: 2.6299796104431152
loss: 2.7775440216064453
loss: 3.3029251098632812
epoch: 99, train_loss: 2.6988000869750977, train_acc: 81.19, train_fscore: 81.06, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3029000759124756, test_acc: 72.03, test_fscore: 72.33, time: 6.03 sec
loss: 2.6561074256896973
loss: 2.6956076622009277
loss: 3.3193421363830566
epoch: 100, train_loss: 2.674099922180176, train_acc: 81.5, train_fscore: 81.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3192999362945557, test_acc: 71.47, test_fscore: 71.83, time: 6.24 sec
              precision    recall  f1-score   support

           0     0.5388    0.7708    0.6343     144.0
           1     0.8304    0.7592    0.7932     245.0
           2     0.7365    0.6771    0.7056     384.0
           3     0.6510    0.7353    0.6906     170.0
           4     0.8672    0.7860    0.8246     299.0
           5     0.6684    0.6614    0.6649     381.0

    accuracy                         0.7203    1623.0
   macro avg     0.7154    0.7316    0.7189    1623.0
weighted avg     0.7323    0.7203    0.7233    1623.0

[[111.   4.   7.   0.  22.   0.]
 [  3. 186.  19.   0.   0.  37.]
 [ 40.  17. 260.  17.   7.  43.]
 [  0.   0.   1. 125.   0.  44.]
 [ 51.   2.  10.   0. 235.   1.]
 [  1.  15.  56.  50.   7. 252.]]
loss: 2.612359046936035
loss: 2.7398407459259033
loss: 3.350388526916504
epoch: 101, train_loss: 2.6707000732421875, train_acc: 81.19, train_fscore: 81.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.350399971008301, test_acc: 71.23, test_fscore: 71.57, time: 6.22 sec
loss: 2.636989116668701
loss: 2.694284200668335
loss: 3.3302464485168457
epoch: 102, train_loss: 2.663100004196167, train_acc: 81.51, train_fscore: 81.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.330199956893921, test_acc: 71.1, test_fscore: 71.36, time: 6.38 sec
loss: 2.839482307434082
loss: 2.4243555068969727
loss: 3.321704149246216
epoch: 103, train_loss: 2.6519999504089355, train_acc: 82.03, train_fscore: 81.89, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.321700096130371, test_acc: 71.16, test_fscore: 71.53, time: 5.58 sec
loss: 2.6131067276000977
loss: 2.6665072441101074
loss: 3.3254480361938477
epoch: 104, train_loss: 2.6368000507354736, train_acc: 81.88, train_fscore: 81.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3254001140594482, test_acc: 71.35, test_fscore: 71.7, time: 6.63 sec
loss: 2.5844714641571045
loss: 2.6575870513916016
loss: 3.2959907054901123
epoch: 105, train_loss: 2.6198999881744385, train_acc: 81.94, train_fscore: 81.85, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2960000038146973, test_acc: 71.72, test_fscore: 71.99, time: 6.5 sec
loss: 2.6873693466186523
loss: 2.5259478092193604
loss: 3.325671434402466
epoch: 106, train_loss: 2.6089000701904297, train_acc: 81.94, train_fscore: 81.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.325700044631958, test_acc: 71.6, test_fscore: 71.97, time: 5.66 sec
loss: 2.6982181072235107
loss: 2.515395164489746
loss: 3.312130928039551
epoch: 107, train_loss: 2.6103999614715576, train_acc: 82.19, train_fscore: 82.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3120999336242676, test_acc: 71.47, test_fscore: 71.77, time: 6.33 sec
loss: 2.5980207920074463
loss: 2.6270933151245117
loss: 3.3491108417510986
epoch: 108, train_loss: 2.611799955368042, train_acc: 81.69, train_fscore: 81.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.349100112915039, test_acc: 71.53, test_fscore: 71.81, time: 5.28 sec
loss: 2.6171672344207764
loss: 2.552845001220703
loss: 3.352598190307617
epoch: 109, train_loss: 2.5859999656677246, train_acc: 82.39, train_fscore: 82.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.35260009765625, test_acc: 70.73, test_fscore: 71.06, time: 5.55 sec
loss: 2.454902172088623
loss: 2.7170305252075195
loss: 3.337683916091919
epoch: 110, train_loss: 2.582900047302246, train_acc: 82.17, train_fscore: 82.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3376998901367188, test_acc: 71.04, test_fscore: 71.41, time: 6.44 sec
              precision    recall  f1-score   support

           0     0.5388    0.7708    0.6343     144.0
           1     0.8304    0.7592    0.7932     245.0
           2     0.7365    0.6771    0.7056     384.0
           3     0.6510    0.7353    0.6906     170.0
           4     0.8672    0.7860    0.8246     299.0
           5     0.6684    0.6614    0.6649     381.0

    accuracy                         0.7203    1623.0
   macro avg     0.7154    0.7316    0.7189    1623.0
weighted avg     0.7323    0.7203    0.7233    1623.0

[[111.   4.   7.   0.  22.   0.]
 [  3. 186.  19.   0.   0.  37.]
 [ 40.  17. 260.  17.   7.  43.]
 [  0.   0.   1. 125.   0.  44.]
 [ 51.   2.  10.   0. 235.   1.]
 [  1.  15.  56.  50.   7. 252.]]
loss: 2.5349974632263184
loss: 2.638195037841797
loss: 3.3291096687316895
epoch: 111, train_loss: 2.5817999839782715, train_acc: 82.58, train_fscore: 82.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3290998935699463, test_acc: 71.23, test_fscore: 71.5, time: 6.32 sec
loss: 2.436650514602661
loss: 2.7008070945739746
loss: 3.3161637783050537
epoch: 112, train_loss: 2.5615999698638916, train_acc: 82.5, train_fscore: 82.43, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.316200017929077, test_acc: 71.1, test_fscore: 71.5, time: 6.39 sec
loss: 2.5315463542938232
loss: 2.594409704208374
loss: 3.3334732055664062
epoch: 113, train_loss: 2.5599000453948975, train_acc: 82.31, train_fscore: 82.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3334999084472656, test_acc: 71.84, test_fscore: 72.14, time: 6.1 sec
loss: 2.3796935081481934
loss: 2.7409987449645996
loss: 3.331198215484619
epoch: 114, train_loss: 2.5481998920440674, train_acc: 83.05, train_fscore: 82.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.331199884414673, test_acc: 71.29, test_fscore: 71.68, time: 6.54 sec
loss: 2.5699567794799805
loss: 2.530852794647217
loss: 3.3319292068481445
epoch: 115, train_loss: 2.551500082015991, train_acc: 82.55, train_fscore: 82.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.331899881362915, test_acc: 71.35, test_fscore: 71.63, time: 6.08 sec
loss: 2.396031379699707
loss: 2.6953492164611816
loss: 3.3481788635253906
epoch: 116, train_loss: 2.5404000282287598, train_acc: 82.99, train_fscore: 82.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3482000827789307, test_acc: 71.1, test_fscore: 71.44, time: 6.49 sec
loss: 2.5121665000915527
loss: 2.5297653675079346
loss: 3.3395941257476807
epoch: 117, train_loss: 2.5202999114990234, train_acc: 83.17, train_fscore: 83.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.339600086212158, test_acc: 70.73, test_fscore: 71.15, time: 6.32 sec
loss: 2.647491693496704
loss: 2.3899524211883545
loss: 3.300987720489502
epoch: 118, train_loss: 2.5234999656677246, train_acc: 82.86, train_fscore: 82.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3010001182556152, test_acc: 71.72, test_fscore: 71.97, time: 6.06 sec
loss: 2.5764851570129395
loss: 2.416548013687134
loss: 3.308485507965088
epoch: 119, train_loss: 2.5006000995635986, train_acc: 83.7, train_fscore: 83.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.308500051498413, test_acc: 71.6, test_fscore: 71.94, time: 5.75 sec
loss: 2.536925792694092
loss: 2.4545066356658936
loss: 3.308899402618408
epoch: 120, train_loss: 2.4981000423431396, train_acc: 83.3, train_fscore: 83.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3089001178741455, test_acc: 71.6, test_fscore: 71.89, time: 5.45 sec
              precision    recall  f1-score   support

           0     0.5388    0.7708    0.6343     144.0
           1     0.8304    0.7592    0.7932     245.0
           2     0.7365    0.6771    0.7056     384.0
           3     0.6510    0.7353    0.6906     170.0
           4     0.8672    0.7860    0.8246     299.0
           5     0.6684    0.6614    0.6649     381.0

    accuracy                         0.7203    1623.0
   macro avg     0.7154    0.7316    0.7189    1623.0
weighted avg     0.7323    0.7203    0.7233    1623.0

[[111.   4.   7.   0.  22.   0.]
 [  3. 186.  19.   0.   0.  37.]
 [ 40.  17. 260.  17.   7.  43.]
 [  0.   0.   1. 125.   0.  44.]
 [ 51.   2.  10.   0. 235.   1.]
 [  1.  15.  56.  50.   7. 252.]]
loss: 2.4780197143554688
loss: 2.5117526054382324
loss: 3.294689893722534
epoch: 121, train_loss: 2.49429988861084, train_acc: 83.46, train_fscore: 83.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2946999073028564, test_acc: 72.27, test_fscore: 72.59, time: 6.33 sec
loss: 2.5087385177612305
loss: 2.4297938346862793
loss: 3.3187758922576904
epoch: 122, train_loss: 2.4714999198913574, train_acc: 83.37, train_fscore: 83.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3187999725341797, test_acc: 71.41, test_fscore: 71.75, time: 5.95 sec
loss: 2.632976531982422
loss: 2.272352695465088
loss: 3.327446699142456
epoch: 123, train_loss: 2.4702000617980957, train_acc: 83.46, train_fscore: 83.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.327399969100952, test_acc: 71.41, test_fscore: 71.71, time: 6.17 sec
loss: 2.476956367492676
loss: 2.4175634384155273
loss: 3.321678400039673
epoch: 124, train_loss: 2.449399948120117, train_acc: 83.55, train_fscore: 83.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.321700096130371, test_acc: 70.98, test_fscore: 71.26, time: 6.13 sec
loss: 2.4524755477905273
loss: 2.4455978870391846
loss: 3.2997887134552
epoch: 125, train_loss: 2.449199914932251, train_acc: 83.89, train_fscore: 83.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.299799919128418, test_acc: 71.47, test_fscore: 71.81, time: 6.27 sec
loss: 2.492025375366211
loss: 2.3834991455078125
loss: 3.2958600521087646
epoch: 126, train_loss: 2.4398999214172363, train_acc: 83.72, train_fscore: 83.63, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2959001064300537, test_acc: 71.53, test_fscore: 71.93, time: 5.99 sec
loss: 2.3890271186828613
loss: 2.485321044921875
loss: 3.3365955352783203
epoch: 127, train_loss: 2.431999921798706, train_acc: 83.56, train_fscore: 83.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3366000652313232, test_acc: 71.53, test_fscore: 71.89, time: 6.59 sec
loss: 2.516812324523926
loss: 2.3410580158233643
loss: 3.333496332168579
epoch: 128, train_loss: 2.435699939727783, train_acc: 83.75, train_fscore: 83.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3334999084472656, test_acc: 71.35, test_fscore: 71.66, time: 6.42 sec
loss: 2.4080970287323
loss: 2.4171838760375977
loss: 3.3194408416748047
epoch: 129, train_loss: 2.4123001098632812, train_acc: 84.13, train_fscore: 84.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3194000720977783, test_acc: 71.47, test_fscore: 71.84, time: 5.99 sec
loss: 2.511361598968506
loss: 2.2899413108825684
loss: 3.3478477001190186
epoch: 130, train_loss: 2.40939998626709, train_acc: 84.18, train_fscore: 84.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3478000164031982, test_acc: 71.78, test_fscore: 72.14, time: 6.21 sec
              precision    recall  f1-score   support

           0     0.5377    0.7917    0.6404     144.0
           1     0.8240    0.7837    0.8033     245.0
           2     0.7408    0.6849    0.7118     384.0
           3     0.6477    0.7353    0.6887     170.0
           4     0.8711    0.7458    0.8036     299.0
           5     0.6845    0.6719    0.6781     381.0

    accuracy                         0.7227    1623.0
   macro avg     0.7176    0.7355    0.7210    1623.0
weighted avg     0.7364    0.7227    0.7259    1623.0

[[114.   4.   6.   0.  20.   0.]
 [  3. 192.  16.   2.   0.  32.]
 [ 38.  19. 263.  16.   7.  41.]
 [  0.   0.   1. 125.   0.  44.]
 [ 56.   3.  16.   0. 223.   1.]
 [  1.  15.  53.  50.   6. 256.]]
loss: 2.382507085800171
loss: 2.4301648139953613
loss: 3.313952684402466
epoch: 131, train_loss: 2.4056999683380127, train_acc: 84.65, train_fscore: 84.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.313999891281128, test_acc: 71.47, test_fscore: 71.82, time: 5.83 sec
loss: 2.5347189903259277
loss: 2.268735408782959
loss: 3.3631036281585693
epoch: 132, train_loss: 2.401599884033203, train_acc: 84.27, train_fscore: 84.15, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.363100051879883, test_acc: 71.6, test_fscore: 71.9, time: 5.49 sec
loss: 2.435786247253418
loss: 2.378286361694336
loss: 3.328125476837158
epoch: 133, train_loss: 2.410599946975708, train_acc: 84.15, train_fscore: 84.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3280999660491943, test_acc: 71.23, test_fscore: 71.55, time: 6.49 sec
loss: 2.3605103492736816
loss: 2.3912057876586914
loss: 3.3117339611053467
epoch: 134, train_loss: 2.3743999004364014, train_acc: 84.3, train_fscore: 84.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3117001056671143, test_acc: 71.97, test_fscore: 72.31, time: 5.94 sec
loss: 2.3822743892669678
loss: 2.3360326290130615
loss: 3.3204569816589355
epoch: 135, train_loss: 2.3612000942230225, train_acc: 84.7, train_fscore: 84.62, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.320499897003174, test_acc: 71.23, test_fscore: 71.57, time: 6.03 sec
loss: 2.2858152389526367
loss: 2.4720287322998047
loss: 3.323643445968628
epoch: 136, train_loss: 2.375499963760376, train_acc: 84.04, train_fscore: 83.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3236000537872314, test_acc: 71.35, test_fscore: 71.67, time: 6.1 sec
loss: 2.3218092918395996
loss: 2.3972055912017822
loss: 3.3651983737945557
epoch: 137, train_loss: 2.3571999073028564, train_acc: 84.63, train_fscore: 84.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3652000427246094, test_acc: 71.16, test_fscore: 71.45, time: 6.23 sec
loss: 2.2880568504333496
loss: 2.4472830295562744
loss: 3.3311767578125
epoch: 138, train_loss: 2.3629000186920166, train_acc: 84.73, train_fscore: 84.63, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.331199884414673, test_acc: 71.47, test_fscore: 71.84, time: 6.64 sec
loss: 2.367770195007324
loss: 2.3418164253234863
loss: 3.3184516429901123
epoch: 139, train_loss: 2.354599952697754, train_acc: 84.94, train_fscore: 84.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.31850004196167, test_acc: 72.09, test_fscore: 72.41, time: 5.65 sec
loss: 2.42317271232605
loss: 2.2527456283569336
loss: 3.336444616317749
epoch: 140, train_loss: 2.3473000526428223, train_acc: 84.49, train_fscore: 84.37, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.336400032043457, test_acc: 71.35, test_fscore: 71.71, time: 6.25 sec
              precision    recall  f1-score   support

           0     0.5377    0.7917    0.6404     144.0
           1     0.8240    0.7837    0.8033     245.0
           2     0.7408    0.6849    0.7118     384.0
           3     0.6477    0.7353    0.6887     170.0
           4     0.8711    0.7458    0.8036     299.0
           5     0.6845    0.6719    0.6781     381.0

    accuracy                         0.7227    1623.0
   macro avg     0.7176    0.7355    0.7210    1623.0
weighted avg     0.7364    0.7227    0.7259    1623.0

[[114.   4.   6.   0.  20.   0.]
 [  3. 192.  16.   2.   0.  32.]
 [ 38.  19. 263.  16.   7.  41.]
 [  0.   0.   1. 125.   0.  44.]
 [ 56.   3.  16.   0. 223.   1.]
 [  1.  15.  53.  50.   6. 256.]]
loss: 2.294132947921753
loss: 2.3671083450317383
loss: 3.3237195014953613
epoch: 141, train_loss: 2.3292999267578125, train_acc: 84.91, train_fscore: 84.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.323699951171875, test_acc: 71.16, test_fscore: 71.44, time: 6.44 sec
loss: 2.227781295776367
loss: 2.4676361083984375
loss: 3.3391072750091553
epoch: 142, train_loss: 2.3364999294281006, train_acc: 84.77, train_fscore: 84.65, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.339099884033203, test_acc: 72.15, test_fscore: 72.47, time: 5.91 sec
loss: 2.260751724243164
loss: 2.422623872756958
loss: 3.3170065879821777
epoch: 143, train_loss: 2.3397998809814453, train_acc: 85.04, train_fscore: 84.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.316999912261963, test_acc: 71.29, test_fscore: 71.67, time: 6.17 sec
loss: 2.211717367172241
loss: 2.434682846069336
loss: 3.305901527404785
epoch: 144, train_loss: 2.31820011138916, train_acc: 85.04, train_fscore: 84.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3059000968933105, test_acc: 72.21, test_fscore: 72.41, time: 6.3 sec
loss: 2.369795560836792
loss: 2.2515344619750977
loss: 3.31961727142334
epoch: 145, train_loss: 2.313199996948242, train_acc: 85.49, train_fscore: 85.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3196001052856445, test_acc: 71.6, test_fscore: 71.88, time: 6.13 sec
loss: 2.1705315113067627
loss: 2.4251275062561035
loss: 3.3490586280822754
epoch: 146, train_loss: 2.2939000129699707, train_acc: 85.15, train_fscore: 85.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.349100112915039, test_acc: 71.6, test_fscore: 71.93, time: 6.49 sec
loss: 2.3861286640167236
loss: 2.19028902053833
loss: 3.330958366394043
epoch: 147, train_loss: 2.297100067138672, train_acc: 85.28, train_fscore: 85.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3310000896453857, test_acc: 71.66, test_fscore: 71.93, time: 6.1 sec
loss: 2.1832754611968994
loss: 2.3881988525390625
loss: 3.3153817653656006
epoch: 148, train_loss: 2.2813000679016113, train_acc: 85.44, train_fscore: 85.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3153998851776123, test_acc: 71.47, test_fscore: 71.75, time: 6.54 sec
loss: 2.299814462661743
loss: 2.291628360748291
loss: 3.3003323078155518
epoch: 149, train_loss: 2.296099901199341, train_acc: 85.28, train_fscore: 85.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.300299882888794, test_acc: 71.9, test_fscore: 72.24, time: 6.53 sec
loss: 2.203019142150879
loss: 2.3401360511779785
loss: 3.3386547565460205
epoch: 150, train_loss: 2.2701001167297363, train_acc: 85.54, train_fscore: 85.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.33870005607605, test_acc: 71.53, test_fscore: 71.89, time: 6.22 sec
              precision    recall  f1-score   support

           0     0.5377    0.7917    0.6404     144.0
           1     0.8240    0.7837    0.8033     245.0
           2     0.7408    0.6849    0.7118     384.0
           3     0.6477    0.7353    0.6887     170.0
           4     0.8711    0.7458    0.8036     299.0
           5     0.6845    0.6719    0.6781     381.0

    accuracy                         0.7227    1623.0
   macro avg     0.7176    0.7355    0.7210    1623.0
weighted avg     0.7364    0.7227    0.7259    1623.0

[[114.   4.   6.   0.  20.   0.]
 [  3. 192.  16.   2.   0.  32.]
 [ 38.  19. 263.  16.   7.  41.]
 [  0.   0.   1. 125.   0.  44.]
 [ 56.   3.  16.   0. 223.   1.]
 [  1.  15.  53.  50.   6. 256.]]
Test performance..
F-Score: 72.59
F-Score-index: 121
              precision    recall  f1-score   support

           0     0.5377    0.7917    0.6404     144.0
           1     0.8240    0.7837    0.8033     245.0
           2     0.7408    0.6849    0.7118     384.0
           3     0.6477    0.7353    0.6887     170.0
           4     0.8711    0.7458    0.8036     299.0
           5     0.6845    0.6719    0.6781     381.0

    accuracy                         0.7227    1623.0
   macro avg     0.7176    0.7355    0.7210    1623.0
weighted avg     0.7364    0.7227    0.7259    1623.0

[[114.   4.   6.   0.  20.   0.]
 [  3. 192.  16.   2.   0.  32.]
 [ 38.  19. 263.  16.   7.  41.]
 [  0.   0.   1. 125.   0.  44.]
 [ 56.   3.  16.   0. 223.   1.]
 [  1.  15.  53.  50.   6. 256.]]
--- 8 ---
Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.5, batch_size=64, hidden_dim=1024, n_head=8, epochs=150, temp=2, tensorboard=False, class_weight=True, Dataset='IEMOCAP')
Running on GPU
temp 2
total parameters: 79687704
training parameters: 79687704
loss: 7.926156044006348
loss: 11.441617965698242
loss: 8.318522453308105
epoch: 1, train_loss: 9.602800369262695, train_acc: 20.79, train_fscore: 19.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.318499565124512, test_acc: 21.32, test_fscore: 10.45, time: 8.78 sec
loss: 8.4873628616333
loss: 8.157840728759766
loss: 8.28526782989502
epoch: 2, train_loss: 8.328499794006348, train_acc: 24.65, train_fscore: 19.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.285300254821777, test_acc: 29.82, test_fscore: 18.78, time: 7.08 sec
loss: 8.604857444763184
loss: 7.818814277648926
loss: 6.999650478363037
epoch: 3, train_loss: 8.20740032196045, train_acc: 34.65, train_fscore: 29.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.99970006942749, test_acc: 35.61, test_fscore: 30.74, time: 7.34 sec
loss: 7.2818708419799805
loss: 7.3488359451293945
loss: 7.208280086517334
epoch: 4, train_loss: 7.313300132751465, train_acc: 38.55, train_fscore: 33.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.2083001136779785, test_acc: 34.32, test_fscore: 29.37, time: 6.37 sec
loss: 7.393525123596191
loss: 7.377649307250977
loss: 6.967838287353516
epoch: 5, train_loss: 7.385900020599365, train_acc: 42.65, train_fscore: 36.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.967800140380859, test_acc: 35.67, test_fscore: 30.05, time: 4.89 sec
loss: 7.049210548400879
loss: 6.844247817993164
loss: 6.515772819519043
epoch: 6, train_loss: 6.9517998695373535, train_acc: 45.06, train_fscore: 37.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.5157999992370605, test_acc: 48.06, test_fscore: 45.02, time: 2.67 sec
loss: 6.556231498718262
loss: 6.723142147064209
loss: 6.6198811531066895
epoch: 7, train_loss: 6.630300045013428, train_acc: 53.29, train_fscore: 51.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.619900226593018, test_acc: 52.19, test_fscore: 50.33, time: 4.09 sec
loss: 6.597716808319092
loss: 6.670766830444336
loss: 6.469470977783203
epoch: 8, train_loss: 6.632900238037109, train_acc: 54.35, train_fscore: 52.51, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.4695000648498535, test_acc: 54.65, test_fscore: 54.7, time: 5.45 sec
loss: 6.524777412414551
loss: 6.252583026885986
loss: 6.314199447631836
epoch: 9, train_loss: 6.402500152587891, train_acc: 54.78, train_fscore: 54.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.314199924468994, test_acc: 52.8, test_fscore: 50.48, time: 5.51 sec
loss: 6.190309524536133
loss: 6.177433013916016
loss: 6.219052314758301
epoch: 10, train_loss: 6.184199810028076, train_acc: 55.94, train_fscore: 51.64, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.219099998474121, test_acc: 49.85, test_fscore: 46.62, time: 6.37 sec
              precision    recall  f1-score   support

           0     0.2453    0.4514    0.3178     144.0
           1     0.6980    0.7265    0.7120     245.0
           2     0.6425    0.3229    0.4298     384.0
           3     0.5243    0.6353    0.5745     170.0
           4     0.6154    0.6421    0.6285     299.0
           5     0.5612    0.5774    0.5692     381.0

    accuracy                         0.5465    1623.0
   macro avg     0.5478    0.5593    0.5386    1623.0
weighted avg     0.5792    0.5465    0.5470    1623.0

[[ 65.  10.   6.   5.  55.   3.]
 [ 27. 178.   9.   4.   7.  20.]
 [ 90.  35. 124.  16.  19. 100.]
 [  0.   7.   6. 108.  13.  36.]
 [ 69.   4.  14.   7. 192.  13.]
 [ 14.  21.  34.  66.  26. 220.]]
loss: 5.964322566986084
loss: 6.118337154388428
loss: 6.0570454597473145
epoch: 11, train_loss: 6.035999774932861, train_acc: 56.23, train_fscore: 51.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.057000160217285, test_acc: 50.34, test_fscore: 49.57, time: 5.97 sec
loss: 5.866677284240723
loss: 5.759981155395508
loss: 5.78848934173584
epoch: 12, train_loss: 5.81689977645874, train_acc: 59.36, train_fscore: 58.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.78849983215332, test_acc: 56.5, test_fscore: 56.37, time: 5.21 sec
loss: 5.631104469299316
loss: 5.55420446395874
loss: 5.542903900146484
epoch: 13, train_loss: 5.593400001525879, train_acc: 63.44, train_fscore: 62.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.542900085449219, test_acc: 60.51, test_fscore: 59.75, time: 6.46 sec
loss: 5.364658355712891
loss: 5.4302544593811035
loss: 5.390909194946289
epoch: 14, train_loss: 5.394599914550781, train_acc: 63.61, train_fscore: 62.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.390900135040283, test_acc: 61.31, test_fscore: 60.74, time: 6.42 sec
loss: 5.257467269897461
loss: 5.2575883865356445
loss: 5.213827133178711
epoch: 15, train_loss: 5.257500171661377, train_acc: 62.05, train_fscore: 60.44, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.213799953460693, test_acc: 59.95, test_fscore: 59.66, time: 6.25 sec
loss: 4.998132705688477
loss: 5.215862274169922
loss: 5.059085845947266
epoch: 16, train_loss: 5.096799850463867, train_acc: 63.15, train_fscore: 61.85, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.059100151062012, test_acc: 63.09, test_fscore: 63.22, time: 6.82 sec
loss: 4.796513080596924
loss: 5.184168815612793
loss: 4.943158149719238
epoch: 17, train_loss: 4.990300178527832, train_acc: 64.49, train_fscore: 63.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.94320011138916, test_acc: 63.03, test_fscore: 62.6, time: 6.04 sec
loss: 4.855932712554932
loss: 4.940671920776367
loss: 4.84410285949707
epoch: 18, train_loss: 4.895400047302246, train_acc: 64.84, train_fscore: 63.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.844099998474121, test_acc: 62.97, test_fscore: 63.22, time: 6.48 sec
loss: 4.910073280334473
loss: 4.638297080993652
loss: 4.75127649307251
epoch: 19, train_loss: 4.784800052642822, train_acc: 64.92, train_fscore: 64.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.751299858093262, test_acc: 65.56, test_fscore: 65.88, time: 5.87 sec
loss: 4.761126518249512
loss: 4.5793137550354
loss: 4.629905700683594
epoch: 20, train_loss: 4.679699897766113, train_acc: 65.83, train_fscore: 65.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.629899978637695, test_acc: 65.87, test_fscore: 65.85, time: 5.98 sec
              precision    recall  f1-score   support

           0     0.3778    0.4722    0.4198     144.0
           1     0.7655    0.7061    0.7346     245.0
           2     0.6471    0.5729    0.6077     384.0
           3     0.5913    0.7235    0.6508     170.0
           4     0.8198    0.7759    0.7973     299.0
           5     0.6425    0.6509    0.6467     381.0

    accuracy                         0.6556    1623.0
   macro avg     0.6407    0.6503    0.6428    1623.0
weighted avg     0.6660    0.6556    0.6588    1623.0

[[ 68.  12.  15.   3.  44.   2.]
 [  3. 173.  36.   1.   0.  32.]
 [ 51.  25. 220.  23.   5.  60.]
 [  0.   0.   4. 123.   0.  43.]
 [ 54.   0.  12.   0. 232.   1.]
 [  4.  16.  53.  58.   2. 248.]]
loss: 4.491458892822266
loss: 4.710320949554443
loss: 4.540910243988037
epoch: 21, train_loss: 4.593800067901611, train_acc: 66.09, train_fscore: 65.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.540900230407715, test_acc: 64.51, test_fscore: 64.68, time: 5.88 sec
loss: 4.5057501792907715
loss: 4.5930867195129395
loss: 4.477451324462891
epoch: 22, train_loss: 4.546000003814697, train_acc: 64.89, train_fscore: 64.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.477499961853027, test_acc: 64.88, test_fscore: 65.26, time: 5.75 sec
loss: 4.392218589782715
loss: 4.566751480102539
loss: 4.430252552032471
epoch: 23, train_loss: 4.474999904632568, train_acc: 65.9, train_fscore: 65.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.430300235748291, test_acc: 63.96, test_fscore: 63.9, time: 6.07 sec
loss: 4.3013410568237305
loss: 4.508238792419434
loss: 4.406766891479492
epoch: 24, train_loss: 4.406400203704834, train_acc: 66.2, train_fscore: 65.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.406799793243408, test_acc: 62.91, test_fscore: 63.58, time: 5.78 sec
loss: 4.346019268035889
loss: 4.356566429138184
loss: 4.3722100257873535
epoch: 25, train_loss: 4.3506999015808105, train_acc: 66.88, train_fscore: 66.15, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.372200012207031, test_acc: 65.19, test_fscore: 65.19, time: 5.92 sec
loss: 4.109189510345459
loss: 4.461396217346191
loss: 4.345578193664551
epoch: 26, train_loss: 4.275199890136719, train_acc: 66.99, train_fscore: 66.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.345600128173828, test_acc: 65.87, test_fscore: 65.9, time: 7.19 sec
loss: 4.299864292144775
loss: 4.1799092292785645
loss: 4.309273719787598
epoch: 27, train_loss: 4.2469000816345215, train_acc: 67.31, train_fscore: 66.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.309299945831299, test_acc: 65.19, test_fscore: 65.65, time: 6.44 sec
loss: 4.1348466873168945
loss: 4.235168933868408
loss: 4.250150680541992
epoch: 28, train_loss: 4.182000160217285, train_acc: 68.16, train_fscore: 67.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.250199794769287, test_acc: 66.11, test_fscore: 65.7, time: 6.04 sec
loss: 4.215839862823486
loss: 4.095811367034912
loss: 4.227283954620361
epoch: 29, train_loss: 4.160600185394287, train_acc: 68.36, train_fscore: 67.51, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.22730016708374, test_acc: 66.3, test_fscore: 66.55, time: 6.35 sec
loss: 4.093471050262451
loss: 4.130580425262451
loss: 4.200971603393555
epoch: 30, train_loss: 4.110899925231934, train_acc: 68.49, train_fscore: 67.89, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.201000213623047, test_acc: 66.05, test_fscore: 66.33, time: 6.4 sec
              precision    recall  f1-score   support

           0     0.4194    0.5417    0.4727     144.0
           1     0.7273    0.7184    0.7228     245.0
           2     0.6697    0.5703    0.6160     384.0
           3     0.5767    0.7294    0.6442     170.0
           4     0.8427    0.8060    0.8239     299.0
           5     0.6485    0.6247    0.6364     381.0

    accuracy                         0.6630    1623.0
   macro avg     0.6474    0.6651    0.6527    1623.0
weighted avg     0.6733    0.6630    0.6655    1623.0

[[ 78.  11.  13.   4.  36.   2.]
 [  6. 176.  31.   0.   0.  32.]
 [ 48.  32. 219.  27.   6.  52.]
 [  0.   0.   4. 124.   0.  42.]
 [ 49.   0.   8.   0. 241.   1.]
 [  5.  23.  52.  60.   3. 238.]]
loss: 4.041712284088135
loss: 4.107148170471191
loss: 4.179802894592285
epoch: 31, train_loss: 4.071300029754639, train_acc: 69.28, train_fscore: 68.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.179800033569336, test_acc: 67.28, test_fscore: 67.4, time: 6.56 sec
loss: 4.172717094421387
loss: 3.864875078201294
loss: 4.156798362731934
epoch: 32, train_loss: 4.0355000495910645, train_acc: 69.57, train_fscore: 69.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.156799793243408, test_acc: 67.22, test_fscore: 67.48, time: 5.61 sec
loss: 3.9516735076904297
loss: 3.977402925491333
loss: 4.107480525970459
epoch: 33, train_loss: 3.96370005607605, train_acc: 70.02, train_fscore: 69.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.107500076293945, test_acc: 67.65, test_fscore: 67.99, time: 6.7 sec
loss: 3.919192314147949
loss: 3.9283721446990967
loss: 4.075509071350098
epoch: 34, train_loss: 3.9235999584198, train_acc: 70.33, train_fscore: 69.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.075500011444092, test_acc: 68.02, test_fscore: 68.15, time: 6.13 sec
loss: 3.8912880420684814
loss: 3.885326623916626
loss: 4.0200324058532715
epoch: 35, train_loss: 3.888700008392334, train_acc: 69.9, train_fscore: 69.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.019999980926514, test_acc: 67.16, test_fscore: 67.52, time: 6.32 sec
loss: 3.9342732429504395
loss: 3.768183946609497
loss: 3.988584518432617
epoch: 36, train_loss: 3.858799934387207, train_acc: 70.48, train_fscore: 70.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.988600015640259, test_acc: 67.41, test_fscore: 67.73, time: 5.9 sec
loss: 3.773179292678833
loss: 3.842000961303711
loss: 3.963219165802002
epoch: 37, train_loss: 3.804500102996826, train_acc: 70.72, train_fscore: 70.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.963200092315674, test_acc: 68.45, test_fscore: 68.48, time: 6.23 sec
loss: 3.706881523132324
loss: 3.863201141357422
loss: 3.9370875358581543
epoch: 38, train_loss: 3.7776999473571777, train_acc: 71.31, train_fscore: 70.89, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9370999336242676, test_acc: 68.15, test_fscore: 68.43, time: 5.73 sec
loss: 3.69907283782959
loss: 3.792205333709717
loss: 3.9027762413024902
epoch: 39, train_loss: 3.740600109100342, train_acc: 72.43, train_fscore: 72.09, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9028000831604004, test_acc: 68.08, test_fscore: 68.39, time: 5.86 sec
loss: 3.7143335342407227
loss: 3.6940886974334717
loss: 3.883439540863037
epoch: 40, train_loss: 3.704900026321411, train_acc: 71.98, train_fscore: 71.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8833999633789062, test_acc: 68.45, test_fscore: 68.64, time: 6.73 sec
              precision    recall  f1-score   support

           0     0.4706    0.5556    0.5096     144.0
           1     0.7822    0.7184    0.7489     245.0
           2     0.6851    0.6458    0.6649     384.0
           3     0.6077    0.7471    0.6702     170.0
           4     0.8191    0.8027    0.8108     299.0
           5     0.6593    0.6299    0.6443     381.0

    accuracy                         0.6845    1623.0
   macro avg     0.6707    0.6832    0.6748    1623.0
weighted avg     0.6913    0.6845    0.6864    1623.0

[[ 80.   9.  17.   4.  33.   1.]
 [  5. 176.  27.   0.   0.  37.]
 [ 35.  22. 248.  22.  13.  44.]
 [  0.   0.   2. 127.   0.  41.]
 [ 50.   0.   8.   0. 240.   1.]
 [  0.  18.  60.  56.   7. 240.]]
loss: 3.7053656578063965
loss: 3.6683754920959473
loss: 3.8688371181488037
epoch: 41, train_loss: 3.687999963760376, train_acc: 73.15, train_fscore: 72.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.868799924850464, test_acc: 68.27, test_fscore: 68.64, time: 6.12 sec
loss: 3.451624631881714
loss: 3.854246139526367
loss: 3.838214159011841
epoch: 42, train_loss: 3.6308000087738037, train_acc: 72.03, train_fscore: 71.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.838200092315674, test_acc: 68.33, test_fscore: 68.64, time: 6.65 sec
loss: 3.5769119262695312
loss: 3.6345274448394775
loss: 3.8150558471679688
epoch: 43, train_loss: 3.603300094604492, train_acc: 72.6, train_fscore: 72.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8150999546051025, test_acc: 68.39, test_fscore: 68.66, time: 6.45 sec
loss: 3.5396084785461426
loss: 3.6262784004211426
loss: 3.762369155883789
epoch: 44, train_loss: 3.5790998935699463, train_acc: 73.05, train_fscore: 72.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.762399911880493, test_acc: 68.27, test_fscore: 68.59, time: 5.95 sec
loss: 3.6718053817749023
loss: 3.4144930839538574
loss: 3.7349205017089844
epoch: 45, train_loss: 3.549499988555908, train_acc: 72.86, train_fscore: 72.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7348999977111816, test_acc: 68.45, test_fscore: 68.72, time: 6.87 sec
loss: 3.596799612045288
loss: 3.462843894958496
loss: 3.7224416732788086
epoch: 46, train_loss: 3.5306999683380127, train_acc: 73.41, train_fscore: 73.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.722399950027466, test_acc: 69.07, test_fscore: 69.25, time: 5.5 sec
loss: 3.406615734100342
loss: 3.619694709777832
loss: 3.7204766273498535
epoch: 47, train_loss: 3.5041000843048096, train_acc: 73.44, train_fscore: 73.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7204999923706055, test_acc: 68.64, test_fscore: 69.0, time: 6.28 sec
loss: 3.415370464324951
loss: 3.5598948001861572
loss: 3.6830978393554688
epoch: 48, train_loss: 3.482800006866455, train_acc: 73.7, train_fscore: 73.47, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6830999851226807, test_acc: 68.76, test_fscore: 69.08, time: 6.0 sec
loss: 3.5731430053710938
loss: 3.3154852390289307
loss: 3.669490337371826
epoch: 49, train_loss: 3.4572999477386475, train_acc: 73.67, train_fscore: 73.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6695001125335693, test_acc: 69.01, test_fscore: 69.33, time: 6.37 sec
loss: 3.3401646614074707
loss: 3.4743599891662598
loss: 3.670715808868408
epoch: 50, train_loss: 3.401900053024292, train_acc: 74.11, train_fscore: 73.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6707000732421875, test_acc: 68.82, test_fscore: 69.16, time: 5.63 sec
              precision    recall  f1-score   support

           0     0.4663    0.6250    0.5341     144.0
           1     0.7870    0.7388    0.7621     245.0
           2     0.7064    0.6328    0.6676     384.0
           3     0.6190    0.7647    0.6842     170.0
           4     0.8400    0.7726    0.8049     299.0
           5     0.6604    0.6430    0.6516     381.0

    accuracy                         0.6901    1623.0
   macro avg     0.6798    0.6962    0.6841    1623.0
weighted avg     0.7019    0.6901    0.6933    1623.0

[[ 90.   9.  16.   1.  27.   1.]
 [  5. 181.  22.   0.   0.  37.]
 [ 39.  21. 243.  22.  10.  49.]
 [  0.   0.   2. 130.   0.  38.]
 [ 59.   0.   8.   0. 231.   1.]
 [  0.  19.  53.  57.   7. 245.]]
loss: 3.5030593872070312
loss: 3.2980263233184814
loss: 3.6441242694854736
epoch: 51, train_loss: 3.4075000286102295, train_acc: 74.01, train_fscore: 73.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6440999507904053, test_acc: 68.95, test_fscore: 69.22, time: 6.47 sec
loss: 3.31882905960083
loss: 3.4376091957092285
loss: 3.642753839492798
epoch: 52, train_loss: 3.378200054168701, train_acc: 74.65, train_fscore: 74.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6428000926971436, test_acc: 68.82, test_fscore: 69.2, time: 4.5 sec
loss: 3.521491527557373
loss: 3.1692299842834473
loss: 3.6274285316467285
epoch: 53, train_loss: 3.360300064086914, train_acc: 74.65, train_fscore: 74.44, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6273999214172363, test_acc: 69.07, test_fscore: 69.49, time: 6.23 sec
loss: 3.267733097076416
loss: 3.406059980392456
loss: 3.5976366996765137
epoch: 54, train_loss: 3.3345999717712402, train_acc: 74.84, train_fscore: 74.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.597599983215332, test_acc: 68.82, test_fscore: 69.2, time: 5.75 sec
loss: 3.344346046447754
loss: 3.2685041427612305
loss: 3.5701138973236084
epoch: 55, train_loss: 3.309499979019165, train_acc: 75.18, train_fscore: 74.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5701000690460205, test_acc: 69.44, test_fscore: 69.78, time: 6.3 sec
loss: 3.2122373580932617
loss: 3.3507161140441895
loss: 3.555534839630127
epoch: 56, train_loss: 3.276099920272827, train_acc: 75.22, train_fscore: 74.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.555500030517578, test_acc: 69.07, test_fscore: 69.43, time: 5.94 sec
loss: 3.209442615509033
loss: 3.307741641998291
loss: 3.560966730117798
epoch: 57, train_loss: 3.255500078201294, train_acc: 75.13, train_fscore: 74.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.561000108718872, test_acc: 69.44, test_fscore: 69.79, time: 5.96 sec
loss: 3.263540029525757
loss: 3.2285382747650146
loss: 3.5397775173187256
epoch: 58, train_loss: 3.247299909591675, train_acc: 75.42, train_fscore: 75.24, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.539799928665161, test_acc: 69.13, test_fscore: 69.52, time: 7.0 sec
loss: 3.1400632858276367
loss: 3.3245925903320312
loss: 3.515202522277832
epoch: 59, train_loss: 3.235100030899048, train_acc: 75.89, train_fscore: 75.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.515199899673462, test_acc: 69.38, test_fscore: 69.8, time: 5.56 sec
loss: 3.432749032974243
loss: 2.9121477603912354
loss: 3.5159523487091064
epoch: 60, train_loss: 3.2060000896453857, train_acc: 75.39, train_fscore: 75.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5160000324249268, test_acc: 69.69, test_fscore: 70.03, time: 5.97 sec
              precision    recall  f1-score   support

           0     0.4796    0.6528    0.5529     144.0
           1     0.8044    0.7388    0.7702     245.0
           2     0.7110    0.6536    0.6811     384.0
           3     0.6172    0.7588    0.6807     170.0
           4     0.8467    0.7759    0.8098     299.0
           5     0.6667    0.6404    0.6533     381.0

    accuracy                         0.6969    1623.0
   macro avg     0.6876    0.7034    0.6913    1623.0
weighted avg     0.7094    0.6969    0.7003    1623.0

[[ 94.   8.  15.   1.  26.   0.]
 [  5. 181.  22.   0.   0.  37.]
 [ 40.  18. 251.  21.  10.  44.]
 [  0.   0.   1. 129.   0.  40.]
 [ 57.   1.   8.   0. 232.   1.]
 [  0.  17.  56.  58.   6. 244.]]
loss: 3.2681617736816406
loss: 3.1024556159973145
loss: 3.478543758392334
epoch: 61, train_loss: 3.186199903488159, train_acc: 76.21, train_fscore: 76.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4784998893737793, test_acc: 69.13, test_fscore: 69.5, time: 6.49 sec
loss: 3.1273350715637207
loss: 3.249133586883545
loss: 3.4847464561462402
epoch: 62, train_loss: 3.180999994277954, train_acc: 76.2, train_fscore: 75.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4846999645233154, test_acc: 69.93, test_fscore: 70.38, time: 6.67 sec
loss: 3.1813528537750244
loss: 3.141402006149292
loss: 3.4810256958007812
epoch: 63, train_loss: 3.1633999347686768, train_acc: 76.51, train_fscore: 76.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4809999465942383, test_acc: 69.32, test_fscore: 69.75, time: 6.04 sec
loss: 3.2082207202911377
loss: 3.085977792739868
loss: 3.4817824363708496
epoch: 64, train_loss: 3.15120005607605, train_acc: 76.56, train_fscore: 76.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.481800079345703, test_acc: 69.69, test_fscore: 70.05, time: 5.3 sec
loss: 3.1679227352142334
loss: 3.076465606689453
loss: 3.4615721702575684
epoch: 65, train_loss: 3.124300003051758, train_acc: 76.87, train_fscore: 76.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4616000652313232, test_acc: 69.38, test_fscore: 69.81, time: 6.51 sec
loss: 3.0630247592926025
loss: 3.1842150688171387
loss: 3.4270544052124023
epoch: 66, train_loss: 3.1191000938415527, train_acc: 76.33, train_fscore: 76.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4270999431610107, test_acc: 69.75, test_fscore: 70.12, time: 6.28 sec
loss: 3.357651710510254
loss: 2.802845001220703
loss: 3.4605724811553955
epoch: 67, train_loss: 3.096400022506714, train_acc: 77.04, train_fscore: 76.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.460599899291992, test_acc: 70.06, test_fscore: 70.47, time: 6.38 sec
loss: 2.980738878250122
loss: 3.195854902267456
loss: 3.4567718505859375
epoch: 68, train_loss: 3.079900026321411, train_acc: 77.4, train_fscore: 77.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4567999839782715, test_acc: 70.06, test_fscore: 70.49, time: 6.24 sec
loss: 3.1535325050354004
loss: 3.0119051933288574
loss: 3.436688184738159
epoch: 69, train_loss: 3.0892999172210693, train_acc: 77.06, train_fscore: 76.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4367001056671143, test_acc: 70.55, test_fscore: 70.85, time: 6.53 sec
loss: 3.1457550525665283
loss: 2.9395029544830322
loss: 3.422347068786621
epoch: 70, train_loss: 3.052000045776367, train_acc: 77.71, train_fscore: 77.57, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.422300100326538, test_acc: 70.43, test_fscore: 70.74, time: 6.01 sec
              precision    recall  f1-score   support

           0     0.5026    0.6806    0.5782     144.0
           1     0.8070    0.7510    0.7780     245.0
           2     0.7205    0.6510    0.6840     384.0
           3     0.6324    0.7588    0.6898     170.0
           4     0.8509    0.7826    0.8153     299.0
           5     0.6684    0.6562    0.6623     381.0

    accuracy                         0.7055    1623.0
   macro avg     0.6970    0.7134    0.7013    1623.0
weighted avg     0.7168    0.7055    0.7085    1623.0

[[ 98.   9.  13.   0.  24.   0.]
 [  2. 184.  22.   0.   0.  37.]
 [ 40.  18. 250.  20.  10.  46.]
 [  0.   0.   1. 129.   0.  40.]
 [ 55.   1.   8.   0. 234.   1.]
 [  0.  16.  53.  55.   7. 250.]]
loss: 2.9889538288116455
loss: 3.059828996658325
loss: 3.395108938217163
epoch: 71, train_loss: 3.020900011062622, train_acc: 77.42, train_fscore: 77.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3951001167297363, test_acc: 70.49, test_fscore: 70.78, time: 5.73 sec
loss: 2.8582801818847656
loss: 3.2365713119506836
loss: 3.4226865768432617
epoch: 72, train_loss: 3.0322000980377197, train_acc: 77.57, train_fscore: 77.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4226999282836914, test_acc: 70.86, test_fscore: 71.16, time: 6.55 sec
loss: 2.899526596069336
loss: 3.1939284801483154
loss: 3.423369884490967
epoch: 73, train_loss: 3.0348000526428223, train_acc: 78.09, train_fscore: 77.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4233999252319336, test_acc: 70.73, test_fscore: 71.1, time: 6.1 sec
loss: 3.029301643371582
loss: 2.973695755004883
loss: 3.393666982650757
epoch: 74, train_loss: 3.0037999153137207, train_acc: 78.19, train_fscore: 78.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.393699884414673, test_acc: 70.3, test_fscore: 70.72, time: 5.98 sec
loss: 3.032289981842041
loss: 2.946258544921875
loss: 3.3851003646850586
epoch: 75, train_loss: 2.994999885559082, train_acc: 77.95, train_fscore: 77.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3850998878479004, test_acc: 70.86, test_fscore: 71.2, time: 6.29 sec
loss: 2.999162197113037
loss: 2.9399514198303223
loss: 3.3829522132873535
epoch: 76, train_loss: 2.9721999168395996, train_acc: 78.14, train_fscore: 78.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.382999897003174, test_acc: 70.55, test_fscore: 70.91, time: 6.18 sec
loss: 2.9573779106140137
loss: 2.931222915649414
loss: 3.3736722469329834
epoch: 77, train_loss: 2.9453999996185303, train_acc: 78.35, train_fscore: 78.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.373699903488159, test_acc: 70.36, test_fscore: 70.73, time: 6.24 sec
loss: 3.006701707839966
loss: 2.875044345855713
loss: 3.3772263526916504
epoch: 78, train_loss: 2.9407999515533447, train_acc: 78.74, train_fscore: 78.59, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.37719988822937, test_acc: 70.73, test_fscore: 71.06, time: 6.36 sec
loss: 2.846027135848999
loss: 3.017367124557495
loss: 3.379897117614746
epoch: 79, train_loss: 2.925100088119507, train_acc: 78.93, train_fscore: 78.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3798999786376953, test_acc: 70.55, test_fscore: 70.97, time: 6.5 sec
loss: 2.972161054611206
loss: 2.82096004486084
loss: 3.366001844406128
epoch: 80, train_loss: 2.9005000591278076, train_acc: 78.81, train_fscore: 78.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.365999937057495, test_acc: 70.86, test_fscore: 71.22, time: 5.97 sec
              precision    recall  f1-score   support

           0     0.5047    0.7500    0.6034     144.0
           1     0.8095    0.7633    0.7857     245.0
           2     0.7273    0.6667    0.6957     384.0
           3     0.6287    0.7471    0.6828     170.0
           4     0.8692    0.7559    0.8086     299.0
           5     0.6758    0.6457    0.6604     381.0

    accuracy                         0.7086    1623.0
   macro avg     0.7025    0.7214    0.7061    1623.0
weighted avg     0.7237    0.7086    0.7122    1623.0

[[108.   7.  10.   0.  19.   0.]
 [  2. 187.  23.   0.   0.  33.]
 [ 41.  18. 256.  19.   8.  42.]
 [  0.   0.   1. 127.   0.  42.]
 [ 62.   1.   9.   0. 226.   1.]
 [  1.  18.  53.  56.   7. 246.]]
loss: 2.948256731033325
loss: 2.8473098278045654
loss: 3.3564577102661133
epoch: 81, train_loss: 2.9047000408172607, train_acc: 79.33, train_fscore: 79.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3564999103546143, test_acc: 70.79, test_fscore: 71.18, time: 5.77 sec
loss: 2.9236176013946533
loss: 2.8418779373168945
loss: 3.3578155040740967
epoch: 82, train_loss: 2.8852999210357666, train_acc: 79.02, train_fscore: 78.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.357800006866455, test_acc: 70.61, test_fscore: 70.98, time: 6.62 sec
loss: 2.8607470989227295
loss: 2.8869781494140625
loss: 3.3714301586151123
epoch: 83, train_loss: 2.873300075531006, train_acc: 79.26, train_fscore: 79.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3714001178741455, test_acc: 71.23, test_fscore: 71.52, time: 6.32 sec
loss: 2.852114677429199
loss: 2.8933756351470947
loss: 3.3557040691375732
epoch: 84, train_loss: 2.870800018310547, train_acc: 79.02, train_fscore: 78.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3557000160217285, test_acc: 70.79, test_fscore: 71.14, time: 6.4 sec
loss: 3.000392198562622
loss: 2.670401096343994
loss: 3.342660427093506
epoch: 85, train_loss: 2.853600025177002, train_acc: 79.72, train_fscore: 79.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3427000045776367, test_acc: 70.67, test_fscore: 71.01, time: 6.27 sec
loss: 2.6918740272521973
loss: 2.9718871116638184
loss: 3.3330023288726807
epoch: 86, train_loss: 2.827399969100952, train_acc: 80.19, train_fscore: 80.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3329999446868896, test_acc: 70.86, test_fscore: 71.16, time: 6.03 sec
loss: 2.9811697006225586
loss: 2.6863961219787598
loss: 3.32039737701416
epoch: 87, train_loss: 2.8408000469207764, train_acc: 79.4, train_fscore: 79.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3203999996185303, test_acc: 70.86, test_fscore: 71.22, time: 6.96 sec
loss: 2.8572850227355957
loss: 2.753326416015625
loss: 3.3045692443847656
epoch: 88, train_loss: 2.809999942779541, train_acc: 79.62, train_fscore: 79.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3046000003814697, test_acc: 71.16, test_fscore: 71.5, time: 5.85 sec
loss: 2.6793270111083984
loss: 2.9090769290924072
loss: 3.324246883392334
epoch: 89, train_loss: 2.7920000553131104, train_acc: 79.98, train_fscore: 79.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.324199914932251, test_acc: 71.1, test_fscore: 71.4, time: 6.38 sec
loss: 2.688443660736084
loss: 2.887347936630249
loss: 3.340111017227173
epoch: 90, train_loss: 2.781899929046631, train_acc: 80.34, train_fscore: 80.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.340100049972534, test_acc: 70.86, test_fscore: 71.26, time: 6.46 sec
              precision    recall  f1-score   support

           0     0.5176    0.7153    0.6006     144.0
           1     0.8150    0.7551    0.7839     245.0
           2     0.7171    0.6667    0.6910     384.0
           3     0.6580    0.7471    0.6997     170.0
           4     0.8524    0.7726    0.8105     299.0
           5     0.6755    0.6667    0.6711     381.0

    accuracy                         0.7123    1623.0
   macro avg     0.7059    0.7206    0.7095    1623.0
weighted avg     0.7232    0.7123    0.7152    1623.0

[[103.   7.  12.   0.  22.   0.]
 [  2. 185.  23.   0.   1.  34.]
 [ 38.  18. 256.  17.  10.  45.]
 [  0.   0.   1. 127.   0.  42.]
 [ 55.   1.  11.   0. 231.   1.]
 [  1.  16.  54.  49.   7. 254.]]
loss: 2.8052849769592285
loss: 2.75144100189209
loss: 3.30619740486145
epoch: 91, train_loss: 2.7813000679016113, train_acc: 80.29, train_fscore: 80.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3062000274658203, test_acc: 70.92, test_fscore: 71.23, time: 5.4 sec
loss: 2.8249197006225586
loss: 2.7153687477111816
loss: 3.3003721237182617
epoch: 92, train_loss: 2.7737998962402344, train_acc: 80.22, train_fscore: 80.09, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3004000186920166, test_acc: 70.86, test_fscore: 71.23, time: 6.08 sec
loss: 2.71614408493042
loss: 2.797881603240967
loss: 3.3314857482910156
epoch: 93, train_loss: 2.754699945449829, train_acc: 80.05, train_fscore: 79.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3315000534057617, test_acc: 70.86, test_fscore: 71.24, time: 6.46 sec
loss: 2.7703261375427246
loss: 2.7132976055145264
loss: 3.3257529735565186
epoch: 94, train_loss: 2.7446000576019287, train_acc: 80.81, train_fscore: 80.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3257999420166016, test_acc: 71.23, test_fscore: 71.52, time: 6.01 sec
loss: 2.8984084129333496
loss: 2.586989402770996
loss: 3.3060851097106934
epoch: 95, train_loss: 2.752500057220459, train_acc: 80.41, train_fscore: 80.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3060998916625977, test_acc: 71.04, test_fscore: 71.43, time: 6.01 sec
loss: 2.6920275688171387
loss: 2.7676641941070557
loss: 3.2709569931030273
epoch: 96, train_loss: 2.7272000312805176, train_acc: 81.08, train_fscore: 80.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2709999084472656, test_acc: 71.23, test_fscore: 71.56, time: 6.66 sec
loss: 2.7294113636016846
loss: 2.7105488777160645
loss: 3.2960660457611084
epoch: 97, train_loss: 2.721100091934204, train_acc: 81.14, train_fscore: 81.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.296099901199341, test_acc: 71.47, test_fscore: 71.82, time: 5.85 sec
loss: 2.779865264892578
loss: 2.623162269592285
loss: 3.325562000274658
epoch: 98, train_loss: 2.7083001136779785, train_acc: 81.19, train_fscore: 81.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3255999088287354, test_acc: 71.29, test_fscore: 71.7, time: 5.96 sec
loss: 2.6977381706237793
loss: 2.6985983848571777
loss: 3.2786340713500977
epoch: 99, train_loss: 2.6981000900268555, train_acc: 80.84, train_fscore: 80.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.278599977493286, test_acc: 71.66, test_fscore: 71.89, time: 5.92 sec
loss: 2.74208402633667
loss: 2.611107110977173
loss: 3.301701545715332
epoch: 100, train_loss: 2.681999921798706, train_acc: 81.07, train_fscore: 80.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3017001152038574, test_acc: 71.35, test_fscore: 71.72, time: 6.66 sec
              precision    recall  f1-score   support

           0     0.5459    0.7431    0.6294     144.0
           1     0.8333    0.7551    0.7923     245.0
           2     0.7275    0.6745    0.7000     384.0
           3     0.6179    0.7706    0.6859     170.0
           4     0.8464    0.7926    0.8187     299.0
           5     0.6835    0.6404    0.6612     381.0

    accuracy                         0.7166    1623.0
   macro avg     0.7091    0.7294    0.7146    1623.0
weighted avg     0.7275    0.7166    0.7189    1623.0

[[107.   5.   8.   0.  24.   0.]
 [  2. 185.  23.   2.   1.  32.]
 [ 38.  17. 259.  18.  10.  42.]
 [  0.   0.   1. 131.   0.  38.]
 [ 48.   1.  12.   0. 237.   1.]
 [  1.  14.  53.  61.   8. 244.]]
loss: 2.6874990463256836
loss: 2.6684370040893555
loss: 3.321795701980591
epoch: 101, train_loss: 2.6791000366210938, train_acc: 81.36, train_fscore: 81.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3217999935150146, test_acc: 70.79, test_fscore: 71.16, time: 5.57 sec
loss: 2.4918558597564697
loss: 2.878603935241699
loss: 3.313844919204712
epoch: 102, train_loss: 2.669100046157837, train_acc: 81.46, train_fscore: 81.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.313800096511841, test_acc: 71.29, test_fscore: 71.61, time: 6.84 sec
loss: 2.775345802307129
loss: 2.524949550628662
loss: 3.3378512859344482
epoch: 103, train_loss: 2.658799886703491, train_acc: 81.39, train_fscore: 81.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.337899923324585, test_acc: 70.67, test_fscore: 71.04, time: 6.16 sec
loss: 2.741964101791382
loss: 2.56423282623291
loss: 3.3068833351135254
epoch: 104, train_loss: 2.6556999683380127, train_acc: 81.51, train_fscore: 81.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3069000244140625, test_acc: 71.04, test_fscore: 71.41, time: 6.04 sec
loss: 2.645902633666992
loss: 2.604090452194214
loss: 3.2882139682769775
epoch: 105, train_loss: 2.6268999576568604, train_acc: 81.74, train_fscore: 81.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2881999015808105, test_acc: 71.23, test_fscore: 71.56, time: 6.08 sec
loss: 2.6292130947113037
loss: 2.6400022506713867
loss: 3.3001415729522705
epoch: 106, train_loss: 2.634399890899658, train_acc: 82.0, train_fscore: 81.9, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.300100088119507, test_acc: 70.86, test_fscore: 71.2, time: 6.83 sec
loss: 2.3742356300354004
loss: 2.9093475341796875
loss: 3.3011040687561035
epoch: 107, train_loss: 2.622499942779541, train_acc: 82.08, train_fscore: 81.97, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.301100015640259, test_acc: 70.67, test_fscore: 71.03, time: 5.23 sec
loss: 2.5997567176818848
loss: 2.6494545936584473
loss: 3.292670488357544
epoch: 108, train_loss: 2.623199939727783, train_acc: 82.17, train_fscore: 82.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2927000522613525, test_acc: 71.1, test_fscore: 71.46, time: 6.35 sec
loss: 2.509657144546509
loss: 2.6987133026123047
loss: 3.2859232425689697
epoch: 109, train_loss: 2.598599910736084, train_acc: 82.1, train_fscore: 81.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.285900115966797, test_acc: 71.35, test_fscore: 71.71, time: 6.23 sec
loss: 2.7169668674468994
loss: 2.4152772426605225
loss: 3.3025412559509277
epoch: 110, train_loss: 2.574399948120117, train_acc: 82.12, train_fscore: 82.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.302500009536743, test_acc: 71.04, test_fscore: 71.36, time: 6.36 sec
              precision    recall  f1-score   support

           0     0.5459    0.7431    0.6294     144.0
           1     0.8333    0.7551    0.7923     245.0
           2     0.7275    0.6745    0.7000     384.0
           3     0.6179    0.7706    0.6859     170.0
           4     0.8464    0.7926    0.8187     299.0
           5     0.6835    0.6404    0.6612     381.0

    accuracy                         0.7166    1623.0
   macro avg     0.7091    0.7294    0.7146    1623.0
weighted avg     0.7275    0.7166    0.7189    1623.0

[[107.   5.   8.   0.  24.   0.]
 [  2. 185.  23.   2.   1.  32.]
 [ 38.  17. 259.  18.  10.  42.]
 [  0.   0.   1. 131.   0.  38.]
 [ 48.   1.  12.   0. 237.   1.]
 [  1.  14.  53.  61.   8. 244.]]
loss: 2.4651412963867188
loss: 2.7115306854248047
loss: 3.282954454421997
epoch: 111, train_loss: 2.5796000957489014, train_acc: 82.38, train_fscore: 82.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2829999923706055, test_acc: 71.66, test_fscore: 72.02, time: 6.24 sec
loss: 2.684739351272583
loss: 2.4744014739990234
loss: 3.3022494316101074
epoch: 112, train_loss: 2.5820000171661377, train_acc: 82.19, train_fscore: 82.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3022000789642334, test_acc: 71.66, test_fscore: 72.02, time: 6.81 sec
loss: 2.5240156650543213
loss: 2.5704901218414307
loss: 3.3330764770507812
epoch: 113, train_loss: 2.545300006866455, train_acc: 82.63, train_fscore: 82.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3331000804901123, test_acc: 71.6, test_fscore: 72.0, time: 5.45 sec
loss: 2.6466143131256104
loss: 2.4321539402008057
loss: 3.299462080001831
epoch: 114, train_loss: 2.552799940109253, train_acc: 82.67, train_fscore: 82.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.299499988555908, test_acc: 71.1, test_fscore: 71.44, time: 6.73 sec
loss: 2.615841865539551
loss: 2.468282461166382
loss: 3.2929744720458984
epoch: 115, train_loss: 2.5469000339508057, train_acc: 82.72, train_fscore: 82.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2929999828338623, test_acc: 71.6, test_fscore: 71.95, time: 6.29 sec
loss: 2.484138011932373
loss: 2.613187789916992
loss: 3.3101441860198975
epoch: 116, train_loss: 2.5427000522613525, train_acc: 82.99, train_fscore: 82.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3101000785827637, test_acc: 71.6, test_fscore: 71.96, time: 6.31 sec
loss: 2.4375336170196533
loss: 2.647629737854004
loss: 3.3110342025756836
epoch: 117, train_loss: 2.54010009765625, train_acc: 83.06, train_fscore: 82.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.311000108718872, test_acc: 71.47, test_fscore: 71.79, time: 5.47 sec
loss: 2.614166498184204
loss: 2.4581644535064697
loss: 3.304231882095337
epoch: 118, train_loss: 2.5432000160217285, train_acc: 82.86, train_fscore: 82.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3041999340057373, test_acc: 71.66, test_fscore: 72.08, time: 6.13 sec
loss: 2.4612278938293457
loss: 2.595465660095215
loss: 3.301746368408203
epoch: 119, train_loss: 2.5257999897003174, train_acc: 82.6, train_fscore: 82.47, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3017001152038574, test_acc: 71.41, test_fscore: 71.78, time: 6.25 sec
loss: 2.4519097805023193
loss: 2.5480947494506836
loss: 3.3141024112701416
epoch: 120, train_loss: 2.4972000122070312, train_acc: 83.13, train_fscore: 83.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3141000270843506, test_acc: 71.47, test_fscore: 71.81, time: 5.72 sec
              precision    recall  f1-score   support

           0     0.5045    0.7847    0.6141     144.0
           1     0.8421    0.7837    0.8118     245.0
           2     0.7442    0.6667    0.7033     384.0
           3     0.6355    0.7588    0.6917     170.0
           4     0.8715    0.7258    0.7920     299.0
           5     0.6827    0.6719    0.6772     381.0

    accuracy                         0.7166    1623.0
   macro avg     0.7134    0.7319    0.7150    1623.0
weighted avg     0.7353    0.7166    0.7208    1623.0

[[113.   4.   6.   0.  21.   0.]
 [  2. 192.  17.   1.   0.  33.]
 [ 42.  18. 256.  17.   6.  45.]
 [  0.   0.   1. 129.   0.  40.]
 [ 66.   0.  15.   0. 217.   1.]
 [  1.  14.  49.  56.   5. 256.]]
loss: 2.456507682800293
loss: 2.593977689743042
loss: 3.297718048095703
epoch: 121, train_loss: 2.5204999446868896, train_acc: 83.08, train_fscore: 82.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2976999282836914, test_acc: 71.6, test_fscore: 72.01, time: 7.01 sec
loss: 2.4349899291992188
loss: 2.5231094360351562
loss: 3.288857936859131
epoch: 122, train_loss: 2.4746999740600586, train_acc: 83.39, train_fscore: 83.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2888998985290527, test_acc: 71.35, test_fscore: 71.72, time: 5.88 sec
loss: 2.483217716217041
loss: 2.4570107460021973
loss: 3.2979753017425537
epoch: 123, train_loss: 2.470599889755249, train_acc: 83.18, train_fscore: 83.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2980000972747803, test_acc: 71.35, test_fscore: 71.64, time: 6.13 sec
loss: 2.404308795928955
loss: 2.5105221271514893
loss: 3.358348846435547
epoch: 124, train_loss: 2.4551000595092773, train_acc: 83.37, train_fscore: 83.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.358299970626831, test_acc: 71.29, test_fscore: 71.75, time: 5.8 sec
loss: 2.4455678462982178
loss: 2.5049781799316406
loss: 3.3293914794921875
epoch: 125, train_loss: 2.4728000164031982, train_acc: 83.32, train_fscore: 83.24, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.329400062561035, test_acc: 71.47, test_fscore: 71.83, time: 6.58 sec
loss: 2.284101724624634
loss: 2.6536245346069336
loss: 3.3053231239318848
epoch: 126, train_loss: 2.464099884033203, train_acc: 83.61, train_fscore: 83.5, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.305299997329712, test_acc: 71.47, test_fscore: 71.83, time: 5.15 sec
loss: 2.415955066680908
loss: 2.459843873977661
loss: 3.2906038761138916
epoch: 127, train_loss: 2.4375, train_acc: 83.55, train_fscore: 83.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.290600061416626, test_acc: 71.66, test_fscore: 72.03, time: 7.17 sec
loss: 2.3389828205108643
loss: 2.5466413497924805
loss: 3.2848799228668213
epoch: 128, train_loss: 2.429500102996826, train_acc: 83.73, train_fscore: 83.63, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.284899950027466, test_acc: 71.6, test_fscore: 71.97, time: 6.29 sec
loss: 2.461392879486084
loss: 2.420753240585327
loss: 3.3067708015441895
epoch: 129, train_loss: 2.442199945449829, train_acc: 83.68, train_fscore: 83.59, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.30679988861084, test_acc: 71.97, test_fscore: 72.27, time: 6.37 sec
loss: 2.3976526260375977
loss: 2.4554457664489746
loss: 3.3219776153564453
epoch: 130, train_loss: 2.4244000911712646, train_acc: 83.94, train_fscore: 83.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.322000026702881, test_acc: 71.53, test_fscore: 71.88, time: 6.1 sec
              precision    recall  f1-score   support

           0     0.5463    0.7778    0.6418     144.0
           1     0.8370    0.7755    0.8051     245.0
           2     0.7280    0.6901    0.7086     384.0
           3     0.6364    0.7412    0.6848     170.0
           4     0.8643    0.7458    0.8007     299.0
           5     0.6792    0.6614    0.6702     381.0

    accuracy                         0.7197    1623.0
   macro avg     0.7152    0.7320    0.7185    1623.0
weighted avg     0.7324    0.7197    0.7227    1623.0

[[112.   4.   5.   0.  22.   1.]
 [  2. 190.  21.   1.   0.  31.]
 [ 35.  17. 265.  16.   8.  43.]
 [  0.   0.   1. 126.   0.  43.]
 [ 56.   1.  18.   0. 223.   1.]
 [  0.  15.  54.  55.   5. 252.]]
loss: 2.4482059478759766
loss: 2.3870558738708496
loss: 3.2794971466064453
epoch: 131, train_loss: 2.4205000400543213, train_acc: 83.68, train_fscore: 83.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2795000076293945, test_acc: 71.72, test_fscore: 72.1, time: 6.48 sec
loss: 2.365793228149414
loss: 2.4680941104888916
loss: 3.2782156467437744
epoch: 132, train_loss: 2.4123001098632812, train_acc: 83.87, train_fscore: 83.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2781999111175537, test_acc: 71.97, test_fscore: 72.34, time: 5.41 sec
loss: 2.336604595184326
loss: 2.4560375213623047
loss: 3.3077242374420166
epoch: 133, train_loss: 2.3919999599456787, train_acc: 84.17, train_fscore: 84.06, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3076999187469482, test_acc: 71.72, test_fscore: 72.07, time: 6.72 sec
loss: 2.4021780490875244
loss: 2.426009178161621
loss: 3.3237502574920654
epoch: 134, train_loss: 2.413599967956543, train_acc: 84.39, train_fscore: 84.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3238000869750977, test_acc: 71.29, test_fscore: 71.59, time: 6.57 sec
loss: 2.336702585220337
loss: 2.4894981384277344
loss: 3.3323569297790527
epoch: 135, train_loss: 2.409600019454956, train_acc: 84.42, train_fscore: 84.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.33240008354187, test_acc: 71.84, test_fscore: 72.2, time: 6.23 sec
loss: 2.5090999603271484
loss: 2.2516937255859375
loss: 3.315300941467285
epoch: 136, train_loss: 2.3866000175476074, train_acc: 83.89, train_fscore: 83.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3152999877929688, test_acc: 71.84, test_fscore: 72.19, time: 6.76 sec
loss: 2.418743848800659
loss: 2.334136962890625
loss: 3.2942771911621094
epoch: 137, train_loss: 2.379699945449829, train_acc: 84.42, train_fscore: 84.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.294300079345703, test_acc: 71.97, test_fscore: 72.29, time: 6.33 sec
loss: 2.428065776824951
loss: 2.3048672676086426
loss: 3.3038082122802734
epoch: 138, train_loss: 2.3714001178741455, train_acc: 84.37, train_fscore: 84.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.303800106048584, test_acc: 71.66, test_fscore: 72.03, time: 6.67 sec
loss: 2.3414926528930664
loss: 2.369445323944092
loss: 3.3181004524230957
epoch: 139, train_loss: 2.3545000553131104, train_acc: 84.6, train_fscore: 84.49, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3180999755859375, test_acc: 71.9, test_fscore: 72.29, time: 6.39 sec
loss: 2.3879261016845703
loss: 2.3045401573181152
loss: 3.3302597999572754
epoch: 140, train_loss: 2.3501999378204346, train_acc: 84.56, train_fscore: 84.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3303000926971436, test_acc: 72.09, test_fscore: 72.49, time: 5.62 sec
              precision    recall  f1-score   support

           0     0.5177    0.8125    0.6324     144.0
           1     0.8407    0.7755    0.8068     245.0
           2     0.7319    0.7109    0.7213     384.0
           3     0.6477    0.7353    0.6887     170.0
           4     0.8866    0.7057    0.7858     299.0
           5     0.6921    0.6667    0.6791     381.0

    accuracy                         0.7209    1623.0
   macro avg     0.7194    0.7344    0.7190    1623.0
weighted avg     0.7396    0.7209    0.7249    1623.0

[[117.   4.   6.   0.  17.   0.]
 [  2. 190.  21.   2.   0.  30.]
 [ 37.  18. 273.  13.   5.  38.]
 [  0.   0.   1. 125.   0.  44.]
 [ 69.   1.  17.   0. 211.   1.]
 [  1.  13.  55.  53.   5. 254.]]
loss: 2.4091038703918457
loss: 2.28045654296875
loss: 3.300811290740967
epoch: 141, train_loss: 2.3468000888824463, train_acc: 84.99, train_fscore: 84.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.300800085067749, test_acc: 71.9, test_fscore: 72.28, time: 6.45 sec
loss: 2.371138095855713
loss: 2.2907190322875977
loss: 3.2952640056610107
epoch: 142, train_loss: 2.334199905395508, train_acc: 84.84, train_fscore: 84.72, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.295300006866455, test_acc: 72.03, test_fscore: 72.38, time: 6.13 sec
loss: 2.283909559249878
loss: 2.336970090866089
loss: 3.3397068977355957
epoch: 143, train_loss: 2.3106000423431396, train_acc: 85.06, train_fscore: 84.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3396999835968018, test_acc: 71.53, test_fscore: 71.92, time: 6.4 sec
loss: 2.232489585876465
loss: 2.415318727493286
loss: 3.3405439853668213
epoch: 144, train_loss: 2.3222999572753906, train_acc: 84.75, train_fscore: 84.66, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3405001163482666, test_acc: 72.09, test_fscore: 72.47, time: 5.86 sec
loss: 2.443068265914917
loss: 2.1654887199401855
loss: 3.3098247051239014
epoch: 145, train_loss: 2.320499897003174, train_acc: 84.97, train_fscore: 84.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.309799909591675, test_acc: 72.09, test_fscore: 72.44, time: 6.3 sec
loss: 2.314746141433716
loss: 2.3158488273620605
loss: 3.3018174171447754
epoch: 146, train_loss: 2.3152999877929688, train_acc: 85.01, train_fscore: 84.89, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.301800012588501, test_acc: 72.34, test_fscore: 72.67, time: 5.93 sec
loss: 2.269108772277832
loss: 2.31598162651062
loss: 3.323392391204834
epoch: 147, train_loss: 2.2908999919891357, train_acc: 85.47, train_fscore: 85.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3234000205993652, test_acc: 72.09, test_fscore: 72.44, time: 6.23 sec
loss: 2.193265199661255
loss: 2.4139509201049805
loss: 3.3571550846099854
epoch: 148, train_loss: 2.301300048828125, train_acc: 85.18, train_fscore: 85.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3571999073028564, test_acc: 71.41, test_fscore: 71.81, time: 6.32 sec
loss: 2.331874370574951
loss: 2.2515006065368652
loss: 3.376345157623291
epoch: 149, train_loss: 2.294300079345703, train_acc: 85.23, train_fscore: 85.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.376300096511841, test_acc: 71.1, test_fscore: 71.5, time: 5.93 sec
loss: 2.28867244720459
loss: 2.247551202774048
loss: 3.3696422576904297
epoch: 150, train_loss: 2.26990008354187, train_acc: 85.66, train_fscore: 85.57, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3696000576019287, test_acc: 71.47, test_fscore: 71.83, time: 5.37 sec
              precision    recall  f1-score   support

           0     0.5337    0.7708    0.6307     144.0
           1     0.8475    0.7714    0.8077     245.0
           2     0.7244    0.7188    0.7216     384.0
           3     0.6414    0.7471    0.6902     170.0
           4     0.8588    0.7324    0.7906     299.0
           5     0.7039    0.6614    0.6820     381.0

    accuracy                         0.7234    1623.0
   macro avg     0.7183    0.7337    0.7205    1623.0
weighted avg     0.7373    0.7234    0.7267    1623.0

[[111.   4.   6.   0.  23.   0.]
 [  2. 189.  22.   3.   0.  29.]
 [ 34.  17. 276.  14.   8.  35.]
 [  0.   0.   2. 127.   0.  41.]
 [ 60.   1.  18.   0. 219.   1.]
 [  1.  12.  57.  54.   5. 252.]]
Test performance..
F-Score: 72.67
F-Score-index: 146
              precision    recall  f1-score   support

           0     0.5337    0.7708    0.6307     144.0
           1     0.8475    0.7714    0.8077     245.0
           2     0.7244    0.7188    0.7216     384.0
           3     0.6414    0.7471    0.6902     170.0
           4     0.8588    0.7324    0.7906     299.0
           5     0.7039    0.6614    0.6820     381.0

    accuracy                         0.7234    1623.0
   macro avg     0.7183    0.7337    0.7205    1623.0
weighted avg     0.7373    0.7234    0.7267    1623.0

[[111.   4.   6.   0.  23.   0.]
 [  2. 189.  22.   3.   0.  29.]
 [ 34.  17. 276.  14.   8.  35.]
 [  0.   0.   2. 127.   0.  41.]
 [ 60.   1.  18.   0. 219.   1.]
 [  1.  12.  57.  54.   5. 252.]]
--- 9 ---
Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.5, batch_size=64, hidden_dim=1024, n_head=8, epochs=150, temp=2, tensorboard=False, class_weight=True, Dataset='IEMOCAP')
Running on GPU
temp 2
total parameters: 79687704
training parameters: 79687704
loss: 7.869516372680664
loss: 11.108393669128418
loss: 8.060613632202148
epoch: 1, train_loss: 9.404199600219727, train_acc: 19.81, train_fscore: 18.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.060600280761719, test_acc: 14.97, test_fscore: 7.86, time: 7.85 sec
loss: 8.329477310180664
loss: 8.2455415725708
loss: 7.947650909423828
epoch: 2, train_loss: 8.288599967956543, train_acc: 23.25, train_fscore: 17.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.947700023651123, test_acc: 17.62, test_fscore: 10.72, time: 7.04 sec
loss: 8.290262222290039
loss: 7.6606903076171875
loss: 6.9843268394470215
epoch: 3, train_loss: 7.977499961853027, train_acc: 29.45, train_fscore: 28.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.984300136566162, test_acc: 48.61, test_fscore: 42.34, time: 6.44 sec
loss: 7.193564414978027
loss: 7.369301795959473
loss: 7.088000774383545
epoch: 4, train_loss: 7.277299880981445, train_acc: 44.44, train_fscore: 38.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.0879998207092285, test_acc: 41.22, test_fscore: 36.27, time: 6.56 sec
loss: 7.313422203063965
loss: 7.142683506011963
loss: 6.797967433929443
epoch: 5, train_loss: 7.233099937438965, train_acc: 41.31, train_fscore: 37.18, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.797999858856201, test_acc: 47.81, test_fscore: 48.88, time: 5.93 sec
loss: 6.91972541809082
loss: 6.806822776794434
loss: 6.620286464691162
epoch: 6, train_loss: 6.868899822235107, train_acc: 48.38, train_fscore: 48.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.620299816131592, test_acc: 41.65, test_fscore: 41.21, time: 6.23 sec
loss: 6.58983850479126
loss: 6.659115791320801
loss: 6.52705717086792
epoch: 7, train_loss: 6.62529993057251, train_acc: 50.4, train_fscore: 48.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.527100086212158, test_acc: 47.5, test_fscore: 47.89, time: 5.88 sec
loss: 6.509591102600098
loss: 6.545982837677002
loss: 6.462653636932373
epoch: 8, train_loss: 6.5269999504089355, train_acc: 54.04, train_fscore: 52.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.462699890136719, test_acc: 51.26, test_fscore: 51.39, time: 6.34 sec
loss: 6.396548271179199
loss: 6.2521185874938965
loss: 6.257414817810059
epoch: 9, train_loss: 6.328400135040283, train_acc: 57.5, train_fscore: 56.71, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.257400035858154, test_acc: 56.25, test_fscore: 53.86, time: 6.65 sec
loss: 6.128438472747803
loss: 6.085826873779297
loss: 6.116976737976074
epoch: 10, train_loss: 6.1082000732421875, train_acc: 60.28, train_fscore: 58.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.117000102996826, test_acc: 56.32, test_fscore: 55.43, time: 5.85 sec
              precision    recall  f1-score   support

           0     0.2628    0.2500    0.2562     144.0
           1     0.6746    0.8122    0.7370     245.0
           2     0.5267    0.3854    0.4451     384.0
           3     0.5275    0.6765    0.5928     170.0
           4     0.6407    0.7157    0.6761     299.0
           5     0.5642    0.5302    0.5467     381.0

    accuracy                         0.5632    1623.0
   macro avg     0.5328    0.5617    0.5423    1623.0
weighted avg     0.5555    0.5632    0.5543    1623.0

[[ 36.  14.  14.   6.  73.   1.]
 [  5. 199.  16.   3.   3.  19.]
 [ 62.  47. 148.  17.  20.  90.]
 [  0.   2.  13. 115.   7.  33.]
 [ 29.   6.  30.   7. 214.  13.]
 [  5.  27.  60.  70.  17. 202.]]
loss: 5.874903678894043
loss: 6.021524429321289
loss: 5.893950462341309
epoch: 11, train_loss: 5.940000057220459, train_acc: 60.91, train_fscore: 59.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.894000053405762, test_acc: 54.59, test_fscore: 54.76, time: 6.4 sec
loss: 5.760693073272705
loss: 5.601999282836914
loss: 5.603519916534424
epoch: 12, train_loss: 5.684999942779541, train_acc: 59.93, train_fscore: 58.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.603499889373779, test_acc: 55.14, test_fscore: 55.23, time: 5.9 sec
loss: 5.530073642730713
loss: 5.342852592468262
loss: 5.437657833099365
epoch: 13, train_loss: 5.441999912261963, train_acc: 61.67, train_fscore: 60.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.437699794769287, test_acc: 59.7, test_fscore: 59.85, time: 6.07 sec
loss: 5.387869358062744
loss: 5.191281795501709
loss: 5.313365459442139
epoch: 14, train_loss: 5.296000003814697, train_acc: 62.65, train_fscore: 61.57, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.313399791717529, test_acc: 61.0, test_fscore: 60.45, time: 6.76 sec
loss: 5.238436698913574
loss: 5.151950359344482
loss: 5.12641716003418
epoch: 15, train_loss: 5.198599815368652, train_acc: 63.22, train_fscore: 61.75, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.126399993896484, test_acc: 63.52, test_fscore: 63.31, time: 5.68 sec
loss: 4.984987735748291
loss: 5.1245646476745605
loss: 5.04275369644165
epoch: 16, train_loss: 5.047999858856201, train_acc: 64.53, train_fscore: 63.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.042799949645996, test_acc: 60.14, test_fscore: 60.97, time: 6.5 sec
loss: 5.007218360900879
loss: 4.905510902404785
loss: 4.971966743469238
epoch: 17, train_loss: 4.957600116729736, train_acc: 63.91, train_fscore: 63.28, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.9720001220703125, test_acc: 63.4, test_fscore: 63.82, time: 6.2 sec
loss: 4.945562362670898
loss: 4.772180080413818
loss: 4.84223747253418
epoch: 18, train_loss: 4.867199897766113, train_acc: 64.7, train_fscore: 63.85, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.842199802398682, test_acc: 64.88, test_fscore: 64.85, time: 6.29 sec
loss: 4.649741172790527
loss: 4.8573503494262695
loss: 4.711460113525391
epoch: 19, train_loss: 4.747600078582764, train_acc: 65.97, train_fscore: 65.16, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.71150016784668, test_acc: 61.12, test_fscore: 62.16, time: 7.35 sec
loss: 4.673854827880859
loss: 4.6497063636779785
loss: 4.617555618286133
epoch: 20, train_loss: 4.663000106811523, train_acc: 65.59, train_fscore: 65.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.617599964141846, test_acc: 63.22, test_fscore: 64.08, time: 6.13 sec
              precision    recall  f1-score   support

           0     0.3631    0.4514    0.4025     144.0
           1     0.7588    0.7061    0.7315     245.0
           2     0.6655    0.4766    0.5554     384.0
           3     0.6059    0.7235    0.6595     170.0
           4     0.8073    0.8127    0.8100     299.0
           5     0.6087    0.6982    0.6504     381.0

    accuracy                         0.6488    1623.0
   macro avg     0.6349    0.6447    0.6349    1623.0
weighted avg     0.6593    0.6488    0.6485    1623.0

[[ 65.  11.  12.   5.  50.   1.]
 [  5. 173.  27.   2.   0.  38.]
 [ 59.  27. 183.  24.   4.  87.]
 [  0.   0.   4. 123.   0.  43.]
 [ 46.   0.   8.   0. 243.   2.]
 [  4.  17.  41.  49.   4. 266.]]
loss: 4.660705089569092
loss: 4.494171619415283
loss: 4.553773403167725
epoch: 21, train_loss: 4.583700180053711, train_acc: 66.06, train_fscore: 65.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.553800106048584, test_acc: 65.25, test_fscore: 65.3, time: 6.05 sec
loss: 4.598534107208252
loss: 4.429550647735596
loss: 4.49444580078125
epoch: 22, train_loss: 4.5208001136779785, train_acc: 65.47, train_fscore: 64.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.4944000244140625, test_acc: 63.4, test_fscore: 63.96, time: 6.69 sec
loss: 4.291318893432617
loss: 4.653050422668457
loss: 4.449174880981445
epoch: 23, train_loss: 4.464799880981445, train_acc: 65.51, train_fscore: 64.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.44920015335083, test_acc: 63.83, test_fscore: 64.34, time: 5.59 sec
loss: 4.478377342224121
loss: 4.297880172729492
loss: 4.40408992767334
epoch: 24, train_loss: 4.39139986038208, train_acc: 66.54, train_fscore: 65.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.404099941253662, test_acc: 65.19, test_fscore: 65.16, time: 6.36 sec
loss: 4.2966718673706055
loss: 4.363386631011963
loss: 4.348538875579834
epoch: 25, train_loss: 4.325099945068359, train_acc: 66.37, train_fscore: 65.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.348499774932861, test_acc: 64.63, test_fscore: 65.13, time: 6.26 sec
loss: 4.249914646148682
loss: 4.360063076019287
loss: 4.3154473304748535
epoch: 26, train_loss: 4.302700042724609, train_acc: 66.8, train_fscore: 66.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.315400123596191, test_acc: 64.57, test_fscore: 65.17, time: 5.96 sec
loss: 4.278579235076904
loss: 4.195611476898193
loss: 4.306259632110596
epoch: 27, train_loss: 4.240300178527832, train_acc: 67.59, train_fscore: 66.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.306300163269043, test_acc: 66.17, test_fscore: 65.95, time: 6.68 sec
loss: 4.079951286315918
loss: 4.345120906829834
loss: 4.285253047943115
epoch: 28, train_loss: 4.203800201416016, train_acc: 67.61, train_fscore: 66.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.285299777984619, test_acc: 66.79, test_fscore: 66.76, time: 6.5 sec
loss: 4.258998394012451
loss: 4.012243270874023
loss: 4.256791114807129
epoch: 29, train_loss: 4.146299839019775, train_acc: 68.64, train_fscore: 68.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.256800174713135, test_acc: 66.42, test_fscore: 66.92, time: 5.34 sec
loss: 4.0368852615356445
loss: 4.1706976890563965
loss: 4.216230869293213
epoch: 30, train_loss: 4.101399898529053, train_acc: 69.4, train_fscore: 69.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.21619987487793, test_acc: 66.48, test_fscore: 66.56, time: 6.3 sec
              precision    recall  f1-score   support

           0     0.4019    0.5833    0.4759     144.0
           1     0.7689    0.7061    0.7362     245.0
           2     0.6547    0.6172    0.6354     384.0
           3     0.6056    0.7588    0.6736     170.0
           4     0.8523    0.7525    0.7993     299.0
           5     0.6571    0.6037    0.6293     381.0

    accuracy                         0.6642    1623.0
   macro avg     0.6568    0.6703    0.6583    1623.0
weighted avg     0.6813    0.6642    0.6692    1623.0

[[ 84.   8.  17.   3.  31.   1.]
 [  4. 173.  35.   0.   0.  33.]
 [ 54.  20. 237.  23.   2.  48.]
 [  0.   0.   4. 129.   0.  37.]
 [ 65.   0.   8.   0. 225.   1.]
 [  2.  24.  61.  58.   6. 230.]]
loss: 3.952515125274658
loss: 4.163416385650635
loss: 4.183851718902588
epoch: 31, train_loss: 4.046899795532227, train_acc: 69.28, train_fscore: 68.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.183899879455566, test_acc: 65.8, test_fscore: 66.18, time: 6.35 sec
loss: 3.971280574798584
loss: 4.025021553039551
loss: 4.128262519836426
epoch: 32, train_loss: 3.9976000785827637, train_acc: 69.17, train_fscore: 68.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.128300189971924, test_acc: 65.56, test_fscore: 66.12, time: 6.04 sec
loss: 3.7958545684814453
loss: 4.154026508331299
loss: 4.080277919769287
epoch: 33, train_loss: 3.961699962615967, train_acc: 69.26, train_fscore: 68.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.0802998542785645, test_acc: 67.04, test_fscore: 67.21, time: 4.71 sec
loss: 3.9534239768981934
loss: 3.8604469299316406
loss: 4.045401573181152
epoch: 34, train_loss: 3.91129994392395, train_acc: 69.76, train_fscore: 69.16, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.045400142669678, test_acc: 67.65, test_fscore: 68.0, time: 2.77 sec
loss: 3.9959182739257812
loss: 3.711405038833618
loss: 4.011780261993408
epoch: 35, train_loss: 3.8603999614715576, train_acc: 70.67, train_fscore: 70.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.0117998123168945, test_acc: 67.78, test_fscore: 68.18, time: 4.96 sec
loss: 3.86356782913208
loss: 3.7618865966796875
loss: 3.992734670639038
epoch: 36, train_loss: 3.8148999214172363, train_acc: 71.22, train_fscore: 70.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9927000999450684, test_acc: 67.47, test_fscore: 67.8, time: 5.51 sec
loss: 3.8656506538391113
loss: 3.7228360176086426
loss: 3.969102144241333
epoch: 37, train_loss: 3.7994000911712646, train_acc: 71.5, train_fscore: 71.15, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.969099998474121, test_acc: 67.34, test_fscore: 67.74, time: 5.37 sec
loss: 3.9344992637634277
loss: 3.5639076232910156
loss: 3.9405105113983154
epoch: 38, train_loss: 3.7588000297546387, train_acc: 71.58, train_fscore: 71.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.940500020980835, test_acc: 67.71, test_fscore: 68.0, time: 6.09 sec
loss: 3.795520782470703
loss: 3.6692378520965576
loss: 3.9051878452301025
epoch: 39, train_loss: 3.7365000247955322, train_acc: 71.57, train_fscore: 71.17, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9052000045776367, test_acc: 67.59, test_fscore: 67.86, time: 6.44 sec
loss: 3.6179451942443848
loss: 3.7887728214263916
loss: 3.874598979949951
epoch: 40, train_loss: 3.699700117111206, train_acc: 71.67, train_fscore: 71.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8745999336242676, test_acc: 68.52, test_fscore: 68.85, time: 6.09 sec
              precision    recall  f1-score   support

           0     0.4409    0.5694    0.4970     144.0
           1     0.7860    0.7347    0.7595     245.0
           2     0.6980    0.6380    0.6667     384.0
           3     0.5936    0.7647    0.6684     170.0
           4     0.8363    0.7860    0.8103     299.0
           5     0.6723    0.6299    0.6504     381.0

    accuracy                         0.6852    1623.0
   macro avg     0.6712    0.6871    0.6754    1623.0
weighted avg     0.6970    0.6852    0.6885    1623.0

[[ 82.   9.  15.   5.  33.   0.]
 [  5. 180.  24.   1.   0.  35.]
 [ 44.  21. 245.  25.   6.  43.]
 [  0.   0.   2. 130.   0.  38.]
 [ 55.   0.   8.   0. 235.   1.]
 [  0.  19.  57.  58.   7. 240.]]
loss: 3.530914545059204
loss: 3.777222156524658
loss: 3.845738172531128
epoch: 41, train_loss: 3.6491000652313232, train_acc: 72.07, train_fscore: 71.7, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8457000255584717, test_acc: 68.39, test_fscore: 68.72, time: 5.49 sec
loss: 3.6528635025024414
loss: 3.5932726860046387
loss: 3.8306655883789062
epoch: 42, train_loss: 3.6273000240325928, train_acc: 72.63, train_fscore: 72.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.830699920654297, test_acc: 67.71, test_fscore: 68.21, time: 6.4 sec
loss: 3.660675048828125
loss: 3.542964458465576
loss: 3.810563325881958
epoch: 43, train_loss: 3.60479998588562, train_acc: 72.86, train_fscore: 72.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8106000423431396, test_acc: 68.15, test_fscore: 68.55, time: 6.19 sec
loss: 3.519913673400879
loss: 3.624225378036499
loss: 3.7767035961151123
epoch: 44, train_loss: 3.566999912261963, train_acc: 72.84, train_fscore: 72.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.776700019836426, test_acc: 68.82, test_fscore: 69.12, time: 6.09 sec
loss: 3.5525946617126465
loss: 3.540566921234131
loss: 3.760885238647461
epoch: 45, train_loss: 3.5469000339508057, train_acc: 73.72, train_fscore: 73.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7609000205993652, test_acc: 68.82, test_fscore: 69.13, time: 6.43 sec
loss: 3.643127918243408
loss: 3.3536181449890137
loss: 3.7387752532958984
epoch: 46, train_loss: 3.507499933242798, train_acc: 73.17, train_fscore: 72.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.738800048828125, test_acc: 68.7, test_fscore: 69.04, time: 7.04 sec
loss: 3.411933422088623
loss: 3.568835496902466
loss: 3.7055912017822266
epoch: 47, train_loss: 3.4834001064300537, train_acc: 74.1, train_fscore: 73.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7056000232696533, test_acc: 68.58, test_fscore: 69.03, time: 5.53 sec
loss: 3.3772897720336914
loss: 3.5613059997558594
loss: 3.670170307159424
epoch: 48, train_loss: 3.4639999866485596, train_acc: 73.65, train_fscore: 73.37, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6702001094818115, test_acc: 68.88, test_fscore: 69.3, time: 6.19 sec
loss: 3.4604687690734863
loss: 3.3816885948181152
loss: 3.663195848464966
epoch: 49, train_loss: 3.421799898147583, train_acc: 73.96, train_fscore: 73.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6631999015808105, test_acc: 69.01, test_fscore: 69.35, time: 5.8 sec
loss: 3.4941024780273438
loss: 3.2932281494140625
loss: 3.65246844291687
epoch: 50, train_loss: 3.399199962615967, train_acc: 74.39, train_fscore: 74.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6524999141693115, test_acc: 68.76, test_fscore: 69.19, time: 6.27 sec
              precision    recall  f1-score   support

           0     0.4550    0.5972    0.5165     144.0
           1     0.7892    0.7184    0.7521     245.0
           2     0.6972    0.6536    0.6747     384.0
           3     0.6154    0.7529    0.6772     170.0
           4     0.8393    0.7860    0.8117     299.0
           5     0.6722    0.6404    0.6559     381.0

    accuracy                         0.6901    1623.0
   macro avg     0.6781    0.6914    0.6814    1623.0
weighted avg     0.7013    0.6901    0.6935    1623.0

[[ 86.   9.  17.   1.  31.   0.]
 [  7. 176.  25.   2.   0.  35.]
 [ 42.  19. 251.  22.   7.  43.]
 [  0.   0.   2. 128.   0.  40.]
 [ 54.   1.   8.   0. 235.   1.]
 [  0.  18.  57.  55.   7. 244.]]
loss: 3.3583874702453613
loss: 3.3602182865142822
loss: 3.643350839614868
epoch: 51, train_loss: 3.359299898147583, train_acc: 74.84, train_fscore: 74.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.643399953842163, test_acc: 68.7, test_fscore: 69.17, time: 6.49 sec
loss: 3.3983137607574463
loss: 3.2990732192993164
loss: 3.6245808601379395
epoch: 52, train_loss: 3.351599931716919, train_acc: 74.82, train_fscore: 74.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6245999336242676, test_acc: 68.7, test_fscore: 69.14, time: 6.81 sec
loss: 3.187370538711548
loss: 3.480551242828369
loss: 3.5916190147399902
epoch: 53, train_loss: 3.320199966430664, train_acc: 75.13, train_fscore: 74.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.591599941253662, test_acc: 69.38, test_fscore: 69.74, time: 5.63 sec
loss: 3.334381103515625
loss: 3.281338691711426
loss: 3.5545198917388916
epoch: 54, train_loss: 3.309799909591675, train_acc: 74.78, train_fscore: 74.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.554500102996826, test_acc: 69.13, test_fscore: 69.52, time: 6.18 sec
loss: 3.184136152267456
loss: 3.430601119995117
loss: 3.5672619342803955
epoch: 55, train_loss: 3.2957000732421875, train_acc: 74.72, train_fscore: 74.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5673000812530518, test_acc: 69.01, test_fscore: 69.46, time: 6.09 sec
loss: 3.443671941757202
loss: 3.0984530448913574
loss: 3.5667757987976074
epoch: 56, train_loss: 3.277100086212158, train_acc: 75.58, train_fscore: 75.43, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.566800117492676, test_acc: 68.88, test_fscore: 69.43, time: 6.06 sec
loss: 3.3655903339385986
loss: 3.1659622192382812
loss: 3.5501649379730225
epoch: 57, train_loss: 3.2711000442504883, train_acc: 75.63, train_fscore: 75.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5501999855041504, test_acc: 68.82, test_fscore: 69.31, time: 6.19 sec
loss: 3.1574606895446777
loss: 3.341116428375244
loss: 3.545271635055542
epoch: 58, train_loss: 3.2441000938415527, train_acc: 75.46, train_fscore: 75.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.545300006866455, test_acc: 69.25, test_fscore: 69.68, time: 6.28 sec
loss: 3.2111544609069824
loss: 3.221489906311035
loss: 3.5214452743530273
epoch: 59, train_loss: 3.21589994430542, train_acc: 75.56, train_fscore: 75.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.521399974822998, test_acc: 69.25, test_fscore: 69.66, time: 5.84 sec
loss: 3.202712059020996
loss: 3.210710048675537
loss: 3.4928033351898193
epoch: 60, train_loss: 3.2065999507904053, train_acc: 76.45, train_fscore: 76.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.492799997329712, test_acc: 69.62, test_fscore: 70.06, time: 6.4 sec
              precision    recall  f1-score   support

           0     0.4559    0.6458    0.5345     144.0
           1     0.8026    0.7469    0.7738     245.0
           2     0.7159    0.6562    0.6848     384.0
           3     0.6238    0.7412    0.6774     170.0
           4     0.8556    0.7726    0.8120     299.0
           5     0.6676    0.6430    0.6551     381.0

    accuracy                         0.6962    1623.0
   macro avg     0.6869    0.7010    0.6896    1623.0
weighted avg     0.7107    0.6962    0.7006    1623.0

[[ 93.   9.  13.   0.  29.   0.]
 [  5. 183.  22.   0.   0.  35.]
 [ 47.  18. 252.  21.   3.  43.]
 [  0.   0.   1. 126.   0.  43.]
 [ 58.   1.   8.   0. 231.   1.]
 [  1.  17.  56.  55.   7. 245.]]
loss: 3.269261360168457
loss: 3.0912086963653564
loss: 3.478032112121582
epoch: 61, train_loss: 3.1881000995635986, train_acc: 76.68, train_fscore: 76.51, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4779999256134033, test_acc: 69.81, test_fscore: 70.24, time: 7.2 sec
loss: 3.1922638416290283
loss: 3.1540608406066895
loss: 3.5005221366882324
epoch: 62, train_loss: 3.173799991607666, train_acc: 76.54, train_fscore: 76.37, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.500499963760376, test_acc: 69.5, test_fscore: 69.92, time: 5.97 sec
loss: 3.313905954360962
loss: 2.952336311340332
loss: 3.4733481407165527
epoch: 63, train_loss: 3.1410000324249268, train_acc: 76.7, train_fscore: 76.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.473299980163574, test_acc: 69.62, test_fscore: 70.07, time: 5.98 sec
loss: 3.202699661254883
loss: 3.07875657081604
loss: 3.458000421524048
epoch: 64, train_loss: 3.1422998905181885, train_acc: 76.56, train_fscore: 76.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4579999446868896, test_acc: 69.38, test_fscore: 69.76, time: 5.98 sec
loss: 3.1445837020874023
loss: 3.0830695629119873
loss: 3.4622325897216797
epoch: 65, train_loss: 3.115299940109253, train_acc: 76.85, train_fscore: 76.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4621999263763428, test_acc: 69.62, test_fscore: 70.06, time: 6.74 sec
loss: 3.2047665119171143
loss: 3.010230302810669
loss: 3.465985059738159
epoch: 66, train_loss: 3.1103999614715576, train_acc: 77.06, train_fscore: 76.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4660000801086426, test_acc: 69.69, test_fscore: 70.06, time: 6.41 sec
loss: 3.0484731197357178
loss: 3.120699882507324
loss: 3.4517879486083984
epoch: 67, train_loss: 3.0826001167297363, train_acc: 77.33, train_fscore: 77.15, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4518001079559326, test_acc: 69.81, test_fscore: 70.28, time: 6.63 sec
loss: 2.9209389686584473
loss: 3.216024398803711
loss: 3.4531285762786865
epoch: 68, train_loss: 3.065700054168701, train_acc: 77.73, train_fscore: 77.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4530999660491943, test_acc: 70.18, test_fscore: 70.59, time: 6.06 sec
loss: 3.0077290534973145
loss: 3.1194281578063965
loss: 3.4582135677337646
epoch: 69, train_loss: 3.061000108718872, train_acc: 77.76, train_fscore: 77.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.458199977874756, test_acc: 69.93, test_fscore: 70.33, time: 6.45 sec
loss: 2.889075517654419
loss: 3.218517541885376
loss: 3.429932117462158
epoch: 70, train_loss: 3.0429000854492188, train_acc: 77.81, train_fscore: 77.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4298999309539795, test_acc: 70.18, test_fscore: 70.55, time: 6.1 sec
              precision    recall  f1-score   support

           0     0.4703    0.6597    0.5491     144.0
           1     0.8079    0.7551    0.7806     245.0
           2     0.7244    0.6641    0.6929     384.0
           3     0.6214    0.7529    0.6809     170.0
           4     0.8582    0.7692    0.8113     299.0
           5     0.6721    0.6457    0.6586     381.0

    accuracy                         0.7018    1623.0
   macro avg     0.6924    0.7078    0.6956    1623.0
weighted avg     0.7160    0.7018    0.7059    1623.0

[[ 95.   9.  13.   0.  27.   0.]
 [  2. 185.  22.   0.   0.  36.]
 [ 46.  17. 255.  20.   4.  42.]
 [  0.   0.   1. 128.   0.  41.]
 [ 58.   2.   8.   0. 230.   1.]
 [  1.  16.  53.  58.   7. 246.]]
loss: 2.901632308959961
loss: 3.146289825439453
loss: 3.4165821075439453
epoch: 71, train_loss: 3.0160000324249268, train_acc: 77.68, train_fscore: 77.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.416599988937378, test_acc: 70.06, test_fscore: 70.44, time: 6.81 sec
loss: 3.046595335006714
loss: 2.991917133331299
loss: 3.411834955215454
epoch: 72, train_loss: 3.021899938583374, train_acc: 77.49, train_fscore: 77.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.411799907684326, test_acc: 70.67, test_fscore: 71.05, time: 5.94 sec
loss: 2.9536049365997314
loss: 3.0558769702911377
loss: 3.4011435508728027
epoch: 73, train_loss: 3.0023000240325928, train_acc: 77.61, train_fscore: 77.44, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.401099920272827, test_acc: 70.49, test_fscore: 70.85, time: 6.58 sec
loss: 2.9963057041168213
loss: 2.958678960800171
loss: 3.3847827911376953
epoch: 74, train_loss: 2.979300022125244, train_acc: 78.36, train_fscore: 78.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3847999572753906, test_acc: 70.61, test_fscore: 70.98, time: 6.03 sec
loss: 2.995455741882324
loss: 2.9265077114105225
loss: 3.3893256187438965
epoch: 75, train_loss: 2.963099956512451, train_acc: 78.36, train_fscore: 78.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3893001079559326, test_acc: 70.43, test_fscore: 70.78, time: 6.2 sec
loss: 2.9161300659179688
loss: 3.019902229309082
loss: 3.4151978492736816
epoch: 76, train_loss: 2.9639999866485596, train_acc: 78.19, train_fscore: 78.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4151999950408936, test_acc: 70.49, test_fscore: 70.88, time: 6.08 sec
loss: 3.084251642227173
loss: 2.7723188400268555
loss: 3.4000327587127686
epoch: 77, train_loss: 2.9412999153137207, train_acc: 78.47, train_fscore: 78.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4000000953674316, test_acc: 70.18, test_fscore: 70.52, time: 6.88 sec
loss: 3.048285722732544
loss: 2.818551778793335
loss: 3.3767120838165283
epoch: 78, train_loss: 2.9439001083374023, train_acc: 78.8, train_fscore: 78.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.376699924468994, test_acc: 70.49, test_fscore: 70.83, time: 5.92 sec
loss: 2.989415407180786
loss: 2.8234453201293945
loss: 3.3536734580993652
epoch: 79, train_loss: 2.908400058746338, train_acc: 78.64, train_fscore: 78.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3536999225616455, test_acc: 70.98, test_fscore: 71.32, time: 5.94 sec
loss: 2.849801540374756
loss: 2.9603724479675293
loss: 3.3396456241607666
epoch: 80, train_loss: 2.902600049972534, train_acc: 78.88, train_fscore: 78.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.339600086212158, test_acc: 70.92, test_fscore: 71.26, time: 6.92 sec
              precision    recall  f1-score   support

           0     0.5000    0.7014    0.5838     144.0
           1     0.8062    0.7469    0.7754     245.0
           2     0.7194    0.6745    0.6962     384.0
           3     0.6422    0.7706    0.7005     170.0
           4     0.8657    0.7759    0.8183     299.0
           5     0.6796    0.6457    0.6622     381.0

    accuracy                         0.7098    1623.0
   macro avg     0.7022    0.7192    0.7061    1623.0
weighted avg     0.7225    0.7098    0.7132    1623.0

[[101.   9.  10.   0.  24.   0.]
 [  2. 183.  23.   2.   0.  35.]
 [ 44.  17. 259.  18.   4.  42.]
 [  0.   0.   1. 131.   0.  38.]
 [ 55.   2.   9.   0. 232.   1.]
 [  0.  16.  58.  53.   8. 246.]]
loss: 2.83463454246521
loss: 2.9572839736938477
loss: 3.3394017219543457
epoch: 81, train_loss: 2.894700050354004, train_acc: 79.1, train_fscore: 78.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.339400053024292, test_acc: 71.1, test_fscore: 71.46, time: 6.12 sec
loss: 2.6835341453552246
loss: 3.073251724243164
loss: 3.3364999294281006
epoch: 82, train_loss: 2.872299909591675, train_acc: 79.21, train_fscore: 79.06, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3364999294281006, test_acc: 70.79, test_fscore: 71.09, time: 6.34 sec
loss: 3.0005459785461426
loss: 2.6749298572540283
loss: 3.3605782985687256
epoch: 83, train_loss: 2.8478000164031982, train_acc: 79.26, train_fscore: 79.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.360599994659424, test_acc: 70.73, test_fscore: 71.06, time: 6.0 sec
loss: 2.773517608642578
loss: 2.9124913215637207
loss: 3.35546612739563
epoch: 84, train_loss: 2.839099884033203, train_acc: 80.1, train_fscore: 79.98, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3554999828338623, test_acc: 70.79, test_fscore: 71.14, time: 6.8 sec
loss: 2.9239072799682617
loss: 2.753018379211426
loss: 3.3609914779663086
epoch: 85, train_loss: 2.8382999897003174, train_acc: 79.66, train_fscore: 79.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3610000610351562, test_acc: 70.73, test_fscore: 71.05, time: 5.97 sec
loss: 2.8769495487213135
loss: 2.7679080963134766
loss: 3.338616371154785
epoch: 86, train_loss: 2.8255999088287354, train_acc: 79.72, train_fscore: 79.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.338599920272827, test_acc: 70.79, test_fscore: 71.12, time: 6.52 sec
loss: 2.7731852531433105
loss: 2.8665919303894043
loss: 3.344411611557007
epoch: 87, train_loss: 2.815999984741211, train_acc: 79.59, train_fscore: 79.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.344399929046631, test_acc: 71.04, test_fscore: 71.43, time: 6.29 sec
loss: 2.673548936843872
loss: 2.9150571823120117
loss: 3.3419723510742188
epoch: 88, train_loss: 2.7881999015808105, train_acc: 80.05, train_fscore: 79.94, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3420000076293945, test_acc: 71.04, test_fscore: 71.38, time: 6.12 sec
loss: 2.736976146697998
loss: 2.850891590118408
loss: 3.3501362800598145
epoch: 89, train_loss: 2.7869999408721924, train_acc: 79.72, train_fscore: 79.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.350100040435791, test_acc: 70.92, test_fscore: 71.27, time: 6.35 sec
loss: 2.8440427780151367
loss: 2.7185747623443604
loss: 3.3593499660491943
epoch: 90, train_loss: 2.7848000526428223, train_acc: 79.9, train_fscore: 79.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3594000339508057, test_acc: 70.92, test_fscore: 71.3, time: 5.65 sec
              precision    recall  f1-score   support

           0     0.4976    0.7083    0.5845     144.0
           1     0.8214    0.7510    0.7846     245.0
           2     0.7333    0.6589    0.6941     384.0
           3     0.6422    0.7706    0.7005     170.0
           4     0.8593    0.7759    0.8155     299.0
           5     0.6720    0.6614    0.6667     381.0

    accuracy                         0.7110    1623.0
   macro avg     0.7043    0.7210    0.7077    1623.0
weighted avg     0.7250    0.7110    0.7146    1623.0

[[102.   7.   9.   0.  26.   0.]
 [  2. 184.  22.   2.   0.  35.]
 [ 45.  15. 253.  18.   4.  49.]
 [  0.   0.   1. 131.   0.  38.]
 [ 56.   1.   9.   0. 232.   1.]
 [  0.  17.  51.  53.   8. 252.]]
loss: 2.6825194358825684
loss: 2.9002349376678467
loss: 3.3313522338867188
epoch: 91, train_loss: 2.7808001041412354, train_acc: 80.1, train_fscore: 79.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.331399917602539, test_acc: 70.73, test_fscore: 71.06, time: 6.61 sec
loss: 2.846886157989502
loss: 2.667525053024292
loss: 3.323101043701172
epoch: 92, train_loss: 2.761199951171875, train_acc: 80.12, train_fscore: 80.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3231000900268555, test_acc: 70.86, test_fscore: 71.19, time: 5.9 sec
loss: 2.6238112449645996
loss: 2.907453775405884
loss: 3.3386292457580566
epoch: 93, train_loss: 2.757999897003174, train_acc: 80.17, train_fscore: 80.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.338599920272827, test_acc: 70.79, test_fscore: 71.17, time: 5.8 sec
loss: 2.696960687637329
loss: 2.7888872623443604
loss: 3.347489356994629
epoch: 94, train_loss: 2.741300106048584, train_acc: 80.69, train_fscore: 80.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3475000858306885, test_acc: 71.23, test_fscore: 71.58, time: 6.17 sec
loss: 2.718306541442871
loss: 2.721057415008545
loss: 3.3420820236206055
epoch: 95, train_loss: 2.7195000648498535, train_acc: 81.05, train_fscore: 80.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.342099905014038, test_acc: 71.1, test_fscore: 71.46, time: 6.21 sec
loss: 2.7932868003845215
loss: 2.6343510150909424
loss: 3.324450731277466
epoch: 96, train_loss: 2.7139999866485596, train_acc: 80.74, train_fscore: 80.62, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.32450008392334, test_acc: 71.29, test_fscore: 71.64, time: 5.93 sec
loss: 2.748455047607422
loss: 2.6668105125427246
loss: 3.3248658180236816
epoch: 97, train_loss: 2.7121999263763428, train_acc: 80.71, train_fscore: 80.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.324899911880493, test_acc: 71.1, test_fscore: 71.46, time: 6.55 sec
loss: 2.8262417316436768
loss: 2.582174301147461
loss: 3.343895673751831
epoch: 98, train_loss: 2.7065999507904053, train_acc: 81.02, train_fscore: 80.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.343899965286255, test_acc: 70.79, test_fscore: 71.16, time: 5.54 sec
loss: 2.5909056663513184
loss: 2.7590842247009277
loss: 3.332904815673828
epoch: 99, train_loss: 2.66729998588562, train_acc: 81.22, train_fscore: 81.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.332900047302246, test_acc: 71.23, test_fscore: 71.59, time: 6.68 sec
loss: 2.607252359390259
loss: 2.7418313026428223
loss: 3.3078203201293945
epoch: 100, train_loss: 2.672800064086914, train_acc: 82.05, train_fscore: 81.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.307800054550171, test_acc: 70.73, test_fscore: 71.07, time: 6.42 sec
              precision    recall  f1-score   support

           0     0.5171    0.7361    0.6074     144.0
           1     0.8206    0.7469    0.7821     245.0
           2     0.7247    0.6719    0.6973     384.0
           3     0.6418    0.7588    0.6954     170.0
           4     0.8717    0.7726    0.8191     299.0
           5     0.6702    0.6562    0.6631     381.0

    accuracy                         0.7129    1623.0
   macro avg     0.7077    0.7237    0.7107    1623.0
weighted avg     0.7264    0.7129    0.7164    1623.0

[[106.   8.   7.   0.  23.   0.]
 [  2. 183.  23.   1.   0.  36.]
 [ 43.  16. 258.  18.   3.  46.]
 [  0.   0.   1. 129.   0.  40.]
 [ 54.   1.  12.   0. 231.   1.]
 [  0.  15.  55.  53.   8. 250.]]
loss: 2.675158977508545
loss: 2.683885335922241
loss: 3.3283233642578125
epoch: 101, train_loss: 2.67930006980896, train_acc: 81.08, train_fscore: 80.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3282999992370605, test_acc: 71.23, test_fscore: 71.59, time: 6.26 sec
loss: 2.7309226989746094
loss: 2.5492210388183594
loss: 3.3292133808135986
epoch: 102, train_loss: 2.6449999809265137, train_acc: 81.41, train_fscore: 81.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.329200029373169, test_acc: 70.92, test_fscore: 71.27, time: 6.08 sec
loss: 2.6488232612609863
loss: 2.651510238647461
loss: 3.3252947330474854
epoch: 103, train_loss: 2.650099992752075, train_acc: 81.93, train_fscore: 81.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3252999782562256, test_acc: 71.23, test_fscore: 71.61, time: 6.27 sec
loss: 2.663895845413208
loss: 2.680549144744873
loss: 3.311534881591797
epoch: 104, train_loss: 2.6719000339508057, train_acc: 81.74, train_fscore: 81.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.311500072479248, test_acc: 71.41, test_fscore: 71.77, time: 6.07 sec
loss: 2.606248378753662
loss: 2.6956775188446045
loss: 3.300482749938965
epoch: 105, train_loss: 2.64490008354187, train_acc: 81.43, train_fscore: 81.34, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.30049991607666, test_acc: 71.47, test_fscore: 71.82, time: 6.04 sec
loss: 2.6255524158477783
loss: 2.6038103103637695
loss: 3.2987637519836426
epoch: 106, train_loss: 2.6147000789642334, train_acc: 81.58, train_fscore: 81.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.298799991607666, test_acc: 71.53, test_fscore: 71.87, time: 5.65 sec
loss: 2.5277369022369385
loss: 2.707643985748291
loss: 3.304917097091675
epoch: 107, train_loss: 2.614300012588501, train_acc: 81.89, train_fscore: 81.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3048999309539795, test_acc: 71.29, test_fscore: 71.61, time: 5.37 sec
loss: 2.542126178741455
loss: 2.687319278717041
loss: 3.3014512062072754
epoch: 108, train_loss: 2.611599922180176, train_acc: 82.01, train_fscore: 81.89, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.301500082015991, test_acc: 71.72, test_fscore: 72.09, time: 6.49 sec
loss: 2.5963845252990723
loss: 2.575997829437256
loss: 3.2978029251098633
epoch: 109, train_loss: 2.587399959564209, train_acc: 82.44, train_fscore: 82.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.297800064086914, test_acc: 71.72, test_fscore: 72.08, time: 5.26 sec
loss: 2.377249240875244
loss: 2.7369184494018555
loss: 3.28635835647583
epoch: 110, train_loss: 2.553299903869629, train_acc: 82.31, train_fscore: 82.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.286400079727173, test_acc: 71.78, test_fscore: 72.16, time: 6.85 sec
              precision    recall  f1-score   support

           0     0.5143    0.7500    0.6102     144.0
           1     0.8304    0.7592    0.7932     245.0
           2     0.7358    0.6745    0.7038     384.0
           3     0.6497    0.7529    0.6975     170.0
           4     0.8736    0.7625    0.8143     299.0
           5     0.6755    0.6719    0.6737     381.0

    accuracy                         0.7178    1623.0
   macro avg     0.7132    0.7285    0.7154    1623.0
weighted avg     0.7326    0.7178    0.7216    1623.0

[[108.   7.   6.   0.  23.   0.]
 [  2. 186.  21.   1.   0.  35.]
 [ 43.  16. 259.  17.   3.  46.]
 [  0.   0.   1. 128.   0.  41.]
 [ 57.   0.  13.   0. 228.   1.]
 [  0.  15.  52.  51.   7. 256.]]
loss: 2.548218250274658
loss: 2.616058826446533
loss: 3.2914724349975586
epoch: 111, train_loss: 2.579900026321411, train_acc: 81.84, train_fscore: 81.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2915000915527344, test_acc: 71.41, test_fscore: 71.78, time: 7.05 sec
loss: 2.5472187995910645
loss: 2.6037726402282715
loss: 3.3233675956726074
epoch: 112, train_loss: 2.5729000568389893, train_acc: 82.72, train_fscore: 82.62, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3234000205993652, test_acc: 71.53, test_fscore: 71.94, time: 5.49 sec
loss: 2.5012052059173584
loss: 2.61458683013916
loss: 3.322436809539795
epoch: 113, train_loss: 2.5585999488830566, train_acc: 82.32, train_fscore: 82.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3224000930786133, test_acc: 71.29, test_fscore: 71.67, time: 6.6 sec
loss: 2.486034870147705
loss: 2.6069765090942383
loss: 3.3046035766601562
epoch: 114, train_loss: 2.5441999435424805, train_acc: 82.67, train_fscore: 82.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3046000003814697, test_acc: 71.6, test_fscore: 72.0, time: 6.26 sec
loss: 2.6276803016662598
loss: 2.4504525661468506
loss: 3.291830539703369
epoch: 115, train_loss: 2.543600082397461, train_acc: 82.5, train_fscore: 82.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.291800022125244, test_acc: 71.53, test_fscore: 71.91, time: 6.57 sec
loss: 2.675420045852661
loss: 2.3539764881134033
loss: 3.2989938259124756
epoch: 116, train_loss: 2.526900053024292, train_acc: 82.99, train_fscore: 82.9, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2990000247955322, test_acc: 71.53, test_fscore: 71.9, time: 4.99 sec
loss: 2.504436731338501
loss: 2.5539095401763916
loss: 3.3243601322174072
epoch: 117, train_loss: 2.5290000438690186, train_acc: 82.84, train_fscore: 82.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.324399948120117, test_acc: 71.04, test_fscore: 71.44, time: 6.65 sec
loss: 2.5594334602355957
loss: 2.4641690254211426
loss: 3.3249759674072266
epoch: 118, train_loss: 2.5143001079559326, train_acc: 82.89, train_fscore: 82.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.325000047683716, test_acc: 71.29, test_fscore: 71.63, time: 5.73 sec
loss: 2.4096765518188477
loss: 2.5797739028930664
loss: 3.315659999847412
epoch: 119, train_loss: 2.489000082015991, train_acc: 82.93, train_fscore: 82.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.315700054168701, test_acc: 71.47, test_fscore: 71.86, time: 5.87 sec
loss: 2.5576295852661133
loss: 2.46108341217041
loss: 3.297462224960327
epoch: 120, train_loss: 2.511899948120117, train_acc: 83.12, train_fscore: 83.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.297499895095825, test_acc: 71.78, test_fscore: 72.17, time: 6.53 sec
              precision    recall  f1-score   support

           0     0.5185    0.7778    0.6222     144.0
           1     0.8462    0.7633    0.8026     245.0
           2     0.7283    0.6979    0.7128     384.0
           3     0.6150    0.7706    0.6841     170.0
           4     0.8789    0.7525    0.8108     299.0
           5     0.6934    0.6352    0.6630     381.0

    accuracy                         0.7178    1623.0
   macro avg     0.7134    0.7329    0.7159    1623.0
weighted avg     0.7352    0.7178    0.7217    1623.0

[[112.   4.   6.   0.  22.   0.]
 [  3. 187.  20.   3.   0.  32.]
 [ 43.  16. 268.  18.   3.  36.]
 [  0.   0.   1. 131.   0.  38.]
 [ 57.   0.  16.   0. 225.   1.]
 [  1.  14.  57.  61.   6. 242.]]
loss: 2.4924216270446777
loss: 2.5149059295654297
loss: 3.321898937225342
epoch: 121, train_loss: 2.503000020980835, train_acc: 83.32, train_fscore: 83.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.321899890899658, test_acc: 71.6, test_fscore: 71.99, time: 6.57 sec
loss: 2.5415124893188477
loss: 2.4562203884124756
loss: 3.366076946258545
epoch: 122, train_loss: 2.5013999938964844, train_acc: 83.41, train_fscore: 83.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3661000728607178, test_acc: 71.1, test_fscore: 71.49, time: 6.47 sec
loss: 2.4796700477600098
loss: 2.504714250564575
loss: 3.326209783554077
epoch: 123, train_loss: 2.491499900817871, train_acc: 83.12, train_fscore: 83.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.326200008392334, test_acc: 71.6, test_fscore: 72.0, time: 5.54 sec
loss: 2.4089622497558594
loss: 2.5319106578826904
loss: 3.3052189350128174
epoch: 124, train_loss: 2.467400074005127, train_acc: 83.17, train_fscore: 83.06, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3052000999450684, test_acc: 71.23, test_fscore: 71.6, time: 6.05 sec
loss: 2.4116811752319336
loss: 2.531376361846924
loss: 3.3329288959503174
epoch: 125, train_loss: 2.4674999713897705, train_acc: 83.72, train_fscore: 83.59, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.332900047302246, test_acc: 71.66, test_fscore: 72.01, time: 6.03 sec
loss: 2.4139838218688965
loss: 2.477238178253174
loss: 3.3376972675323486
epoch: 126, train_loss: 2.4430999755859375, train_acc: 83.91, train_fscore: 83.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3376998901367188, test_acc: 71.66, test_fscore: 72.07, time: 6.25 sec
loss: 2.3395919799804688
loss: 2.585031509399414
loss: 3.3322548866271973
epoch: 127, train_loss: 2.4511001110076904, train_acc: 83.99, train_fscore: 83.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3322999477386475, test_acc: 71.72, test_fscore: 72.1, time: 6.12 sec
loss: 2.3530116081237793
loss: 2.4887874126434326
loss: 3.314884662628174
epoch: 128, train_loss: 2.419100046157837, train_acc: 83.92, train_fscore: 83.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3148999214172363, test_acc: 71.53, test_fscore: 71.93, time: 6.05 sec
loss: 2.6319704055786133
loss: 2.216277599334717
loss: 3.33166766166687
epoch: 129, train_loss: 2.441200017929077, train_acc: 83.67, train_fscore: 83.56, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.331700086593628, test_acc: 71.41, test_fscore: 71.82, time: 5.71 sec
loss: 2.5609898567199707
loss: 2.272599935531616
loss: 3.3197476863861084
epoch: 130, train_loss: 2.4323999881744385, train_acc: 83.63, train_fscore: 83.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.319700002670288, test_acc: 71.78, test_fscore: 72.15, time: 6.45 sec
              precision    recall  f1-score   support

           0     0.5185    0.7778    0.6222     144.0
           1     0.8462    0.7633    0.8026     245.0
           2     0.7283    0.6979    0.7128     384.0
           3     0.6150    0.7706    0.6841     170.0
           4     0.8789    0.7525    0.8108     299.0
           5     0.6934    0.6352    0.6630     381.0

    accuracy                         0.7178    1623.0
   macro avg     0.7134    0.7329    0.7159    1623.0
weighted avg     0.7352    0.7178    0.7217    1623.0

[[112.   4.   6.   0.  22.   0.]
 [  3. 187.  20.   3.   0.  32.]
 [ 43.  16. 268.  18.   3.  36.]
 [  0.   0.   1. 131.   0.  38.]
 [ 57.   0.  16.   0. 225.   1.]
 [  1.  14.  57.  61.   6. 242.]]
loss: 2.477226495742798
loss: 2.3561551570892334
loss: 3.3356032371520996
epoch: 131, train_loss: 2.4189000129699707, train_acc: 83.77, train_fscore: 83.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.335599899291992, test_acc: 71.04, test_fscore: 71.43, time: 6.56 sec
loss: 2.307023525238037
loss: 2.5270183086395264
loss: 3.340132236480713
epoch: 132, train_loss: 2.4156999588012695, train_acc: 84.49, train_fscore: 84.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.340100049972534, test_acc: 71.29, test_fscore: 71.71, time: 6.23 sec
loss: 2.5421223640441895
loss: 2.2684550285339355
loss: 3.3422129154205322
epoch: 133, train_loss: 2.4223999977111816, train_acc: 84.41, train_fscore: 84.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3422000408172607, test_acc: 71.23, test_fscore: 71.66, time: 6.17 sec
loss: 2.4552505016326904
loss: 2.32234263420105
loss: 3.3121156692504883
epoch: 134, train_loss: 2.397700071334839, train_acc: 84.56, train_fscore: 84.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3120999336242676, test_acc: 71.78, test_fscore: 72.14, time: 5.85 sec
loss: 2.4442453384399414
loss: 2.30999755859375
loss: 3.3124516010284424
epoch: 135, train_loss: 2.3798999786376953, train_acc: 84.34, train_fscore: 84.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3125, test_acc: 71.78, test_fscore: 72.14, time: 7.05 sec
loss: 2.330626964569092
loss: 2.459425926208496
loss: 3.329359769821167
epoch: 136, train_loss: 2.3880999088287354, train_acc: 84.65, train_fscore: 84.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.329400062561035, test_acc: 71.47, test_fscore: 71.9, time: 6.32 sec
loss: 2.342433452606201
loss: 2.423905849456787
loss: 3.315585136413574
epoch: 137, train_loss: 2.3814001083374023, train_acc: 84.92, train_fscore: 84.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3155999183654785, test_acc: 71.6, test_fscore: 71.98, time: 5.77 sec
loss: 2.4183311462402344
loss: 2.336275339126587
loss: 3.3098697662353516
epoch: 138, train_loss: 2.3787999153137207, train_acc: 84.94, train_fscore: 84.84, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3099000453948975, test_acc: 71.9, test_fscore: 72.29, time: 6.25 sec
loss: 2.3242197036743164
loss: 2.4430980682373047
loss: 3.3313827514648438
epoch: 139, train_loss: 2.374500036239624, train_acc: 84.6, train_fscore: 84.49, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.331399917602539, test_acc: 71.9, test_fscore: 72.26, time: 5.94 sec
loss: 2.3567862510681152
loss: 2.311774492263794
loss: 3.3813538551330566
epoch: 140, train_loss: 2.337599992752075, train_acc: 84.61, train_fscore: 84.49, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3814001083374023, test_acc: 71.04, test_fscore: 71.44, time: 6.25 sec
              precision    recall  f1-score   support

           0     0.5166    0.7569    0.6141     144.0
           1     0.8462    0.7633    0.8026     245.0
           2     0.7302    0.6979    0.7137     384.0
           3     0.6350    0.7471    0.6865     170.0
           4     0.8687    0.7525    0.8065     299.0
           5     0.6877    0.6588    0.6729     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7141    0.7294    0.7160    1623.0
weighted avg     0.7343    0.7190    0.7229    1623.0

[[109.   5.   5.   0.  25.   0.]
 [  2. 187.  21.   3.   0.  32.]
 [ 43.  16. 268.  15.   3.  39.]
 [  0.   0.   1. 127.   0.  42.]
 [ 57.   0.  16.   0. 225.   1.]
 [  0.  13.  56.  55.   6. 251.]]
loss: 2.2832701206207275
loss: 2.4269649982452393
loss: 3.348121166229248
epoch: 141, train_loss: 2.3513998985290527, train_acc: 84.84, train_fscore: 84.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.348099946975708, test_acc: 71.41, test_fscore: 71.79, time: 6.75 sec
loss: 2.3916518688201904
loss: 2.2208807468414307
loss: 3.322798252105713
epoch: 142, train_loss: 2.3124001026153564, train_acc: 85.09, train_fscore: 85.0, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3227999210357666, test_acc: 71.9, test_fscore: 72.28, time: 5.92 sec
loss: 2.2878003120422363
loss: 2.354194164276123
loss: 3.3441550731658936
epoch: 143, train_loss: 2.319200038909912, train_acc: 84.92, train_fscore: 84.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3441998958587646, test_acc: 71.72, test_fscore: 72.05, time: 6.21 sec
loss: 2.392594337463379
loss: 2.212418556213379
loss: 3.3727619647979736
epoch: 144, train_loss: 2.311300039291382, train_acc: 84.92, train_fscore: 84.8, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.37280011177063, test_acc: 71.35, test_fscore: 71.72, time: 5.93 sec
loss: 2.269134759902954
loss: 2.3668339252471924
loss: 3.3500828742980957
epoch: 145, train_loss: 2.3160998821258545, train_acc: 84.87, train_fscore: 84.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.350100040435791, test_acc: 71.35, test_fscore: 71.71, time: 6.55 sec
loss: 2.45357608795166
loss: 2.124934196472168
loss: 3.364032745361328
epoch: 146, train_loss: 2.3029000759124756, train_acc: 85.37, train_fscore: 85.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.364000082015991, test_acc: 71.72, test_fscore: 72.12, time: 5.96 sec
loss: 2.2319252490997314
loss: 2.395510673522949
loss: 3.367772102355957
epoch: 147, train_loss: 2.3087000846862793, train_acc: 85.2, train_fscore: 85.11, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.367799997329712, test_acc: 71.66, test_fscore: 72.01, time: 6.11 sec
loss: 2.3763887882232666
loss: 2.1961512565612793
loss: 3.393732786178589
epoch: 148, train_loss: 2.292799949645996, train_acc: 85.03, train_fscore: 84.94, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.393699884414673, test_acc: 71.41, test_fscore: 71.78, time: 5.7 sec
loss: 2.28184175491333
loss: 2.3085107803344727
loss: 3.3766839504241943
epoch: 149, train_loss: 2.2939999103546143, train_acc: 85.11, train_fscore: 85.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.376699924468994, test_acc: 71.72, test_fscore: 72.12, time: 4.48 sec
loss: 2.280526876449585
loss: 2.3296754360198975
loss: 3.3757197856903076
epoch: 150, train_loss: 2.3039000034332275, train_acc: 85.27, train_fscore: 85.16, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.375699996948242, test_acc: 72.27, test_fscore: 72.6, time: 6.53 sec
              precision    recall  f1-score   support

           0     0.5381    0.7361    0.6217     144.0
           1     0.8500    0.7633    0.8043     245.0
           2     0.7237    0.7161    0.7199     384.0
           3     0.6300    0.7412    0.6811     170.0
           4     0.8609    0.7659    0.8106     299.0
           5     0.6944    0.6562    0.6748     381.0

    accuracy                         0.7227    1623.0
   macro avg     0.7162    0.7298    0.7187    1623.0
weighted avg     0.7349    0.7227    0.7260    1623.0

[[106.   4.   6.   0.  28.   0.]
 [  2. 187.  22.   3.   0.  31.]
 [ 38.  16. 275.  16.   4.  35.]
 [  0.   0.   1. 126.   0.  43.]
 [ 51.   0.  18.   0. 229.   1.]
 [  0.  13.  58.  55.   5. 250.]]
Test performance..
F-Score: 72.6
F-Score-index: 150
              precision    recall  f1-score   support

           0     0.5381    0.7361    0.6217     144.0
           1     0.8500    0.7633    0.8043     245.0
           2     0.7237    0.7161    0.7199     384.0
           3     0.6300    0.7412    0.6811     170.0
           4     0.8609    0.7659    0.8106     299.0
           5     0.6944    0.6562    0.6748     381.0

    accuracy                         0.7227    1623.0
   macro avg     0.7162    0.7298    0.7187    1623.0
weighted avg     0.7349    0.7227    0.7260    1623.0

[[106.   4.   6.   0.  28.   0.]
 [  2. 187.  22.   3.   0.  31.]
 [ 38.  16. 275.  16.   4.  35.]
 [  0.   0.   1. 126.   0.  43.]
 [ 51.   0.  18.   0. 229.   1.]
 [  0.  13.  58.  55.   5. 250.]]
--- 10 ---
Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.5, batch_size=64, hidden_dim=1024, n_head=8, epochs=150, temp=2, tensorboard=False, class_weight=True, Dataset='IEMOCAP')
Running on GPU
temp 2
total parameters: 79687704
training parameters: 79687704
loss: 8.051502227783203
loss: 11.340412139892578
loss: 8.026651382446289
epoch: 1, train_loss: 9.587300300598145, train_acc: 17.49, train_fscore: 15.91, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.026700019836426, test_acc: 13.86, test_fscore: 11.02, time: 8.21 sec
loss: 8.167860984802246
loss: 8.297555923461914
loss: 8.056127548217773
epoch: 2, train_loss: 8.228500366210938, train_acc: 20.52, train_fscore: 17.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 8.056099891662598, test_acc: 43.38, test_fscore: 36.94, time: 7.59 sec
loss: 8.380401611328125
loss: 8.073204040527344
loss: 7.322385787963867
epoch: 3, train_loss: 8.242600440979004, train_acc: 33.41, train_fscore: 31.5, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.322400093078613, test_acc: 36.35, test_fscore: 33.48, time: 6.48 sec
loss: 7.547983169555664
loss: 7.3034281730651855
loss: 6.9947509765625
epoch: 4, train_loss: 7.434700012207031, train_acc: 37.42, train_fscore: 32.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.994800090789795, test_acc: 35.86, test_fscore: 30.24, time: 6.34 sec
loss: 7.083144664764404
loss: 7.087943077087402
loss: 7.098705291748047
epoch: 5, train_loss: 7.085400104522705, train_acc: 45.51, train_fscore: 43.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 7.098700046539307, test_acc: 40.11, test_fscore: 38.85, time: 6.3 sec
loss: 7.1252946853637695
loss: 6.946427822113037
loss: 6.722417831420898
epoch: 6, train_loss: 7.043700218200684, train_acc: 47.76, train_fscore: 46.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.722400188446045, test_acc: 44.61, test_fscore: 43.93, time: 6.32 sec
loss: 6.792521953582764
loss: 6.566093921661377
loss: 6.440838813781738
epoch: 7, train_loss: 6.689599990844727, train_acc: 51.19, train_fscore: 50.03, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.440800189971924, test_acc: 50.28, test_fscore: 50.19, time: 6.93 sec
loss: 6.524993419647217
loss: 6.417187690734863
loss: 6.438070297241211
epoch: 8, train_loss: 6.4721999168396, train_acc: 53.94, train_fscore: 53.07, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.4380998611450195, test_acc: 50.65, test_fscore: 51.05, time: 5.5 sec
loss: 6.386147975921631
loss: 6.493448257446289
loss: 6.291845798492432
epoch: 9, train_loss: 6.438499927520752, train_acc: 52.46, train_fscore: 51.76, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.291800022125244, test_acc: 50.46, test_fscore: 50.75, time: 6.47 sec
loss: 6.140016078948975
loss: 6.334453105926514
loss: 6.069751739501953
epoch: 10, train_loss: 6.231500148773193, train_acc: 54.65, train_fscore: 53.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 6.069799900054932, test_acc: 53.11, test_fscore: 53.23, time: 6.37 sec
              precision    recall  f1-score   support

           0     0.2273    0.3472    0.2747     144.0
           1     0.7619    0.7837    0.7726     245.0
           2     0.4638    0.3672    0.4099     384.0
           3     0.4406    0.7412    0.5526     170.0
           4     0.6762    0.6355    0.6552     299.0
           5     0.5821    0.4278    0.4932     381.0

    accuracy                         0.5311    1623.0
   macro avg     0.5253    0.5504    0.5264    1623.0
weighted avg     0.5523    0.5311    0.5323    1623.0

[[ 50.   8.  15.  10.  61.   0.]
 [ 14. 192.  16.   3.   3.  17.]
 [ 96.  27. 141.  33.   8.  79.]
 [  0.   5.  14. 126.   8.  17.]
 [ 50.   3.  41.  11. 190.   4.]
 [ 10.  17.  77. 103.  11. 163.]]
loss: 6.017071723937988
loss: 5.904372692108154
loss: 5.912617206573486
epoch: 11, train_loss: 5.9633002281188965, train_acc: 59.97, train_fscore: 59.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.912600040435791, test_acc: 55.02, test_fscore: 54.64, time: 6.46 sec
loss: 5.83046817779541
loss: 5.7199859619140625
loss: 5.737215042114258
epoch: 12, train_loss: 5.776899814605713, train_acc: 60.74, train_fscore: 59.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.737199783325195, test_acc: 55.51, test_fscore: 54.79, time: 6.41 sec
loss: 5.631709575653076
loss: 5.548588752746582
loss: 5.492433547973633
epoch: 13, train_loss: 5.5929999351501465, train_acc: 60.9, train_fscore: 59.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.492400169372559, test_acc: 59.15, test_fscore: 59.13, time: 5.95 sec
loss: 5.533610820770264
loss: 5.199461936950684
loss: 5.299630641937256
epoch: 14, train_loss: 5.375699996948242, train_acc: 63.46, train_fscore: 62.19, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.299600124359131, test_acc: 61.31, test_fscore: 61.28, time: 6.48 sec
loss: 5.256442546844482
loss: 5.137398719787598
loss: 5.154216289520264
epoch: 15, train_loss: 5.200399875640869, train_acc: 63.65, train_fscore: 62.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.154200077056885, test_acc: 62.78, test_fscore: 62.67, time: 6.99 sec
loss: 5.195814609527588
loss: 4.989741802215576
loss: 5.035808563232422
epoch: 16, train_loss: 5.101099967956543, train_acc: 63.8, train_fscore: 62.45, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 5.035799980163574, test_acc: 63.71, test_fscore: 63.65, time: 6.14 sec
loss: 5.182317733764648
loss: 4.7960524559021
loss: 4.903723239898682
epoch: 17, train_loss: 4.9984002113342285, train_acc: 64.89, train_fscore: 63.59, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.90369987487793, test_acc: 63.22, test_fscore: 63.93, time: 5.63 sec
loss: 4.949894428253174
loss: 4.731556415557861
loss: 4.773099899291992
epoch: 18, train_loss: 4.8541998863220215, train_acc: 65.49, train_fscore: 64.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.773099899291992, test_acc: 61.74, test_fscore: 62.65, time: 6.93 sec
loss: 4.689141273498535
loss: 4.8244242668151855
loss: 4.679923057556152
epoch: 19, train_loss: 4.756400108337402, train_acc: 65.27, train_fscore: 64.68, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.679900169372559, test_acc: 61.0, test_fscore: 61.98, time: 6.95 sec
loss: 4.776417255401611
loss: 4.57680606842041
loss: 4.592624187469482
epoch: 20, train_loss: 4.684899806976318, train_acc: 65.16, train_fscore: 64.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.592599868774414, test_acc: 62.78, test_fscore: 63.67, time: 6.19 sec
              precision    recall  f1-score   support

           0     0.3191    0.5694    0.4090     144.0
           1     0.7627    0.7347    0.7484     245.0
           2     0.6456    0.4792    0.5501     384.0
           3     0.5981    0.7529    0.6667     170.0
           4     0.8448    0.6555    0.7382     299.0
           5     0.6416    0.6719    0.6564     381.0

    accuracy                         0.6322    1623.0
   macro avg     0.6353    0.6439    0.6281    1623.0
weighted avg     0.6651    0.6322    0.6393    1623.0

[[ 82.  12.  13.   4.  33.   0.]
 [  4. 180.  30.   2.   0.  29.]
 [ 68.  29. 184.  25.   2.  76.]
 [  0.   0.   5. 128.   0.  37.]
 [ 94.   0.   8.   0. 196.   1.]
 [  9.  15.  45.  55.   1. 256.]]
loss: 4.693817615509033
loss: 4.423353672027588
loss: 4.537102222442627
epoch: 21, train_loss: 4.578400135040283, train_acc: 64.99, train_fscore: 64.36, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.537099838256836, test_acc: 63.59, test_fscore: 63.84, time: 6.19 sec
loss: 4.4998602867126465
loss: 4.599094390869141
loss: 4.456036567687988
epoch: 22, train_loss: 4.546500205993652, train_acc: 65.01, train_fscore: 64.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.455999851226807, test_acc: 64.51, test_fscore: 64.73, time: 7.3 sec
loss: 4.373059272766113
loss: 4.559464454650879
loss: 4.377903461456299
epoch: 23, train_loss: 4.4629998207092285, train_acc: 65.68, train_fscore: 64.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.377900123596191, test_acc: 65.19, test_fscore: 65.57, time: 6.4 sec
loss: 4.449177265167236
loss: 4.292784690856934
loss: 4.348913669586182
epoch: 24, train_loss: 4.3805999755859375, train_acc: 66.7, train_fscore: 65.92, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.348899841308594, test_acc: 65.56, test_fscore: 65.96, time: 6.25 sec
loss: 4.300998687744141
loss: 4.377676010131836
loss: 4.3315110206604
epoch: 25, train_loss: 4.335100173950195, train_acc: 66.8, train_fscore: 66.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.331500053405762, test_acc: 64.26, test_fscore: 64.69, time: 6.86 sec
loss: 4.350312232971191
loss: 4.224301815032959
loss: 4.326371192932129
epoch: 26, train_loss: 4.291900157928467, train_acc: 67.37, train_fscore: 66.55, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.326399803161621, test_acc: 64.08, test_fscore: 64.55, time: 5.61 sec
loss: 4.161752700805664
loss: 4.301384925842285
loss: 4.289007663726807
epoch: 27, train_loss: 4.227700233459473, train_acc: 67.88, train_fscore: 67.12, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.289000034332275, test_acc: 64.63, test_fscore: 65.07, time: 5.87 sec
loss: 4.2956719398498535
loss: 4.044418811798096
loss: 4.239586353302002
epoch: 28, train_loss: 4.174900054931641, train_acc: 68.02, train_fscore: 67.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.23960018157959, test_acc: 64.76, test_fscore: 65.21, time: 6.63 sec
loss: 4.116533279418945
loss: 4.159914493560791
loss: 4.19401741027832
epoch: 29, train_loss: 4.136000156402588, train_acc: 67.97, train_fscore: 67.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.193999767303467, test_acc: 65.13, test_fscore: 65.46, time: 5.72 sec
loss: 4.182416915893555
loss: 3.9852142333984375
loss: 4.170424938201904
epoch: 30, train_loss: 4.092400074005127, train_acc: 68.97, train_fscore: 68.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.170400142669678, test_acc: 65.74, test_fscore: 66.13, time: 6.69 sec
              precision    recall  f1-score   support

           0     0.4103    0.5556    0.4720     144.0
           1     0.7554    0.7184    0.7364     245.0
           2     0.6758    0.5755    0.6217     384.0
           3     0.5634    0.7059    0.6266     170.0
           4     0.8381    0.7793    0.8076     299.0
           5     0.6286    0.6220    0.6253     381.0

    accuracy                         0.6574    1623.0
   macro avg     0.6453    0.6594    0.6483    1623.0
weighted avg     0.6713    0.6574    0.6613    1623.0

[[ 80.   9.  14.   4.  35.   2.]
 [  4. 176.  31.   0.   0.  34.]
 [ 51.  25. 221.  26.   4.  57.]
 [  0.   0.   4. 120.   0.  46.]
 [ 58.   0.   7.   0. 233.   1.]
 [  2.  23.  50.  63.   6. 237.]]
loss: 3.9573450088500977
loss: 4.176904201507568
loss: 4.137547016143799
epoch: 31, train_loss: 4.06689977645874, train_acc: 69.14, train_fscore: 68.58, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.137499809265137, test_acc: 66.42, test_fscore: 66.79, time: 5.94 sec
loss: 4.119054794311523
loss: 3.853337049484253
loss: 4.101414203643799
epoch: 32, train_loss: 3.9946000576019287, train_acc: 69.48, train_fscore: 68.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.101399898529053, test_acc: 66.85, test_fscore: 67.3, time: 5.94 sec
loss: 3.881107807159424
loss: 4.052643299102783
loss: 4.0505266189575195
epoch: 33, train_loss: 3.9655001163482666, train_acc: 69.83, train_fscore: 69.38, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.05049991607666, test_acc: 66.67, test_fscore: 67.17, time: 6.03 sec
loss: 3.8523545265197754
loss: 4.012181758880615
loss: 4.018670082092285
epoch: 34, train_loss: 3.924499988555908, train_acc: 69.86, train_fscore: 69.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 4.018700122833252, test_acc: 67.16, test_fscore: 67.48, time: 6.08 sec
loss: 3.8717918395996094
loss: 3.874417304992676
loss: 3.9810492992401123
epoch: 35, train_loss: 3.872999906539917, train_acc: 70.72, train_fscore: 70.27, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9809999465942383, test_acc: 66.91, test_fscore: 67.19, time: 6.05 sec
loss: 3.6500353813171387
loss: 4.045773506164551
loss: 3.9629507064819336
epoch: 36, train_loss: 3.8341000080108643, train_acc: 71.34, train_fscore: 70.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.9630000591278076, test_acc: 67.59, test_fscore: 67.81, time: 6.51 sec
loss: 3.859933853149414
loss: 3.7074878215789795
loss: 3.9427216053009033
epoch: 37, train_loss: 3.790800094604492, train_acc: 71.17, train_fscore: 70.75, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.942699909210205, test_acc: 67.65, test_fscore: 67.97, time: 5.88 sec
loss: 3.8239386081695557
loss: 3.6998112201690674
loss: 3.8930532932281494
epoch: 38, train_loss: 3.7685000896453857, train_acc: 71.39, train_fscore: 71.06, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8931000232696533, test_acc: 67.96, test_fscore: 68.29, time: 7.18 sec
loss: 3.7800934314727783
loss: 3.6562676429748535
loss: 3.842301368713379
epoch: 39, train_loss: 3.725800037384033, train_acc: 71.81, train_fscore: 71.42, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.8422999382019043, test_acc: 67.71, test_fscore: 68.08, time: 6.0 sec
loss: 3.7801036834716797
loss: 3.57472562789917
loss: 3.840610980987549
epoch: 40, train_loss: 3.6839001178741455, train_acc: 71.86, train_fscore: 71.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.84060001373291, test_acc: 68.39, test_fscore: 68.79, time: 6.08 sec
              precision    recall  f1-score   support

           0     0.4490    0.6111    0.5176     144.0
           1     0.7635    0.7510    0.7572     245.0
           2     0.7069    0.6406    0.6721     384.0
           3     0.6158    0.6882    0.6500     170.0
           4     0.8513    0.7659    0.8063     299.0
           5     0.6491    0.6457    0.6474     381.0

    accuracy                         0.6839    1623.0
   macro avg     0.6726    0.6838    0.6751    1623.0
weighted avg     0.6960    0.6839    0.6879    1623.0

[[ 88.   9.  15.   3.  29.   0.]
 [  4. 184.  22.   0.   0.  35.]
 [ 43.  23. 246.  20.   5.  47.]
 [  0.   0.   3. 117.   0.  50.]
 [ 60.   1.   8.   0. 229.   1.]
 [  1.  24.  54.  50.   6. 246.]]
loss: 3.7143936157226562
loss: 3.648179769515991
loss: 3.793433427810669
epoch: 41, train_loss: 3.6846001148223877, train_acc: 71.79, train_fscore: 71.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7934000492095947, test_acc: 68.64, test_fscore: 68.93, time: 6.37 sec
loss: 3.719187021255493
loss: 3.537170171737671
loss: 3.7791390419006348
epoch: 42, train_loss: 3.6333000659942627, train_acc: 71.91, train_fscore: 71.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.779099941253662, test_acc: 68.76, test_fscore: 69.13, time: 6.82 sec
loss: 3.510369062423706
loss: 3.660646677017212
loss: 3.760263442993164
epoch: 43, train_loss: 3.5808000564575195, train_acc: 72.65, train_fscore: 72.33, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7602999210357666, test_acc: 68.7, test_fscore: 69.07, time: 6.27 sec
loss: 3.5633187294006348
loss: 3.598081111907959
loss: 3.7394959926605225
epoch: 44, train_loss: 3.5792999267578125, train_acc: 72.31, train_fscore: 71.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.739500045776367, test_acc: 68.39, test_fscore: 68.77, time: 6.2 sec
loss: 3.4508113861083984
loss: 3.614959955215454
loss: 3.720653533935547
epoch: 45, train_loss: 3.525899887084961, train_acc: 73.6, train_fscore: 73.32, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.7207000255584717, test_acc: 69.25, test_fscore: 69.66, time: 6.21 sec
loss: 3.459481716156006
loss: 3.5782370567321777
loss: 3.695692777633667
epoch: 46, train_loss: 3.515500068664551, train_acc: 73.55, train_fscore: 73.29, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.69569993019104, test_acc: 68.95, test_fscore: 69.3, time: 6.42 sec
loss: 3.4655370712280273
loss: 3.5155835151672363
loss: 3.6771340370178223
epoch: 47, train_loss: 3.4883999824523926, train_acc: 73.27, train_fscore: 72.97, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6770999431610107, test_acc: 68.95, test_fscore: 69.31, time: 6.41 sec
loss: 3.3984546661376953
loss: 3.487184762954712
loss: 3.659808874130249
epoch: 48, train_loss: 3.4391000270843506, train_acc: 74.08, train_fscore: 73.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6598000526428223, test_acc: 69.01, test_fscore: 69.38, time: 5.62 sec
loss: 3.4347357749938965
loss: 3.42999267578125
loss: 3.657439708709717
epoch: 49, train_loss: 3.4323999881744385, train_acc: 74.27, train_fscore: 73.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.657399892807007, test_acc: 68.95, test_fscore: 69.35, time: 7.17 sec
loss: 3.436556100845337
loss: 3.3596031665802
loss: 3.6612119674682617
epoch: 50, train_loss: 3.40120005607605, train_acc: 74.41, train_fscore: 74.12, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.6612000465393066, test_acc: 68.82, test_fscore: 69.22, time: 6.84 sec
              precision    recall  f1-score   support

           0     0.4650    0.6458    0.5407     144.0
           1     0.7833    0.7673    0.7753     245.0
           2     0.7054    0.6484    0.6757     384.0
           3     0.6198    0.7000    0.6575     170.0
           4     0.8636    0.7625    0.8099     299.0
           5     0.6604    0.6483    0.6543     381.0

    accuracy                         0.6925    1623.0
   macro avg     0.6829    0.6954    0.6856    1623.0
weighted avg     0.7055    0.6925    0.6966    1623.0

[[ 93.   9.  16.   1.  25.   0.]
 [  3. 188.  21.   0.   0.  33.]
 [ 43.  21. 249.  21.   5.  45.]
 [  0.   0.   3. 119.   0.  48.]
 [ 61.   1.   8.   0. 228.   1.]
 [  0.  21.  56.  51.   6. 247.]]
loss: 3.48392391204834
loss: 3.246628999710083
loss: 3.61552095413208
epoch: 51, train_loss: 3.3815999031066895, train_acc: 74.75, train_fscore: 74.52, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.615499973297119, test_acc: 69.38, test_fscore: 69.77, time: 5.58 sec
loss: 3.418323040008545
loss: 3.283647060394287
loss: 3.5782768726348877
epoch: 52, train_loss: 3.356100082397461, train_acc: 74.99, train_fscore: 74.77, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5782999992370605, test_acc: 69.25, test_fscore: 69.66, time: 5.84 sec
loss: 3.356945037841797
loss: 3.313040018081665
loss: 3.5842621326446533
epoch: 53, train_loss: 3.336699962615967, train_acc: 74.42, train_fscore: 74.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5843000411987305, test_acc: 69.25, test_fscore: 69.61, time: 6.63 sec
loss: 3.1956605911254883
loss: 3.4388060569763184
loss: 3.558722496032715
epoch: 54, train_loss: 3.314500093460083, train_acc: 74.92, train_fscore: 74.67, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5587000846862793, test_acc: 69.44, test_fscore: 69.82, time: 6.24 sec
loss: 3.3602964878082275
loss: 3.2508065700531006
loss: 3.544320821762085
epoch: 55, train_loss: 3.308500051498413, train_acc: 74.53, train_fscore: 74.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.544300079345703, test_acc: 68.95, test_fscore: 69.3, time: 5.86 sec
loss: 3.235910415649414
loss: 3.328216075897217
loss: 3.5448219776153564
epoch: 56, train_loss: 3.2799999713897705, train_acc: 75.13, train_fscore: 74.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.544800043106079, test_acc: 69.5, test_fscore: 69.88, time: 6.51 sec
loss: 3.1745126247406006
loss: 3.3309216499328613
loss: 3.5078585147857666
epoch: 57, train_loss: 3.2469000816345215, train_acc: 75.82, train_fscore: 75.66, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.5078999996185303, test_acc: 69.75, test_fscore: 70.14, time: 5.89 sec
loss: 3.285057544708252
loss: 3.209296941757202
loss: 3.4937639236450195
epoch: 58, train_loss: 3.249500036239624, train_acc: 75.15, train_fscore: 74.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.493799924850464, test_acc: 69.5, test_fscore: 69.9, time: 6.23 sec
loss: 3.2300891876220703
loss: 3.227367639541626
loss: 3.491273880004883
epoch: 59, train_loss: 3.228800058364868, train_acc: 75.87, train_fscore: 75.62, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.491300106048584, test_acc: 69.56, test_fscore: 70.01, time: 5.16 sec
loss: 3.276163339614868
loss: 3.1137590408325195
loss: 3.4724478721618652
epoch: 60, train_loss: 3.1981000900268555, train_acc: 76.3, train_fscore: 76.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.472399950027466, test_acc: 70.06, test_fscore: 70.39, time: 3.54 sec
              precision    recall  f1-score   support

           0     0.4951    0.7083    0.5829     144.0
           1     0.7932    0.7673    0.7801     245.0
           2     0.7233    0.6536    0.6867     384.0
           3     0.5973    0.7765    0.6752     170.0
           4     0.8750    0.7726    0.8206     299.0
           5     0.6695    0.6115    0.6392     381.0

    accuracy                         0.7006    1623.0
   macro avg     0.6923    0.7150    0.6974    1623.0
weighted avg     0.7158    0.7006    0.7039    1623.0

[[102.   9.  12.   1.  20.   0.]
 [  2. 188.  21.   0.   0.  34.]
 [ 44.  19. 251.  21.   6.  43.]
 [  0.   0.   1. 132.   0.  37.]
 [ 57.   2.   8.   0. 231.   1.]
 [  1.  19.  54.  67.   7. 233.]]
loss: 3.347568988800049
loss: 2.9898681640625
loss: 3.495008945465088
epoch: 61, train_loss: 3.181299924850464, train_acc: 76.23, train_fscore: 76.02, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.494999885559082, test_acc: 70.24, test_fscore: 70.63, time: 4.38 sec
loss: 3.2492313385009766
loss: 3.0446877479553223
loss: 3.4678690433502197
epoch: 62, train_loss: 3.1531999111175537, train_acc: 77.01, train_fscore: 76.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.467900037765503, test_acc: 70.3, test_fscore: 70.72, time: 4.57 sec
loss: 3.232203722000122
loss: 3.0452938079833984
loss: 3.427070140838623
epoch: 63, train_loss: 3.1435999870300293, train_acc: 76.49, train_fscore: 76.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4270999431610107, test_acc: 69.81, test_fscore: 70.21, time: 4.99 sec
loss: 3.430875539779663
loss: 2.8246536254882812
loss: 3.449514865875244
epoch: 64, train_loss: 3.1419999599456787, train_acc: 76.47, train_fscore: 76.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.44950008392334, test_acc: 69.93, test_fscore: 70.38, time: 6.33 sec
loss: 3.1941800117492676
loss: 3.0508110523223877
loss: 3.446627140045166
epoch: 65, train_loss: 3.1250998973846436, train_acc: 76.21, train_fscore: 76.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4465999603271484, test_acc: 70.24, test_fscore: 70.64, time: 6.14 sec
loss: 2.9695467948913574
loss: 3.2788310050964355
loss: 3.4439048767089844
epoch: 66, train_loss: 3.1108999252319336, train_acc: 77.19, train_fscore: 77.05, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4439001083374023, test_acc: 70.36, test_fscore: 70.76, time: 6.65 sec
loss: 2.921827793121338
loss: 3.194995403289795
loss: 3.448956251144409
epoch: 67, train_loss: 3.0536000728607178, train_acc: 77.37, train_fscore: 77.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4489998817443848, test_acc: 69.99, test_fscore: 70.42, time: 5.82 sec
loss: 3.1827120780944824
loss: 2.942091703414917
loss: 3.4067704677581787
epoch: 68, train_loss: 3.0752999782562256, train_acc: 77.42, train_fscore: 77.24, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4068000316619873, test_acc: 70.98, test_fscore: 71.38, time: 6.3 sec
loss: 3.1004295349121094
loss: 2.9900522232055664
loss: 3.406080722808838
epoch: 69, train_loss: 3.0485999584198, train_acc: 78.09, train_fscore: 77.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.406100034713745, test_acc: 70.61, test_fscore: 71.02, time: 6.29 sec
loss: 2.9772963523864746
loss: 3.0866074562072754
loss: 3.418541431427002
epoch: 70, train_loss: 3.02839994430542, train_acc: 78.0, train_fscore: 77.85, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.4184999465942383, test_acc: 70.43, test_fscore: 70.83, time: 6.02 sec
              precision    recall  f1-score   support

           0     0.5000    0.7500    0.6000     144.0
           1     0.8095    0.7633    0.7857     245.0
           2     0.7299    0.6615    0.6940     384.0
           3     0.6359    0.7294    0.6795     170.0
           4     0.8764    0.7592    0.8136     299.0
           5     0.6738    0.6614    0.6675     381.0

    accuracy                         0.7098    1623.0
   macro avg     0.7043    0.7208    0.7067    1623.0
weighted avg     0.7255    0.7098    0.7138    1623.0

[[108.   7.  11.   0.  18.   0.]
 [  2. 187.  22.   0.   0.  34.]
 [ 44.  18. 254.  19.   6.  43.]
 [  0.   0.   2. 124.   0.  44.]
 [ 61.   2.   8.   0. 227.   1.]
 [  1.  17.  51.  52.   8. 252.]]
loss: 3.0567002296447754
loss: 2.980473041534424
loss: 3.3957598209381104
epoch: 71, train_loss: 3.0183000564575195, train_acc: 77.64, train_fscore: 77.5, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3958001136779785, test_acc: 70.49, test_fscore: 70.88, time: 6.07 sec
loss: 3.0537919998168945
loss: 2.951460838317871
loss: 3.371408462524414
epoch: 72, train_loss: 3.005500078201294, train_acc: 77.92, train_fscore: 77.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3714001178741455, test_acc: 70.49, test_fscore: 70.86, time: 7.25 sec
loss: 3.1237292289733887
loss: 2.8478569984436035
loss: 3.3563027381896973
epoch: 73, train_loss: 2.9946999549865723, train_acc: 77.95, train_fscore: 77.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.356300115585327, test_acc: 70.61, test_fscore: 71.03, time: 6.19 sec
loss: 2.984652519226074
loss: 2.9730887413024902
loss: 3.368459701538086
epoch: 74, train_loss: 2.979300022125244, train_acc: 78.5, train_fscore: 78.39, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.368499994277954, test_acc: 70.92, test_fscore: 71.17, time: 5.64 sec
loss: 3.005869150161743
loss: 2.941699981689453
loss: 3.357287883758545
epoch: 75, train_loss: 2.976099967956543, train_acc: 78.67, train_fscore: 78.49, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.357300043106079, test_acc: 70.24, test_fscore: 70.65, time: 5.99 sec
loss: 2.797619581222534
loss: 3.1137077808380127
loss: 3.353363037109375
epoch: 76, train_loss: 2.9502999782562256, train_acc: 78.33, train_fscore: 78.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3533999919891357, test_acc: 70.49, test_fscore: 70.88, time: 6.38 sec
loss: 2.8734841346740723
loss: 3.0073800086975098
loss: 3.347151041030884
epoch: 77, train_loss: 2.938999891281128, train_acc: 78.9, train_fscore: 78.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3471999168395996, test_acc: 70.79, test_fscore: 71.18, time: 7.04 sec
loss: 2.8337903022766113
loss: 3.013604164123535
loss: 3.3423187732696533
epoch: 78, train_loss: 2.918600082397461, train_acc: 79.12, train_fscore: 79.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3422999382019043, test_acc: 70.36, test_fscore: 70.76, time: 6.05 sec
loss: 2.7825684547424316
loss: 3.0892832279205322
loss: 3.3563649654388428
epoch: 79, train_loss: 2.920099973678589, train_acc: 79.14, train_fscore: 78.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3564000129699707, test_acc: 70.86, test_fscore: 71.2, time: 6.96 sec
loss: 2.8783466815948486
loss: 2.890425682067871
loss: 3.3665411472320557
epoch: 80, train_loss: 2.8838000297546387, train_acc: 79.21, train_fscore: 79.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.366499900817871, test_acc: 70.79, test_fscore: 71.17, time: 5.75 sec
              precision    recall  f1-score   support

           0     0.5000    0.7500    0.6000     144.0
           1     0.8095    0.7633    0.7857     245.0
           2     0.7299    0.6615    0.6940     384.0
           3     0.6359    0.7294    0.6795     170.0
           4     0.8764    0.7592    0.8136     299.0
           5     0.6738    0.6614    0.6675     381.0

    accuracy                         0.7098    1623.0
   macro avg     0.7043    0.7208    0.7067    1623.0
weighted avg     0.7255    0.7098    0.7138    1623.0

[[108.   7.  11.   0.  18.   0.]
 [  2. 187.  22.   0.   0.  34.]
 [ 44.  18. 254.  19.   6.  43.]
 [  0.   0.   2. 124.   0.  44.]
 [ 61.   2.   8.   0. 227.   1.]
 [  1.  17.  51.  52.   8. 252.]]
loss: 2.8951430320739746
loss: 2.8861355781555176
loss: 3.368943691253662
epoch: 81, train_loss: 2.8907999992370605, train_acc: 78.86, train_fscore: 78.69, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3689000606536865, test_acc: 70.92, test_fscore: 71.33, time: 6.31 sec
loss: 2.7195565700531006
loss: 3.04599928855896
loss: 3.3319332599639893
epoch: 82, train_loss: 2.868000030517578, train_acc: 78.8, train_fscore: 78.66, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.331899881362915, test_acc: 71.23, test_fscore: 71.58, time: 6.06 sec
loss: 2.9463376998901367
loss: 2.7833895683288574
loss: 3.319908857345581
epoch: 83, train_loss: 2.868799924850464, train_acc: 79.67, train_fscore: 79.54, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3199000358581543, test_acc: 70.49, test_fscore: 70.88, time: 6.13 sec
loss: 2.845432758331299
loss: 2.9048473834991455
loss: 3.3349921703338623
epoch: 84, train_loss: 2.873500108718872, train_acc: 79.5, train_fscore: 79.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3350000381469727, test_acc: 71.16, test_fscore: 71.53, time: 7.56 sec
loss: 2.669617176055908
loss: 3.029078960418701
loss: 3.345968246459961
epoch: 85, train_loss: 2.8324999809265137, train_acc: 79.72, train_fscore: 79.61, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3459999561309814, test_acc: 70.86, test_fscore: 71.16, time: 6.4 sec
loss: 2.9620091915130615
loss: 2.681793689727783
loss: 3.3336293697357178
epoch: 86, train_loss: 2.8375000953674316, train_acc: 79.38, train_fscore: 79.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3336000442504883, test_acc: 70.92, test_fscore: 71.28, time: 6.01 sec
loss: 2.7954306602478027
loss: 2.832731246948242
loss: 3.338927745819092
epoch: 87, train_loss: 2.8125, train_acc: 79.97, train_fscore: 79.85, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.338900089263916, test_acc: 70.92, test_fscore: 71.32, time: 7.51 sec
loss: 2.7938036918640137
loss: 2.8557426929473877
loss: 3.322598457336426
epoch: 88, train_loss: 2.822200059890747, train_acc: 80.05, train_fscore: 79.93, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3225998878479004, test_acc: 71.53, test_fscore: 71.9, time: 5.82 sec
loss: 2.762916326522827
loss: 2.8425536155700684
loss: 3.308093547821045
epoch: 89, train_loss: 2.8011999130249023, train_acc: 79.91, train_fscore: 79.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3080999851226807, test_acc: 71.41, test_fscore: 71.69, time: 5.84 sec
loss: 2.8978946208953857
loss: 2.695991039276123
loss: 3.324913263320923
epoch: 90, train_loss: 2.799299955368042, train_acc: 80.07, train_fscore: 79.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.324899911880493, test_acc: 71.29, test_fscore: 71.68, time: 6.49 sec
              precision    recall  f1-score   support

           0     0.5135    0.7917    0.6230     144.0
           1     0.8139    0.7673    0.7899     245.0
           2     0.7385    0.6693    0.7022     384.0
           3     0.6528    0.7412    0.6942     170.0
           4     0.8789    0.7525    0.8108     299.0
           5     0.6729    0.6588    0.6658     381.0

    accuracy                         0.7153    1623.0
   macro avg     0.7118    0.7301    0.7143    1623.0
weighted avg     0.7314    0.7153    0.7190    1623.0

[[114.   6.   7.   0.  17.   0.]
 [  3. 188.  20.   0.   0.  34.]
 [ 42.  18. 257.  16.   6.  45.]
 [  0.   0.   2. 126.   0.  42.]
 [ 62.   2.   9.   0. 225.   1.]
 [  1.  17.  53.  51.   8. 251.]]
loss: 2.588690757751465
loss: 2.951974868774414
loss: 3.3310890197753906
epoch: 91, train_loss: 2.760999917984009, train_acc: 80.19, train_fscore: 80.06, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3310999870300293, test_acc: 71.04, test_fscore: 71.36, time: 5.98 sec
loss: 2.631010055541992
loss: 2.922740936279297
loss: 3.3093671798706055
epoch: 92, train_loss: 2.7565999031066895, train_acc: 80.79, train_fscore: 80.66, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3094000816345215, test_acc: 71.1, test_fscore: 71.45, time: 5.85 sec
loss: 2.9031662940979004
loss: 2.6079890727996826
loss: 3.313793659210205
epoch: 93, train_loss: 2.7757999897003174, train_acc: 80.02, train_fscore: 79.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.313800096511841, test_acc: 70.98, test_fscore: 71.35, time: 6.24 sec
loss: 2.7687418460845947
loss: 2.6916370391845703
loss: 3.316404104232788
epoch: 94, train_loss: 2.7323999404907227, train_acc: 80.07, train_fscore: 79.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3164000511169434, test_acc: 70.86, test_fscore: 71.15, time: 6.29 sec
loss: 2.76261305809021
loss: 2.6770212650299072
loss: 3.3162312507629395
epoch: 95, train_loss: 2.721400022506714, train_acc: 80.91, train_fscore: 80.81, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.316200017929077, test_acc: 70.43, test_fscore: 70.79, time: 6.19 sec
loss: 2.7106924057006836
loss: 2.722404956817627
loss: 3.303558111190796
epoch: 96, train_loss: 2.716200113296509, train_acc: 80.81, train_fscore: 80.7, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3036000728607178, test_acc: 70.98, test_fscore: 71.3, time: 5.72 sec
loss: 2.670952558517456
loss: 2.7826318740844727
loss: 3.2978665828704834
epoch: 97, train_loss: 2.7249999046325684, train_acc: 80.67, train_fscore: 80.53, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2978999614715576, test_acc: 71.23, test_fscore: 71.54, time: 7.02 sec
loss: 2.8360178470611572
loss: 2.574235439300537
loss: 3.272090196609497
epoch: 98, train_loss: 2.7107999324798584, train_acc: 81.1, train_fscore: 80.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2720999717712402, test_acc: 71.23, test_fscore: 71.59, time: 6.18 sec
loss: 2.6989552974700928
loss: 2.6640818119049072
loss: 3.2909348011016846
epoch: 99, train_loss: 2.682300090789795, train_acc: 80.95, train_fscore: 80.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2908999919891357, test_acc: 71.9, test_fscore: 72.24, time: 6.26 sec
loss: 2.654353618621826
loss: 2.710235118865967
loss: 3.2768118381500244
epoch: 100, train_loss: 2.678100109100342, train_acc: 81.57, train_fscore: 81.46, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2767999172210693, test_acc: 71.29, test_fscore: 71.64, time: 6.34 sec
              precision    recall  f1-score   support

           0     0.5234    0.7778    0.6257     144.0
           1     0.8304    0.7796    0.8042     245.0
           2     0.7486    0.6745    0.7096     384.0
           3     0.6355    0.7588    0.6917     170.0
           4     0.8621    0.7525    0.8036     299.0
           5     0.6802    0.6588    0.6693     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7134    0.7337    0.7173    1623.0
weighted avg     0.7340    0.7190    0.7224    1623.0

[[112.   4.   7.   0.  21.   0.]
 [  3. 191.  15.   3.   0.  33.]
 [ 40.  17. 259.  17.   7.  44.]
 [  0.   0.   1. 129.   0.  40.]
 [ 58.   2.  13.   0. 225.   1.]
 [  1.  16.  51.  54.   8. 251.]]
loss: 2.6828229427337646
loss: 2.674257278442383
loss: 3.272235155105591
epoch: 101, train_loss: 2.6789000034332275, train_acc: 81.5, train_fscore: 81.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.272200107574463, test_acc: 71.41, test_fscore: 71.73, time: 6.84 sec
loss: 2.567441940307617
loss: 2.7528765201568604
loss: 3.304027557373047
epoch: 102, train_loss: 2.654400110244751, train_acc: 80.96, train_fscore: 80.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.303999900817871, test_acc: 71.29, test_fscore: 71.59, time: 4.87 sec
loss: 2.6066672801971436
loss: 2.6832950115203857
loss: 3.28549861907959
epoch: 103, train_loss: 2.6419999599456787, train_acc: 81.12, train_fscore: 80.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2855000495910645, test_acc: 70.73, test_fscore: 71.07, time: 6.45 sec
loss: 2.61344838142395
loss: 2.706655979156494
loss: 3.2779030799865723
epoch: 104, train_loss: 2.6570000648498535, train_acc: 81.08, train_fscore: 80.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.277899980545044, test_acc: 70.86, test_fscore: 71.18, time: 6.16 sec
loss: 2.516326427459717
loss: 2.7452621459960938
loss: 3.288060188293457
epoch: 105, train_loss: 2.6222000122070312, train_acc: 81.82, train_fscore: 81.7, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.288100004196167, test_acc: 71.23, test_fscore: 71.57, time: 5.99 sec
loss: 2.4861531257629395
loss: 2.769099712371826
loss: 3.302271604537964
epoch: 106, train_loss: 2.6217000484466553, train_acc: 81.88, train_fscore: 81.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.302299976348877, test_acc: 71.1, test_fscore: 71.43, time: 6.11 sec
loss: 2.651174545288086
loss: 2.5746734142303467
loss: 3.274801015853882
epoch: 107, train_loss: 2.615499973297119, train_acc: 82.31, train_fscore: 82.2, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2748000621795654, test_acc: 71.1, test_fscore: 71.4, time: 6.96 sec
loss: 2.577378511428833
loss: 2.634686231613159
loss: 3.2740135192871094
epoch: 108, train_loss: 2.604300022125244, train_acc: 82.0, train_fscore: 81.87, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2739999294281006, test_acc: 71.1, test_fscore: 71.42, time: 5.66 sec
loss: 2.665611743927002
loss: 2.5278658866882324
loss: 3.2642393112182617
epoch: 109, train_loss: 2.604099988937378, train_acc: 81.86, train_fscore: 81.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.26419997215271, test_acc: 71.53, test_fscore: 71.87, time: 6.45 sec
loss: 2.534618854522705
loss: 2.6180996894836426
loss: 3.2791779041290283
epoch: 110, train_loss: 2.5732998847961426, train_acc: 82.39, train_fscore: 82.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2792000770568848, test_acc: 71.23, test_fscore: 71.5, time: 7.08 sec
              precision    recall  f1-score   support

           0     0.5234    0.7778    0.6257     144.0
           1     0.8304    0.7796    0.8042     245.0
           2     0.7486    0.6745    0.7096     384.0
           3     0.6355    0.7588    0.6917     170.0
           4     0.8621    0.7525    0.8036     299.0
           5     0.6802    0.6588    0.6693     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7134    0.7337    0.7173    1623.0
weighted avg     0.7340    0.7190    0.7224    1623.0

[[112.   4.   7.   0.  21.   0.]
 [  3. 191.  15.   3.   0.  33.]
 [ 40.  17. 259.  17.   7.  44.]
 [  0.   0.   1. 129.   0.  40.]
 [ 58.   2.  13.   0. 225.   1.]
 [  1.  16.  51.  54.   8. 251.]]
loss: 2.69462513923645
loss: 2.4396159648895264
loss: 3.2767651081085205
epoch: 111, train_loss: 2.5787999629974365, train_acc: 82.32, train_fscore: 82.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2767999172210693, test_acc: 71.53, test_fscore: 71.84, time: 5.76 sec
loss: 2.3881866931915283
loss: 2.7719616889953613
loss: 3.275899887084961
epoch: 112, train_loss: 2.5676000118255615, train_acc: 82.51, train_fscore: 82.4, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.275899887084961, test_acc: 71.78, test_fscore: 72.09, time: 5.57 sec
loss: 2.594240188598633
loss: 2.5273733139038086
loss: 3.2551522254943848
epoch: 113, train_loss: 2.5631000995635986, train_acc: 82.07, train_fscore: 81.95, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.255199909210205, test_acc: 71.1, test_fscore: 71.43, time: 6.27 sec
loss: 2.653872013092041
loss: 2.5073328018188477
loss: 3.266505479812622
epoch: 114, train_loss: 2.581399917602539, train_acc: 82.6, train_fscore: 82.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2664999961853027, test_acc: 71.23, test_fscore: 71.56, time: 5.74 sec
loss: 2.5015830993652344
loss: 2.5733516216278076
loss: 3.306349039077759
epoch: 115, train_loss: 2.5357000827789307, train_acc: 83.1, train_fscore: 83.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.306299924850464, test_acc: 71.47, test_fscore: 71.75, time: 6.43 sec
loss: 2.6058268547058105
loss: 2.454735040664673
loss: 3.289917230606079
epoch: 116, train_loss: 2.5381999015808105, train_acc: 83.17, train_fscore: 83.04, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.289900064468384, test_acc: 71.47, test_fscore: 71.75, time: 7.26 sec
loss: 2.5364842414855957
loss: 2.5132088661193848
loss: 3.270005226135254
epoch: 117, train_loss: 2.5257999897003174, train_acc: 82.69, train_fscore: 82.6, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2699999809265137, test_acc: 71.1, test_fscore: 71.42, time: 6.0 sec
loss: 2.4941394329071045
loss: 2.58777117729187
loss: 3.2453761100769043
epoch: 118, train_loss: 2.5357000827789307, train_acc: 83.12, train_fscore: 83.01, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2453999519348145, test_acc: 71.53, test_fscore: 71.85, time: 6.21 sec
loss: 2.5189597606658936
loss: 2.477093458175659
loss: 3.289796829223633
epoch: 119, train_loss: 2.499000072479248, train_acc: 82.84, train_fscore: 82.74, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.289799928665161, test_acc: 71.6, test_fscore: 71.89, time: 6.21 sec
loss: 2.660238265991211
loss: 2.3684444427490234
loss: 3.2618651390075684
epoch: 120, train_loss: 2.522200107574463, train_acc: 83.2, train_fscore: 83.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.261899948120117, test_acc: 71.78, test_fscore: 72.17, time: 6.09 sec
              precision    recall  f1-score   support

           0     0.5234    0.7778    0.6257     144.0
           1     0.8304    0.7796    0.8042     245.0
           2     0.7486    0.6745    0.7096     384.0
           3     0.6355    0.7588    0.6917     170.0
           4     0.8621    0.7525    0.8036     299.0
           5     0.6802    0.6588    0.6693     381.0

    accuracy                         0.7190    1623.0
   macro avg     0.7134    0.7337    0.7173    1623.0
weighted avg     0.7340    0.7190    0.7224    1623.0

[[112.   4.   7.   0.  21.   0.]
 [  3. 191.  15.   3.   0.  33.]
 [ 40.  17. 259.  17.   7.  44.]
 [  0.   0.   1. 129.   0.  40.]
 [ 58.   2.  13.   0. 225.   1.]
 [  1.  16.  51.  54.   8. 251.]]
loss: 2.4799535274505615
loss: 2.494844436645508
loss: 3.2488036155700684
epoch: 121, train_loss: 2.4874000549316406, train_acc: 82.94, train_fscore: 82.83, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.248800039291382, test_acc: 71.53, test_fscore: 71.78, time: 5.46 sec
loss: 2.426954746246338
loss: 2.5473499298095703
loss: 3.270479440689087
epoch: 122, train_loss: 2.486599922180176, train_acc: 83.61, train_fscore: 83.51, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2704999446868896, test_acc: 71.78, test_fscore: 72.07, time: 6.47 sec
loss: 2.4361214637756348
loss: 2.507681369781494
loss: 3.2646920680999756
epoch: 123, train_loss: 2.469399929046631, train_acc: 83.58, train_fscore: 83.47, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.264699935913086, test_acc: 71.41, test_fscore: 71.71, time: 5.99 sec
loss: 2.559493064880371
loss: 2.3487424850463867
loss: 3.2990918159484863
epoch: 124, train_loss: 2.463599920272827, train_acc: 83.37, train_fscore: 83.25, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.299099922180176, test_acc: 71.84, test_fscore: 72.14, time: 6.47 sec
loss: 2.3689217567443848
loss: 2.5317206382751465
loss: 3.2962777614593506
epoch: 125, train_loss: 2.4442999362945557, train_acc: 83.58, train_fscore: 83.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.296299934387207, test_acc: 70.98, test_fscore: 71.25, time: 6.19 sec
loss: 2.5269365310668945
loss: 2.368241786956787
loss: 3.276073694229126
epoch: 126, train_loss: 2.4535000324249268, train_acc: 83.41, train_fscore: 83.3, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.276099920272827, test_acc: 71.72, test_fscore: 72.01, time: 6.44 sec
loss: 2.511876106262207
loss: 2.411284923553467
loss: 3.2780137062072754
epoch: 127, train_loss: 2.464099884033203, train_acc: 83.51, train_fscore: 83.41, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2780001163482666, test_acc: 71.84, test_fscore: 72.09, time: 6.61 sec
loss: 2.476170063018799
loss: 2.424386501312256
loss: 3.272787570953369
epoch: 128, train_loss: 2.4512999057769775, train_acc: 83.48, train_fscore: 83.35, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2727999687194824, test_acc: 71.72, test_fscore: 72.01, time: 6.74 sec
loss: 2.423121452331543
loss: 2.4206008911132812
loss: 3.304939031600952
epoch: 129, train_loss: 2.4219000339508057, train_acc: 83.58, train_fscore: 83.47, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3048999309539795, test_acc: 71.6, test_fscore: 71.9, time: 5.84 sec
loss: 2.278984785079956
loss: 2.5572798252105713
loss: 3.2836997509002686
epoch: 130, train_loss: 2.407099962234497, train_acc: 83.92, train_fscore: 83.82, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2836999893188477, test_acc: 72.03, test_fscore: 72.34, time: 6.94 sec
              precision    recall  f1-score   support

           0     0.5374    0.7986    0.6425     144.0
           1     0.8291    0.7918    0.8100     245.0
           2     0.7331    0.6797    0.7054     384.0
           3     0.6337    0.7529    0.6882     170.0
           4     0.8810    0.7425    0.8058     299.0
           5     0.6822    0.6535    0.6676     381.0

    accuracy                         0.7203    1623.0
   macro avg     0.7161    0.7365    0.7199    1623.0
weighted avg     0.7351    0.7203    0.7234    1623.0

[[115.   4.   6.   0.  19.   0.]
 [  3. 194.  16.   3.   0.  29.]
 [ 38.  18. 261.  16.   6.  45.]
 [  0.   0.   1. 128.   0.  41.]
 [ 58.   2.  16.   0. 222.   1.]
 [  0.  16.  56.  55.   5. 249.]]
loss: 2.4512791633605957
loss: 2.3869566917419434
loss: 3.271207332611084
epoch: 131, train_loss: 2.4217000007629395, train_acc: 83.96, train_fscore: 83.88, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.271199941635132, test_acc: 71.66, test_fscore: 71.94, time: 7.62 sec
loss: 2.5439672470092773
loss: 2.2391421794891357
loss: 3.281212568283081
epoch: 132, train_loss: 2.4026999473571777, train_acc: 84.32, train_fscore: 84.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2811999320983887, test_acc: 71.53, test_fscore: 71.8, time: 6.15 sec
loss: 2.3952629566192627
loss: 2.435957670211792
loss: 3.2928836345672607
epoch: 133, train_loss: 2.4142000675201416, train_acc: 83.98, train_fscore: 83.86, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2929000854492188, test_acc: 71.47, test_fscore: 71.77, time: 5.99 sec
loss: 2.3001325130462646
loss: 2.48824143409729
loss: 3.3023390769958496
epoch: 134, train_loss: 2.3882999420166016, train_acc: 84.58, train_fscore: 84.48, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.302299976348877, test_acc: 71.78, test_fscore: 72.07, time: 6.11 sec
loss: 2.3685078620910645
loss: 2.3866801261901855
loss: 3.311929941177368
epoch: 135, train_loss: 2.37719988822937, train_acc: 84.17, train_fscore: 84.08, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3118999004364014, test_acc: 71.66, test_fscore: 71.98, time: 5.25 sec
loss: 2.5411996841430664
loss: 2.1888813972473145
loss: 3.298135995864868
epoch: 136, train_loss: 2.3770999908447266, train_acc: 84.32, train_fscore: 84.23, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.298099994659424, test_acc: 71.66, test_fscore: 71.92, time: 6.1 sec
loss: 2.4503960609436035
loss: 2.2859725952148438
loss: 3.3242545127868652
epoch: 137, train_loss: 2.373800039291382, train_acc: 84.23, train_fscore: 84.14, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3243000507354736, test_acc: 71.6, test_fscore: 71.89, time: 6.4 sec
loss: 2.5270018577575684
loss: 2.20797061920166
loss: 3.2859859466552734
epoch: 138, train_loss: 2.384700059890747, train_acc: 84.39, train_fscore: 84.31, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2860000133514404, test_acc: 71.66, test_fscore: 71.98, time: 6.87 sec
loss: 2.399972677230835
loss: 2.2952027320861816
loss: 3.310335874557495
epoch: 139, train_loss: 2.3489999771118164, train_acc: 85.06, train_fscore: 84.96, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.31030011177063, test_acc: 72.4, test_fscore: 72.66, time: 6.68 sec
loss: 2.408390760421753
loss: 2.3200039863586426
loss: 3.3093032836914062
epoch: 140, train_loss: 2.3671000003814697, train_acc: 84.73, train_fscore: 84.64, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.309299945831299, test_acc: 72.03, test_fscore: 72.32, time: 5.97 sec
              precision    recall  f1-score   support

           0     0.5419    0.7639    0.6340     144.0
           1     0.8248    0.7878    0.8058     245.0
           2     0.7285    0.7057    0.7169     384.0
           3     0.6564    0.7529    0.7014     170.0
           4     0.8654    0.7525    0.8050     299.0
           5     0.6908    0.6509    0.6703     381.0

    accuracy                         0.7240    1623.0
   macro avg     0.7180    0.7356    0.7222    1623.0
weighted avg     0.7353    0.7240    0.7266    1623.0

[[110.   4.   6.   0.  24.   0.]
 [  2. 193.  18.   2.   0.  30.]
 [ 35.  20. 271.  12.   6.  40.]
 [  0.   0.   2. 128.   0.  40.]
 [ 56.   1.  16.   0. 225.   1.]
 [  0.  16.  59.  53.   5. 248.]]
loss: 2.346562385559082
loss: 2.343393325805664
loss: 3.3117542266845703
epoch: 141, train_loss: 2.3450000286102295, train_acc: 85.08, train_fscore: 84.99, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.311800003051758, test_acc: 71.29, test_fscore: 71.56, time: 5.64 sec
loss: 2.2604894638061523
loss: 2.417485237121582
loss: 3.327329158782959
epoch: 142, train_loss: 2.3326001167297363, train_acc: 85.23, train_fscore: 85.13, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3273000717163086, test_acc: 71.6, test_fscore: 71.85, time: 5.96 sec
loss: 2.380882740020752
loss: 2.296261787414551
loss: 3.3262643814086914
epoch: 143, train_loss: 2.341900110244751, train_acc: 84.84, train_fscore: 84.73, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3262999057769775, test_acc: 71.9, test_fscore: 72.22, time: 6.67 sec
loss: 2.3399276733398438
loss: 2.290825128555298
loss: 3.3171768188476562
epoch: 144, train_loss: 2.316200017929077, train_acc: 84.85, train_fscore: 84.79, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.317199945449829, test_acc: 71.78, test_fscore: 72.08, time: 6.54 sec
loss: 2.3587958812713623
loss: 2.24920654296875
loss: 3.306206226348877
epoch: 145, train_loss: 2.3060998916625977, train_acc: 84.91, train_fscore: 84.78, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3062000274658203, test_acc: 71.72, test_fscore: 72.0, time: 6.47 sec
loss: 2.202610492706299
loss: 2.4372849464416504
loss: 3.313783645629883
epoch: 146, train_loss: 2.3136000633239746, train_acc: 85.3, train_fscore: 85.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.313800096511841, test_acc: 71.84, test_fscore: 72.12, time: 6.24 sec
loss: 2.288381576538086
loss: 2.310814619064331
loss: 3.330453395843506
epoch: 147, train_loss: 2.2990000247955322, train_acc: 85.35, train_fscore: 85.26, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3304998874664307, test_acc: 72.09, test_fscore: 72.39, time: 6.61 sec
loss: 2.352619171142578
loss: 2.243938684463501
loss: 3.2920496463775635
epoch: 148, train_loss: 2.3027000427246094, train_acc: 85.2, train_fscore: 85.1, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.2920000553131104, test_acc: 71.41, test_fscore: 71.69, time: 5.75 sec
loss: 2.2758450508117676
loss: 2.305518627166748
loss: 3.315117359161377
epoch: 149, train_loss: 2.2904000282287598, train_acc: 85.34, train_fscore: 85.22, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.3150999546051025, test_acc: 71.53, test_fscore: 71.81, time: 6.3 sec
loss: 2.1144027709960938
loss: 2.4879138469696045
loss: 3.316695213317871
epoch: 150, train_loss: 2.291599988937378, train_acc: 85.32, train_fscore: 85.21, valid_loss: nan, valid_acc: nan, valid_fscore: nan, test_loss: 3.316699981689453, test_acc: 71.53, test_fscore: 71.83, time: 5.46 sec
              precision    recall  f1-score   support

           0     0.5419    0.7639    0.6340     144.0
           1     0.8248    0.7878    0.8058     245.0
           2     0.7285    0.7057    0.7169     384.0
           3     0.6564    0.7529    0.7014     170.0
           4     0.8654    0.7525    0.8050     299.0
           5     0.6908    0.6509    0.6703     381.0

    accuracy                         0.7240    1623.0
   macro avg     0.7180    0.7356    0.7222    1623.0
weighted avg     0.7353    0.7240    0.7266    1623.0

[[110.   4.   6.   0.  24.   0.]
 [  2. 193.  18.   2.   0.  30.]
 [ 35.  20. 271.  12.   6.  40.]
 [  0.   0.   2. 128.   0.  40.]
 [ 56.   1.  16.   0. 225.   1.]
 [  0.  16.  59.  53.   5. 248.]]
Test performance..
F-Score: 72.66
F-Score-index: 139
              precision    recall  f1-score   support

           0     0.5419    0.7639    0.6340     144.0
           1     0.8248    0.7878    0.8058     245.0
           2     0.7285    0.7057    0.7169     384.0
           3     0.6564    0.7529    0.7014     170.0
           4     0.8654    0.7525    0.8050     299.0
           5     0.6908    0.6509    0.6703     381.0

    accuracy                         0.7240    1623.0
   macro avg     0.7180    0.7356    0.7222    1623.0
weighted avg     0.7353    0.7240    0.7266    1623.0

[[110.   4.   6.   0.  24.   0.]
 [  2. 193.  18.   2.   0.  30.]
 [ 35.  20. 271.  12.   6.  40.]
 [  0.   0.   2. 128.   0.  40.]
 [ 56.   1.  16.   0. 225.   1.]
 [  0.  16.  59.  53.   5. 248.]]
